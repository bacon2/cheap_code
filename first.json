[
  {
    "role": "system",
    "content": "You are a helpful assistant."
  },
  {
    "role": "user",
    "content": "This marks the beginning of our first conversation that will be saved to my machine."
  },
  {
    "role": "assistant",
    "content": "It's nice to be a part of this milestone. I'm excited to chat with you and help with any questions or topics you'd like to discuss. Since this is the start of our conversation history, I'm curious - what would you like to talk about first?"
  },
  {
    "role": "user",
    "content": "I saved and loaded it. What was the first thing I told you?"
  },
  {
    "role": "assistant",
    "content": "You initially told me that our conversation marked the beginning of our first conversation that would be saved to your machine."
  },
  {
    "role": "user",
    "content": "That's great. The save feature worked. I'm wondering if you can help me with a glitch in your UI. I can't read the full text of your messages unless I hover over them and scroll down. Basically the textbox height is not enough."
  },
  {
    "role": "user",
    "content": "Here's my current code:\nimport os\nimport re\nimport threading\nimport queue\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom unidiff import PatchSet\nimport difflib\nimport json\n\n# -----------------------------\n# Environment / Client Setup\n# -----------------------------\n\nload_dotenv()\n\ngroq_client = OpenAI(\n    api_key=os.getenv(\"GROQ_API_KEY\"),\n    base_url=\"https://api.groq.com/openai/v1\",\n)\n\nopenai_client = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\n# Model configurations\nGROQ_MODELS = [\n    \"llama-3.3-70b-versatile\",\n    \"moonshotai/kimi-k2-instruct\",\n    \"moonshotai/kimi-k2-instruct-0905\",\n    \"openai/gpt-oss-120b\",\n    \"openai/gpt-oss-20b\",\n    \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n]\n\nOPENAI_MODELS = [\n    \"gpt-5.2\",\n    \"gpt-5-mini\",\n    \"gpt-4.1\",\n]\n\nALL_MODELS = GROQ_MODELS + OPENAI_MODELS\n\nCODE_BLOCK_REGEX = re.compile(\n    r\"```(?:\\w+)?\\n(.*?)```\",\n    re.DOTALL\n)\n\n# -----------------------------\n# UI\n# -----------------------------\nclass CodeProposal(ttk.Frame):\n    def __init__(self, parent, code: str, on_replace, on_add):\n        super().__init__(parent)\n\n        self.code = code\n\n        self.text = tk.Text(\n            self,\n            height=min(12, code.count(\"\\n\") + 2),\n            wrap=\"word\",\n            font=(\"Courier New\", 10)\n        )\n        self.text.insert(\"1.0\", code)\n        self.text.configure(state=\"disabled\")\n        self.text.pack(fill=\"x\", padx=5, pady=5)\n\n        button_row = ttk.Frame(self)\n        button_row.pack(anchor=\"e\", padx=5, pady=(0, 5))\n\n        ttk.Button(\n            button_row, text=\"Replace\",\n            command=lambda: on_replace(code)\n        ).pack(side=\"right\", padx=2)\n\n        ttk.Button(\n            button_row, text=\"Add\",\n            command=lambda: on_add(code)\n        ).pack(side=\"right\", padx=2)\n\nclass ChatWithCode(ttk.Frame):\n    def __init__(self, parent):\n        super().__init__(parent)\n\n        # Scrollable canvas\n        self.canvas = tk.Canvas(self)\n        self.scrollbar = ttk.Scrollbar(self, command=self.canvas.yview)\n        self.canvas.configure(yscrollcommand=self.scrollbar.set)\n\n        self.scrollbar.pack(side=\"right\", fill=\"y\")\n        self.canvas.pack(side=\"left\", fill=\"both\", expand=True)\n\n        # Inner frame that holds all messages\n        self.inner_frame = ttk.Frame(self.canvas)\n        self.canvas.create_window((0, 0), window=self.inner_frame, anchor=\"nw\")\n\n        # Resize scrollregion when inner_frame changes\n        self.inner_frame.bind(\"<Configure>\", lambda e: self.canvas.configure(scrollregion=self.canvas.bbox(\"all\")))\n\n    def add_message_with_code(self, text: str, code: str, on_replace, on_add):\n        # Text explanation\n        text_label = tk.Label(\n            self.inner_frame,\n            text=text,\n            wraplength=600,\n            justify=\"left\"\n        )\n        text_label.pack(fill=\"x\", padx=5, pady=(5, 0))\n\n        # CodeProposal widget\n        proposal = CodeProposal(self.inner_frame, code, on_replace, on_add)\n        proposal.pack(fill=\"x\", padx=5, pady=(0, 10))\n\n        # Scroll to bottom\n        self.canvas.update_idletasks()\n        self.canvas.yview_moveto(1.0)\n\n\n# ------------------------------------------------------------------\n#  ChatUI  –  conversation-only persistence\n# ------------------------------------------------------------------\nclass ChatUI:\n    def __init__(self, root: tk.Tk):\n        self.root = root\n        self.root.title(\"Chat + Code UI\")\n        self.root.geometry(\"1100x650\")\n\n        self.conversation = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n        ]\n\n        self.token_queue = queue.Queue()\n        self.is_generating = False\n        self.current_assistant_buffer = \"\"\n\n        self.code_contents = \"\"\n\n        # ---- persistence helpers ----\n        self.auto_save_path: str | None = None   # last saved file\n        self.auto_save_after_id: str | None = None\n        self.auto_save_interval_ms = 30_000      # 30-second auto-save\n\n        # Default models\n        self.chat_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n        self.helper_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n\n        self._build_layout()\n        self._bind_keys()\n        self._poll_tokens()\n\n        # kick off auto-save loop\n        self._schedule_auto_save()\n\n    # -----------------------------\n    # Layout (unchanged except for menu)\n    # -----------------------------\n    def _build_layout(self) -> None:\n        self.root.rowconfigure(0, weight=0)\n        self.root.rowconfigure(1, weight=1)\n        self.root.rowconfigure(2, weight=0)\n        self.root.columnconfigure(0, weight=1)\n\n        # ----------------------\n        # Model Selection Bar\n        # ----------------------\n        model_frame = ttk.Frame(self.root)\n        model_frame.grid(row=0, column=0, sticky=\"ew\", padx=5, pady=5)\n\n        ttk.Label(model_frame, text=\"Chat Model:\").pack(side=\"left\", padx=(0, 5))\n        chat_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.chat_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        chat_dropdown.pack(side=\"left\", padx=(0, 20))\n\n        ttk.Label(model_frame, text=\"Helper Model:\").pack(side=\"left\", padx=(0, 5))\n        helper_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.helper_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        helper_dropdown.pack(side=\"left\")\n\n        # ----------------------\n        # Menu bar\n        # ----------------------\n        menubar = tk.Menu(self.root)\n        file_menu = tk.Menu(menubar, tearoff=0)\n        file_menu.add_command(label=\"Save conversation…\", command=self.save_conversation)\n        file_menu.add_command(label=\"Load conversation…\", command=self.load_conversation)\n        file_menu.add_separator()\n        file_menu.add_command(label=\"Export markdown…\", command=self.export_markdown)\n        menubar.add_cascade(label=\"File\", menu=file_menu)\n        self.root.config(menu=menubar)\n\n        main_pane = ttk.PanedWindow(self.root, orient=\"horizontal\")\n        main_pane.grid(row=1, column=0, sticky=\"nsew\")\n\n        # ----------------------\n        # Chat + Code Proposals\n        # ----------------------\n        chat_frame = ttk.Frame(main_pane)\n        chat_frame.rowconfigure(0, weight=1)\n        chat_frame.columnconfigure(0, weight=1)\n\n        self.chat_canvas = tk.Canvas(chat_frame)\n        self.chat_scrollbar = ttk.Scrollbar(chat_frame, command=self.chat_canvas.yview)\n        self.chat_canvas.configure(yscrollcommand=self.chat_scrollbar.set)\n\n        self.chat_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n        self.chat_canvas.grid(row=0, column=0, sticky=\"nsew\")\n\n        self.chat_inner_frame = ttk.Frame(self.chat_canvas)\n        self.chat_canvas.create_window((0, 0), window=self.chat_inner_frame, anchor=\"nw\")\n        self.chat_inner_frame.bind(\n            \"<Configure>\",\n            lambda e: self.chat_canvas.configure(scrollregion=self.chat_canvas.bbox(\"all\"))\n        )\n        self.bg_color = self.root.cget(\"background\")\n\n        # ----------------------\n        # Code Panel\n        # ----------------------\n        code_frame = ttk.Frame(main_pane)\n        code_frame.rowconfigure(0, weight=1)\n        code_frame.columnconfigure(0, weight=1)\n\n        self.code_panel = tk.Text(code_frame, wrap=\"none\")\n        self.code_panel.grid(row=0, column=0, sticky=\"nsew\")\n        self.code_panel.bind(\"<<Modified>>\", self._on_code_modified)\n\n        code_scroll = ttk.Scrollbar(code_frame, command=self.code_panel.yview)\n        code_scroll.grid(row=0, column=1, sticky=\"ns\")\n        self.code_panel[\"yscrollcommand\"] = code_scroll.set\n\n        main_pane.add(chat_frame, weight=1)\n        main_pane.add(code_frame, weight=1)\n\n        # ----------------------\n        # Input Box\n        # ----------------------\n        self.input_box = tk.Text(self.root, height=5, wrap=\"word\")\n        self.input_box.grid(row=2, column=0, sticky=\"nsew\")\n\n    # -----------------------------\n    # Model helpers (unchanged)\n    # -----------------------------\n    def _get_client_for_model(self, model: str):\n        return openai_client if model in OPENAI_MODELS else groq_client\n\n    def _get_max_tokens_for_model(self, model: str):\n        return 16000 if model in OPENAI_MODELS else 8000\n\n    def _uses_max_completion_tokens(self, model: str):\n        return model in [\"gpt-5.2\", \"gpt-5-mini\"]\n\n    # -----------------------------\n    # Input / generation (unchanged)\n    # -----------------------------\n    def _bind_keys(self):\n        self.input_box.bind(\"<Return>\", self._on_enter)\n        self.input_box.bind(\"<Shift-Return>\", lambda e: None)\n\n    def _on_code_modified(self, event):\n        if self.code_panel.edit_modified():\n            self.code_contents = self.code_panel.get(\"1.0\", \"end-1c\")\n            self.code_panel.edit_modified(False)\n\n    def _on_enter(self, event):\n        if self.is_generating:\n            return \"break\"\n        text = self.input_box.get(\"1.0\", \"end-1c\").strip()\n        if not text:\n            return \"break\"\n        self.input_box.delete(\"1.0\", \"end\")\n        self._append_chat(\"You\", text)\n        self.conversation.append({\"role\": \"user\", \"content\": text})\n        if self.code_contents.strip():\n            self.conversation.append({\n                \"role\": \"user\",\n                \"content\": f\"Here's my current code:\\n{self.code_contents}\"\n            })\n        self._start_generation()\n        return \"break\"\n\n    def _start_generation(self):\n        self.is_generating = True\n        self.input_box.configure(state=\"disabled\")\n        self.current_assistant_buffer = \"\"\n        self._append_chat(\"Assistant\", \"\")\n        threading.Thread(target=self._stream_chat_response, daemon=True).start()\n\n    def _stream_chat_response(self):\n        try:\n            model = self.chat_model.get()\n            client = self._get_client_for_model(model)\n            max_tokens = self._get_max_tokens_for_model(model)\n            params = {\n                \"model\": model,\n                \"messages\": self.conversation,\n                \"stream\": True,\n            }\n            if self._uses_max_completion_tokens(model):\n                params[\"max_completion_tokens\"] = max_tokens\n            else:\n                params[\"max_tokens\"] = max_tokens\n            stream = client.chat.completions.create(**params)\n            for chunk in stream:\n                delta = chunk.choices[0].delta.content or \"\"\n                if delta:\n                    self.token_queue.put(delta)\n        except Exception as e:\n            self.token_queue.put(f\"\\n[Error: {e}]\")\n        finally:\n            self.token_queue.put(None)\n\n    def _poll_tokens(self):\n        try:\n            while True:\n                token = self.token_queue.get_nowait()\n                if token is None:\n                    self._finalize_assistant_message()\n                    break\n                self.current_assistant_buffer += token\n                self._append_to_last_chat(token)\n        except queue.Empty:\n            pass\n        self.root.after(20, self._poll_tokens)\n\n    def _finalize_assistant_message(self):\n        assistant_text = self.current_assistant_buffer\n        self.conversation.append({\"role\": \"assistant\", \"content\": assistant_text})\n        self.is_generating = False\n        self.input_box.configure(state=\"normal\")\n        self._replace_with_parsed_message(assistant_text)\n\n    # -----------------------------\n    # Parsing / UI helpers (unchanged)\n    # -----------------------------\n    def _replace_with_parsed_message(self, text: str):\n        if self.current_message_label:\n            self.current_message_label.destroy()\n            self.current_message_label = None\n        message_container = ttk.Frame(self.chat_inner_frame)\n        message_container.pack(fill=\"x\", padx=5, pady=5)\n        sender_text = tk.Text(\n            message_container,\n            height=1,\n            wrap=\"none\",\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            background=self.bg_color\n        )\n        sender_text.insert(\"1.0\", \"Assistant:\")\n        sender_text.configure(state=\"disabled\")\n        sender_text.pack(fill=\"x\")\n        parts = self._split_text_and_code(text)\n        for part_type, content in parts:\n            if part_type == \"text\" and content.strip():\n                text_widget = tk.Text(\n                    message_container,\n                    height=content.count(\"\\n\") + 1,\n                    wrap=\"word\",\n                    relief=\"flat\",\n                    background=self.bg_color\n                )\n                text_widget.insert(\"1.0\", content)\n                text_widget.configure(state=\"disabled\")\n                text_widget.pack(fill=\"x\", padx=5, pady=2)\n            elif part_type == \"code\":\n                proposal = CodeProposal(\n                    parent=message_container,\n                    code=content,\n                    on_replace=self._replace_code_panel,\n                    on_add=self._add_code_via_helper,\n                )\n                proposal.pack(fill=\"x\", pady=5)\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _split_text_and_code(self, text: str):\n        parts = []\n        last_end = 0\n        for match in CODE_BLOCK_REGEX.finditer(text):\n            if match.start() > last_end:\n                parts.append((\"text\", text[last_end:match.start()]))\n            parts.append((\"code\", match.group(1)))\n            last_end = match.end()\n        if last_end < len(text):\n            parts.append((\"text\", text[last_end:]))\n        return parts\n\n    # -----------------------------\n    # Code handling (unchanged)\n    # -----------------------------\n    def _extract_code_blocks(self, text: str) -> list[str]:\n        return CODE_BLOCK_REGEX.findall(text)\n\n    def _add_code_via_helper(self, new_code: str):\n        if not self.code_contents.strip():\n            self._replace_code_panel(new_code)\n            return\n        diff = self._generate_diff_with_helper(old_code=self.code_contents, new_code=new_code)\n        if not diff.strip():\n            return\n        try:\n            patched = self._apply_unified_diff(self.code_contents, diff)\n        except Exception as e:\n            self._append_chat(\"System\", diff)\n            self._append_chat(\"System\", f\"Diff failed:\\n{e}\")\n            return\n        self.code_contents = patched\n        self.code_panel.delete(\"1.0\", \"end\")\n        self.code_panel.insert(\"1.0\", patched)\n\n    def _replace_code_panel(self, code: str):\n        self.code_contents = code\n        self.code_panel.delete(\"1.0\", \"end\")\n        self.code_panel.insert(\"1.0\", code)\n\n    def _generate_diff_with_helper(self, old_code: str, new_code: str) -> str:\n        system_prompt = (\n            \"You generate unified diffs that will be applied automatically.\\n\"\n            \"The diff must apply cleanly.\\n\"\n            \"Output ONLY a unified diff.\\n\"\n            \"Do NOT include explanations, prose, or markdown.\\n\"\n            \"Always include file headers using 'a/code.py' and 'b/code.py'.\\n\"\n            \"If no changes are required, output an empty response.\"\n        )\n        user_prompt = (\n            \"CURRENT FILE:\\n\"\n            f\"{old_code}\\n\\n\"\n            \"NEW CODE TO INTEGRATE:\\n\"\n            f\"{new_code}\\n\"\n        )\n        model = self.helper_model.get()\n        client = self._get_client_for_model(model)\n        max_tokens = self._get_max_tokens_for_model(model)\n        params = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n        }\n        if self._uses_max_completion_tokens(model):\n            params[\"max_completion_tokens\"] = max_tokens\n        else:\n            params[\"max_tokens\"] = max_tokens\n        response = client.chat.completions.create(**params)\n        diff_text = response.choices[0].message.content or \"\"\n        return diff_text.strip()\n\n    def _fix_diff_hunk_counts(self, diff_text: str) -> str:\n        lines = diff_text.split('\\n')\n        fixed_lines = []\n        i = 0\n        while i < len(lines):\n            line = lines[i]\n            if line.startswith('@@'):\n                match = re.match(r'@@ -(\\d+),?\\d* \\+(\\d+),?\\d* @@', line)\n                if match:\n                    old_start = match.group(1)\n                    new_start = match.group(2)\n                    hunk_lines = []\n                    j = i + 1\n                    while j < len(lines) and not lines[j].startswith('@@'):\n                        hunk_lines.append(lines[j])\n                        j += 1\n                    old_count = sum(1 for l in hunk_lines if l.startswith('-') or l.startswith(' '))\n                    new_count = sum(1 for l in hunk_lines if l.startswith('+') or l.startswith(' '))\n                    fixed_header = f\"@@ -{old_start},{old_count} +{new_start},{new_count} @@\"\n                    fixed_lines.append(fixed_header)\n                    fixed_lines.extend(hunk_lines)\n                    i = j\n                else:\n                    fixed_lines.append(line)\n                    i += 1\n            else:\n                fixed_lines.append(line)\n                i += 1\n        return '\\n'.join(fixed_lines)\n\n    def _apply_unified_diff(self, old_text: str, diff_text: str) -> str:\n        fixed_diff = self._fix_diff_hunk_counts(diff_text)\n        return self._apply_patch_to_text(old_text, fixed_diff)\n\n    def _apply_patch_to_text(self, old_text: str, diff_text: str) -> str:\n        old_lines = old_text.splitlines(keepends=True)\n        patch = PatchSet(diff_text)\n        if len(patch) != 1:\n            raise ValueError(\"Patch must have exactly one file for code panel\")\n        patched_file = patch[0]\n        new_lines = []\n        old_idx = 0\n        for hunk in patched_file:\n            while old_idx < hunk.source_start - 1:\n                new_lines.append(old_lines[old_idx])\n                old_idx += 1\n            for line in hunk:\n                if line.is_added:\n                    new_lines.append(line.value)\n                elif line.is_context:\n                    new_lines.append(old_lines[old_idx])\n                    old_idx += 1\n                elif line.is_removed:\n                    old_idx += 1\n        new_lines.extend(old_lines[old_idx:])\n        return \"\".join(new_lines)\n\n    # -----------------------------\n    # Persistence\n    # -----------------------------\n    def save_conversation(self, path: str | None = None):\n        if path is None:\n            path = filedialog.asksaveasfilename(\n                defaultextension=\".json\",\n                filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n            )\n            if not path:\n                return\n        try:\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self.conversation, f, indent=2, ensure_ascii=False)\n            self.auto_save_path = path\n            messagebox.showinfo(\"Saved\", f\"Conversation saved to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Save error\", str(e))\n\n    def load_conversation(self):\n        path = filedialog.askopenfilename(\n            defaultextension=\".json\",\n            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                loaded = json.load(f)\n            if not isinstance(loaded, list):\n                raise ValueError(\"File must contain a list of messages.\")\n            self.conversation = loaded\n            self.auto_save_path = path\n            self._redraw_chat()\n            messagebox.showinfo(\"Loaded\", f\"Conversation loaded from\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Load error\", str(e))\n\n    def export_markdown(self):\n        path = filedialog.asksaveasfilename(\n            defaultextension=\".md\",\n            filetypes=[(\"Markdown files\", \"*.md\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            lines = []\n            for msg in self.conversation:\n                role = msg.get(\"role\", \"\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue\n                name = \"You\" if role == \"user\" else \"Assistant\"\n                lines.append(f\"**{name}:**\\n{content}\\n\")\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\\n\".join(lines))\n            messagebox.showinfo(\"Exported\", f\"Markdown exported to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Export error\", str(e))\n\n    def _redraw_chat(self):\n        for w in self.chat_inner_frame.winfo_children():\n            w.destroy()\n        self.current_message_label = None\n        for msg in self.conversation:\n            role = msg.get(\"role\")\n            content = msg.get(\"content\", \"\")\n            if role == \"system\":\n                continue\n            sender = \"You\" if role == \"user\" else \"Assistant\"\n            if role == \"assistant\":\n                self._replace_with_parsed_message(content)\n            else:\n                self._append_chat(sender, content)\n\n    def _schedule_auto_save(self):\n        if self.auto_save_path and self.conversation:\n            try:\n                with open(self.auto_save_path, \"w\", encoding=\"utf-8\") as f:\n                    json.dump(self.conversation, f, indent=2, ensure_ascii=False)\n            except Exception:\n                pass\n        self.auto_save_after_id = self.root.after(self.auto_save_interval_ms, self._schedule_auto_save)\n\n    # -----------------------------\n    # UI Helpers (unchanged)\n    # -----------------------------\n    def _append_chat(self, sender: str, text: str):\n        \"\"\"Add a new chat message to the chat display.\"\"\"\n        message_text = f\"{sender}:\\n{text}\"\n        \n        # Use Text widget for copyable content\n        text_widget = tk.Text(\n            self.chat_inner_frame,\n            height=message_text.count(\"\\n\") + 1,\n            wrap=\"word\",\n            relief=\"flat\",\n            background=self.bg_color\n        )\n        text_widget.insert(\"1.0\", message_text)\n        text_widget.configure(state=\"disabled\")\n        text_widget.pack(fill=\"x\", padx=5, pady=5)\n        \n        # Store reference for streaming updates\n        self.current_message_label = text_widget\n        \n        # Scroll to bottom\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _append_to_last_chat(self, text: str):\n        \"\"\"Append text to the current streaming message.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.configure(state=\"normal\")\n            self.current_message_label.insert(\"end\", text)\n            # Update height based on line count\n            line_count = int(self.current_message_label.index(\"end-1c\").split(\".\")[0])\n            self.current_message_label.configure(height=line_count, state=\"disabled\")\n            \n            # Scroll to bottom\n            self.chat_canvas.update_idletasks()\n            self.chat_canvas.yview_moveto(1.0)\n\n\nif __name__ == \"__main__\":\n    root = tk.Tk()\n    ChatUI(root)\n    root.mainloop()"
  },
  {
    "role": "assistant",
    "content": "The problem lies in the height configuration of the `Text` widget for displaying chat messages and the `CodeProposal` widgets. \n\nYou're currently setting the height of the `Text` widget based on the number of newlines in the message, which might not be enough for large messages. Similarly, in the `CodeProposal` class, you're setting the height of the `Text` widget based on the number of newlines in the code.\n\nHere's how you can adjust the height of the `Text` widget in the `_append_chat` and `CodeProposal` classes to ensure that the entire message is visible:\n\n```python\ndef _append_chat(self, sender: str, text: str):\n    # ...\n    text_widget = tk.Text(\n        self.chat_inner_frame,\n        width=80,  # Set a fixed width\n        wrap=\"word\",\n        relief=\"flat\",\n        background=self.bg_color\n    )\n    text_widget.insert(\"1.0\", message_text)\n    text_widget.configure(state=\"disabled\")\n    text_widget.pack(fill=\"both\", expand=True, padx=5, pady=5)\n    # ...\n\nclass CodeProposal(ttk.Frame):\n    def __init__(self, parent, code: str, on_replace, on_add):\n        # ...\n        self.text = tk.Text(\n            self,\n            width=80,  # Set a fixed width\n            wrap=\"word\",\n            font=(\"Courier New\", 10)\n        )\n        # ...\n```\n\nThis will make the `Text` widget expand to fill any extra space in its parent widget, allowing it to display longer messages without requiring a scrollbar.\n\nAlternatively, you can use a `Scrollbar` widget to add a scrollbar to the `Text` widget, like this:\n\n```python\ndef _append_chat(self, sender: str, text: str):\n    # ...\n    message_frame = ttk.Frame(self.chat_inner_frame)\n    message_frame.pack(fill=\"x\", padx=5, pady=5)\n    \n    text_widget = tk.Text(\n        message_frame,\n        width=80,  # Set a fixed width\n        height=10,  # Set a fixed height\n        wrap=\"word\",\n        relief=\"flat\",\n        background=self.bg_color\n    )\n    text_widget.insert(\"1.0\", message_text)\n    text_widget.configure(state=\"disabled\")\n    \n    scrollbar = ttk.Scrollbar(message_frame, command=text_widget.yview)\n    text_widget[\"yscrollcommand\"] = scrollbar.set\n    text_widget.pack(side=\"left\", fill=\"both\", expand=True)\n    scrollbar.pack(side=\"right\", fill=\"y\")\n    # ...\n```\n\nThis will add a scrollbar to the `Text` widget, allowing the user to scroll through long messages."
  },
  {
    "role": "user",
    "content": "I think I fixed it, but is it possible it saves the height of old messages, so they will always be broken when I load them?"
  },
  {
    "role": "assistant",
    "content": "When you save the conversation, you're storing the height of each message at the time it was created. However, when you load the conversation, you're using the same height for each message, even if the message contains more or less text than it did originally.\n\nThis can cause problems if the messages have been modified or if the window size has changed, because the height of each message is not being recalculated.\n\nTo fix this, you should recalculate the height of each message when you load the conversation. Here's how you can do it in the `_redraw_chat` method:\n\n```python\ndef _redraw_chat(self):\n    for w in self.chat_inner_frame.winfo_children():\n        w.destroy()\n    self.current_message_label = None\n    for msg in self.conversation:\n        role = msg.get(\"role\")\n        content = msg.get(\"content\", \"\")\n        if role == \"system\":\n            continue\n        sender = \"You\" if role == \"user\" else \"Assistant\"\n        if role == \"assistant\":\n            self._replace_with_parsed_message(content)\n        else:\n            self._append_chat(sender, content)\n```\n\nHowever, in the `_replace_with_parsed_message` method, the height of the `Text` widget is still being set based on the original height of the message. To fix this, you can modify the `_replace_with_parsed_message` method to recalculate the height of each message:\n\n```python\ndef _replace_with_parsed_message(self, text: str):\n    if self.current_message_label:\n        self.current_message_label.destroy()\n        self.current_message_label = None\n    message_container = ttk.Frame(self.chat_inner_frame)\n    message_container.pack(fill=\"x\", padx=5, pady=5)\n    sender_text = tk.Text(\n        message_container,\n        height=1,\n        wrap=\"none\",\n        font=(\"TkDefaultFont\", 10, \"bold\"),\n        relief=\"flat\",\n        background=self.bg_color\n    )\n    sender_text.insert(\"1.0\", \"Assistant:\")\n    sender_text.configure(state=\"disabled\")\n    sender_text.pack(fill=\"x\")\n    parts = self._split_text_and_code(text)\n    for part_type, content in parts:\n        if part_type == \"text\" and content.strip():\n            text_widget = tk.Text(\n                message_container,\n                width=80,  # Set a fixed width\n                wrap=\"word\",\n                relief=\"flat\",\n                background=self.bg_color\n            )\n            text_widget.insert(\"1.0\", content)\n            text_widget.configure(state=\"disabled\")\n            text_widget.pack(fill=\"x\", padx=5, pady=2)\n        elif part_type == \"code\":\n            proposal = CodeProposal(\n                parent=message_container,\n                code=content,\n                on_replace=self._replace_code_panel,\n                on_add=self._add_code_via_helper,\n            )\n            proposal.pack(fill=\"x\", pady=5)\n    self.chat_canvas.update_idletasks()\n    self.chat_canvas.yview_moveto(1.0)\n```\n\nAnd also in the `CodeProposal` class:\n\n```python\nclass CodeProposal(ttk.Frame):\n    def __init__(self, parent, code: str, on_replace, on_add):\n        super().__init__(parent)\n        self.code = code\n        self.text = tk.Text(\n            self,\n            width=80,  # Set a fixed width\n            wrap=\"word\",\n            font=(\"Courier New\", 10)\n        )\n        self.text.insert(\"1.0\", code)\n        self.text.configure(state=\"disabled\")\n        self.text.pack(fill=\"x\", padx=5, pady=5)\n        # ...\n```\n\nThis will ensure that the height of each message is recalculated when you load the conversation, so they should display correctly even if the text has been modified."
  }
]