[
  {
    "role": "system",
    "content": "You are a helpful coding assistant. Your code snippets must never contain ellipses, or any comment meant to represent code. "
  },
  {
    "role": "user",
    "content": "Suggest 1 change to this kaggle notebook."
  },
  {
    "role": "user",
    "content": "Here's my current code:\n# XGBoost with TabM Pseudo Labels!\n\n# In this starter notebook, we demonstrate the following\n\n#     TabM model\n#     XGBoost model\n#     Basic feature engineering\n#     Pseudo label validation and test data\n#     Train XGB with TabM pseudo label data\n\n\n# Import Libraries\n\n!pip install pytabkit\n\nimport pandas as pd, numpy as np, gc\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nimport torch\nfrom pytabkit import TabM_D_Regressor\n\nimport xgboost as xgb\nprint(f\"XGBoost version: {xgb.__version__}\")\n\nimport importlib.metadata\nprint(f\"TabM version: {importlib.metadata.version('pytabkit')}\")\n# Load Data\n\n# We load train, test, and original dataset.\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s6e1/train.csv\")\nprint(\"Train shape:\", train.shape )\ntrain.head(1)\n\ntest = pd.read_csv(\"/kaggle/input/playground-series-s6e1/test.csv\")\nprint(\"Test shape:\", test.shape )\ntest.head(1)\n\norig = pd.read_csv(\"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\")\nprint(\"Original data shape:\", orig.shape )\norig.head(1)\n\n# Basic Feature Engineering\n\n# We engineer two things. First, we create a new feature called \"formula\" based on Kaggler Broccoli Beef's discussion here. Second, we encode the given 11 features as both ordinal numbers and categories for XGB's default categorical handling and TabM's categorical handling.\nTARGET = \"exam_score\"\nCATS = [\"gender\",\"course\",\"internet_access\",\"study_method\",\"sleep_quality\",\"facility_rating\",\"exam_difficulty\"]\nFEATURES = ['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\nprint(f\"There are {len(FEATURES)} features.\")\n\n# https://www.kaggle.com/competitions/playground-series-s6e1/discussion/665915\ndef formula(df):\n    f = (6*df.study_hours + 0.35*df.class_attendance + 1.5*df.sleep_hours +\n                 5*(df.sleep_quality=='good') + -5*(df.sleep_quality=='poor') +\n                 10*(df.study_method=='coaching') + 5*(df.study_method=='mixed') + 2*(df.study_method=='group study') + 1*(df.study_method=='online videos') +\n                 4*(df.facility_rating=='high') + -4*(df.facility_rating=='low') )\n    return f\n\nfor df in [train, test, orig]:\n    df['formula'] = formula(df)\n\nordinal_maps = {\n    \"gender\" : {\"male\":0, \"female\":1, \"other\":2},\n    \"internet_access\" : {\"no\":0, \"yes\":1},\n    \"sleep_quality\" : {\"poor\":0, \"average\":1, \"good\":2},\n    \"facility_rating\" : {\"low\":0, \"medium\":1, \"high\":2},\n    \"exam_difficulty\" : {\"easy\":0, \"moderate\":1, \"hard\":2},\n    \"course\" : {\"ba\":0, \"b.sc\":1, \"diploma\":2, \"b.tech\":3, \"b.com\":4, \"bca\":5, \"bba\":6},\n    \"study_method\" : {\"self-study\":0, \"online videos\":1, \"group study\":2, \"mixed\":3, \"coaching\":4},\n}\n\nfor df in [train, test, orig]:\n    for c in CATS:\n        df[c] = df[c].map(ordinal_maps[c]).fillna(-1).astype('int32')\n\nNEW_CATS = []\nfor c in FEATURES:\n    n = f\"CAT_{c}\"\n    NEW_CATS.append(n)\n    \n    train[n] = train[c].copy()\n    test[n] = test[c].copy()\n    orig[n] = orig[c].copy()\n\n    combine = pd.concat([train[n],test[n],orig[n]],axis=0)\n    v,_ = combine.factorize()\n    train[n] = v[:len(train)]\n    test[n] = v[len(train):len(train)+len(test)]\n    orig[n] = v[len(train)+len(test):]\n    for df in [train, test, orig]:\n        df[n] = df[n].astype(\"int32\")\n\nFEATURES += ['formula'] + NEW_CATS\nprint(f\"Now there are {len(FEATURES)} features.\")\n\n# XGBoost w/ TabM Pseudo Labels\n\n# We train XGBoost using pseudo labels on validation and test data from TabM.\nxgb_params = {\n    \"n_estimators\": 10_000,           \n    \"learning_rate\": 0.01,          \n    \"max_depth\": 8,                  \n    \"subsample\": 0.8,               \n    \"colsample_bytree\": 0.35,        \n    \"min_child_weight\": 5,           \n    \"early_stopping_rounds\": 100,   \n    \"eval_metric\": \"rmse\",\n    \"enable_categorical\": True,\n    \"device\": \"cuda\",\n    \"random_state\": 42,\n}\n\ntabm_params = {\n    \"verbosity\": 0,\n    \"arch_type\": \"tabm-mini-normal\",\n    \"tabm_k\": 24, \n    \"num_emb_type\": \"pwl\", \n    \"d_embedding\": 8,\n    \"batch_size\": 256,\n    \"lr\": 1e-3, \n    \"n_epochs\": 100, \n    \"dropout\": 0.11, \n    \"d_block\": 256, \n    \"n_blocks\": 5, \n    \"patience\": 4, \n    \"weight_decay\": 1e-2, \n    \"device\": \"cuda\",\n    \"random_state\": 42,\n}\n\nN_SPLITS = 10\n\n# PREPARE DATA\nX = train[FEATURES]\nX_test = test[FEATURES]\nX_orig = orig[FEATURES]\ny = train[TARGET].values\ny_orig = orig[TARGET].values\n\n# STORE OOF AND TEST PREDS\noof_preds_xgb = np.zeros(len(train))\ntest_preds_xgb = np.zeros(len(test))\n\n# KFOLD TRAIN AND VALIDATE\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n    print(\"#\"*25)\n    print(f\"### FOLD {fold}\")\n    print(\"#\"*25)\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    # ADD ORIGINAL DATA\n    X_train = pd.concat([X_train,X_orig],axis=0)\n    y_train = np.concatenate([y_train,y_orig],axis=0)\n\n    # TRAIN TABM\n    model = TabM_D_Regressor(**tabm_params)\n    model.fit(\n        X_train, y_train,\n        X_val, y_val,\n        cat_col_names=NEW_CATS,\n    )\n    oof_preds = model.predict(X_val)\n    test_preds = model.predict(X_test)\n\n    # PSEUDO LABEL WITH TABM FOR XGB\n    X_train_xgb = pd.concat([X_train,X_val,X_test],axis=0)\n    y_train_xgb = np.concatenate([y_train,oof_preds,test_preds],axis=0)\n\n    # PREPARE CATS FOR XGB\n    for c in NEW_CATS:\n        X_train_xgb[c] = X_train_xgb[c].astype('category')\n    X_val_xgb = X_train_xgb.iloc[len(X_train):len(X_train)+len(X_val)]\n    X_test_xgb = X_train_xgb.iloc[len(X_train)+len(X_val):]\n\n    # TRAIN XGB WITH PSEUDO LABELS\n    model = XGBRegressor(**xgb_params)\n    model.fit(\n        X_train_xgb,\n        y_train_xgb,\n        eval_set=[(X_val_xgb, y_val)],\n        verbose=200,\n    )\n    oof_preds = model.predict(X_val_xgb)\n    oof_preds_xgb[val_idx] = oof_preds\n\n    rmse_xgb = mean_squared_error(y_val, oof_preds)**0.5\n    print(f\"Fold {fold} RMSE XGB w/ Pseudo: {rmse_xgb:.5f}\")\n\n    test_preds = model.predict(X_test_xgb)\n    test_preds_xgb += test_preds/N_SPLITS\n\n    # CLEAR MEMORY\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# OVERALL CV SCORE\nprint(\"-\" * 40)\nrmse_xgb = mean_squared_error(y, oof_preds_xgb)**0.5\nprint(f\"OOF RMSE XGB w/ Pseudo:       {rmse_xgb:.5f}\")\nprint()\n\n# XGBoost Feature Importance\nfig, ax = plt.subplots(figsize=(8, 8))  \nplot_importance(\n    model,\n    ax=ax,\n    importance_type=\"gain\", # or \"weight\", \"cover\"\n    max_num_features=40,\n    height=0.5,\n)\nax.set_title(\"XGB Feature Importance (last fold)\")\nfig.tight_layout()\nplt.show()\n\n# Compute CV Score\nrmse_xgb = mean_squared_error(y, oof_preds_xgb)**0.5\nprint(f\"OOF RMSE XGB w/ Pseudo:       {rmse_xgb:.5f}\")\n\nnp.save(\"oof_xgb\",oof_preds_xgb)\nnp.save(\"test_preds_xgb\",test_preds_xgb)\n\n# Create Submission CSV\nsub = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\")\nsub['exam_score'] = np.clip(test_preds_xgb,19.6,100)\nsub.to_csv(\"submission.csv\",index=False)\nsub.head()\n\nplt.hist(sub['exam_score'],bins=100)\nplt.title(\"XGB w/ Pseudo Test Preds\")\nplt.show()"
  },
  {
    "role": "assistant",
    "content": "One potential change to this code is to add a validation check for the saved oof and test predictions before creating the submission CSV. This can ensure that the saved predictions are not empty or contain NaN values.\n\n```python\n# Add validation check before creating submission CSV\nif np.isnan(oof_preds_xgb).any() or np.isnan(test_preds_xgb).any():\n    print(\"Error: Saved predictions contain NaN values.\")\nelse:\n    # Create Submission CSV\n    sub = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\")\n    sub['exam_score'] = np.clip(test_preds_xgb,19.6,100)\n    sub.to_csv(\"submission.csv\",index=False)\n    sub.head()\n```\n\nThis check will prevent the creation of a submission CSV if the saved predictions contain any NaN values, which could indicate an issue with the model training process."
  },
  {
    "role": "user",
    "content": "Great, I added it. Can you describe to me what this notebook is doing, considering I'm new to data science and not incredibly smart."
  },
  {
    "role": "user",
    "content": "Here's my current code:\n# XGBoost with TabM Pseudo Labels!\n\n# In this starter notebook, we demonstrate the following\n\n#     TabM model\n#     XGBoost model\n#     Basic feature engineering\n#     Pseudo label validation and test data\n#     Train XGB with TabM pseudo label data\n\n\n# Import Libraries\n\n!pip install pytabkit\n\nimport pandas as pd, numpy as np, gc\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nimport torch\nfrom pytabkit import TabM_D_Regressor\n\nimport xgboost as xgb\nprint(f\"XGBoost version: {xgb.__version__}\")\n\nimport importlib.metadata\nprint(f\"TabM version: {importlib.metadata.version('pytabkit')}\")\n# Load Data\n\n# We load train, test, and original dataset.\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s6e1/train.csv\")\nprint(\"Train shape:\", train.shape )\ntrain.head(1)\n\ntest = pd.read_csv(\"/kaggle/input/playground-series-s6e1/test.csv\")\nprint(\"Test shape:\", test.shape )\ntest.head(1)\n\norig = pd.read_csv(\"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\")\nprint(\"Original data shape:\", orig.shape )\norig.head(1)\n\n# Basic Feature Engineering\n\n# We engineer two things. First, we create a new feature called \"formula\" based on Kaggler Broccoli Beef's discussion here. Second, we encode the given 11 features as both ordinal numbers and categories for XGB's default categorical handling and TabM's categorical handling.\nTARGET = \"exam_score\"\nCATS = [\"gender\",\"course\",\"internet_access\",\"study_method\",\"sleep_quality\",\"facility_rating\",\"exam_difficulty\"]\nFEATURES = ['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\nprint(f\"There are {len(FEATURES)} features.\")\n\n# https://www.kaggle.com/competitions/playground-series-s6e1/discussion/665915\ndef formula(df):\n    f = (6*df.study_hours + 0.35*df.class_attendance + 1.5*df.sleep_hours +\n                 5*(df.sleep_quality=='good') + -5*(df.sleep_quality=='poor') +\n                 10*(df.study_method=='coaching') + 5*(df.study_method=='mixed') + 2*(df.study_method=='group study') + 1*(df.study_method=='online videos') +\n                 4*(df.facility_rating=='high') + -4*(df.facility_rating=='low') )\n    return f\n\nfor df in [train, test, orig]:\n    df['formula'] = formula(df)\n\nordinal_maps = {\n    \"gender\" : {\"male\":0, \"female\":1, \"other\":2},\n    \"internet_access\" : {\"no\":0, \"yes\":1},\n    \"sleep_quality\" : {\"poor\":0, \"average\":1, \"good\":2},\n    \"facility_rating\" : {\"low\":0, \"medium\":1, \"high\":2},\n    \"exam_difficulty\" : {\"easy\":0, \"moderate\":1, \"hard\":2},\n    \"course\" : {\"ba\":0, \"b.sc\":1, \"diploma\":2, \"b.tech\":3, \"b.com\":4, \"bca\":5, \"bba\":6},\n    \"study_method\" : {\"self-study\":0, \"online videos\":1, \"group study\":2, \"mixed\":3, \"coaching\":4},\n}\n\nfor df in [train, test, orig]:\n    for c in CATS:\n        df[c] = df[c].map(ordinal_maps[c]).fillna(-1).astype('int32')\n\nNEW_CATS = []\nfor c in FEATURES:\n    n = f\"CAT_{c}\"\n    NEW_CATS.append(n)\n    \n    train[n] = train[c].copy()\n    test[n] = test[c].copy()\n    orig[n] = orig[c].copy()\n\n    combine = pd.concat([train[n],test[n],orig[n]],axis=0)\n    v,_ = combine.factorize()\n    train[n] = v[:len(train)]\n    test[n] = v[len(train):len(train)+len(test)]\n    orig[n] = v[len(train)+len(test):]\n    for df in [train, test, orig]:\n        df[n] = df[n].astype(\"int32\")\n\nFEATURES += ['formula'] + NEW_CATS\nprint(f\"Now there are {len(FEATURES)} features.\")\n\n# XGBoost w/ TabM Pseudo Labels\n\n# We train XGBoost using pseudo labels on validation and test data from TabM.\nxgb_params = {\n    \"n_estimators\": 10_000,           \n    \"learning_rate\": 0.01,          \n    \"max_depth\": 8,                  \n    \"subsample\": 0.8,               \n    \"colsample_bytree\": 0.35,        \n    \"min_child_weight\": 5,           \n    \"early_stopping_rounds\": 100,   \n    \"eval_metric\": \"rmse\",\n    \"enable_categorical\": True,\n    \"device\": \"cuda\",\n    \"random_state\": 42,\n}\n\ntabm_params = {\n    \"verbosity\": 0,\n    \"arch_type\": \"tabm-mini-normal\",\n    \"tabm_k\": 24, \n    \"num_emb_type\": \"pwl\", \n    \"d_embedding\": 8,\n    \"batch_size\": 256,\n    \"lr\": 1e-3, \n    \"n_epochs\": 100, \n    \"dropout\": 0.11, \n    \"d_block\": 256, \n    \"n_blocks\": 5, \n    \"patience\": 4, \n    \"weight_decay\": 1e-2, \n    \"device\": \"cuda\",\n    \"random_state\": 42,\n}\n\nN_SPLITS = 10\n\n# PREPARE DATA\nX = train[FEATURES]\nX_test = test[FEATURES]\nX_orig = orig[FEATURES]\ny = train[TARGET].values\ny_orig = orig[TARGET].values\n\n# STORE OOF AND TEST PREDS\noof_preds_xgb = np.zeros(len(train))\ntest_preds_xgb = np.zeros(len(test))\n\n# KFOLD TRAIN AND VALIDATE\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n    print(\"#\"*25)\n    print(f\"### FOLD {fold}\")\n    print(\"#\"*25)\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    # ADD ORIGINAL DATA\n    X_train = pd.concat([X_train,X_orig],axis=0)\n    y_train = np.concatenate([y_train,y_orig],axis=0)\n\n    # TRAIN TABM\n    model = TabM_D_Regressor(**tabm_params)\n    model.fit(\n        X_train, y_train,\n        X_val, y_val,\n        cat_col_names=NEW_CATS,\n    )\n    oof_preds = model.predict(X_val)\n    test_preds = model.predict(X_test)\n\n    # PSEUDO LABEL WITH TABM FOR XGB\n    X_train_xgb = pd.concat([X_train,X_val,X_test],axis=0)\n    y_train_xgb = np.concatenate([y_train,oof_preds,test_preds],axis=0)\n\n    # PREPARE CATS FOR XGB\n    for c in NEW_CATS:\n        X_train_xgb[c] = X_train_xgb[c].astype('category')\n    X_val_xgb = X_train_xgb.iloc[len(X_train):len(X_train)+len(X_val)]\n    X_test_xgb = X_train_xgb.iloc[len(X_train)+len(X_val):]\n\n    # TRAIN XGB WITH PSEUDO LABELS\n    model = XGBRegressor(**xgb_params)\n    model.fit(\n        X_train_xgb,\n        y_train_xgb,\n        eval_set=[(X_val_xgb, y_val)],\n        verbose=200,\n    )\n    oof_preds = model.predict(X_val_xgb)\n    oof_preds_xgb[val_idx] = oof_preds\n\n    rmse_xgb = mean_squared_error(y_val, oof_preds)**0.5\n    print(f\"Fold {fold} RMSE XGB w/ Pseudo: {rmse_xgb:.5f}\")\n\n    test_preds = model.predict(X_test_xgb)\n    test_preds_xgb += test_preds/N_SPLITS\n\n    # CLEAR MEMORY\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# OVERALL CV SCORE\nprint(\"-\" * 40)\nrmse_xgb = mean_squared_error(y, oof_preds_xgb)**0.5\nprint(f\"OOF RMSE XGB w/ Pseudo:       {rmse_xgb:.5f}\")\nprint()\n\n# XGBoost Feature Importance\nfig, ax = plt.subplots(figsize=(8, 8))  \nplot_importance(\n    model,\n    ax=ax,\n    importance_type=\"gain\", # or \"weight\", \"cover\"\n    max_num_features=40,\n    height=0.5,\n)\nax.set_title(\"XGB Feature Importance (last fold)\")\nfig.tight_layout()\nplt.show()\n\n# Compute CV Score\nrmse_xgb = mean_squared_error(y, oof_preds_xgb)**0.5\nprint(f\"OOF RMSE XGB w/ Pseudo:       {rmse_xgb:.5f}\")\n\nnp.save(\"oof_xgb\",oof_preds_xgb)\nnp.save(\"test_preds_xgb\",test_preds_xgb)\n\n# Add validation check before creating submission CSV\nif np.isnan(oof_preds_xgb).any() or np.isnan(test_preds_xgb).any():\n    print(\"Error: Saved predictions contain NaN values.\")\nelse:\n    # Create Submission CSV\n    sub = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\")\n    sub['exam_score'] = np.clip(test_preds_xgb,19.6,100)\n    sub.to_csv(\"submission.csv\",index=False)\n    sub.head()\n\n\nplt.hist(sub['exam_score'],bins=100)\nplt.title(\"XGB w/ Pseudo Test Preds\")\nplt.show()"
  },
  {
    "role": "assistant",
    "content": "I'll explain the code in simple terms.\n\n**What is the code trying to do?**\n\nThe code is trying to predict the exam scores of students based on some features like age, study hours, internet access, etc. It's using two machine learning models: TabM and XGBoost.\n\n**Step 1: Loading Data**\n\nThe code starts by loading three datasets: `train`, `test`, and `original`. The `train` dataset has the features and the actual exam scores, the `test` dataset has the features but no exam scores, and the `original` dataset is like the `train` dataset but comes from a different source.\n\n**Step 2: Feature Engineering**\n\nThe code creates a new feature called \"formula\" based on some of the existing features. It also converts the categorical features (like \"gender\" and \"course\") into numerical values.\n\n**Step 3: Training TabM Model**\n\nThe code trains a TabM model on the `train` dataset and the `original` dataset. TabM is a type of machine learning model that's good at predicting continuous values like exam scores.\n\n**Step 4: Creating Pseudo Labels**\n\nThe TabM model predicts the exam scores for the `validation` and `test` datasets. These predicted scores are called \"pseudo labels\" because they're not the actual exam scores.\n\n**Step 5: Training XGBoost Model**\n\nThe code trains an XGBoost model on the `train` dataset, the `validation` dataset, and the `test` dataset, using the pseudo labels created by the TabM model. XGBoost is another type of machine learning model that's good at predicting continuous values like exam scores.\n\n**Step 6: Evaluating the Models**\n\nThe code evaluates the performance of the XGBoost model on the `validation` dataset and prints out the Root Mean Squared Error (RMSE).\n\n**Step 7: Creating Submission CSV**\n\nFinally, the code creates a submission CSV file with the predicted exam scores for the `test` dataset. It clips the predicted scores to be between 19.6 and 100.\n\n**What's the point of using two models?**\n\nThe point of using two models is to leverage the strengths of each model. TabM is good at predicting continuous values, and XGBoost is good at handling complex interactions between features. By using the pseudo labels created by the TabM model to train the XGBoost model, the code is able to combine the strengths of both models and make more accurate predictions.\n\nI hope this explanation helps! Let me know if you have any further questions."
  },
  {
    "role": "user",
    "content": "So you're telling me that training the XGB model on the TabM's output somehow creates a model that is better than the TabM?"
  },
  {
    "role": "user",
    "content": "Here's my current code:\n# XGBoost with TabM Pseudo Labels!\n\n# In this starter notebook, we demonstrate the following\n\n#     TabM model\n#     XGBoost model\n#     Basic feature engineering\n#     Pseudo label validation and test data\n#     Train XGB with TabM pseudo label data\n\n\n# Import Libraries\n\n!pip install pytabkit\n\nimport pandas as pd, numpy as np, gc\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nimport torch\nfrom pytabkit import TabM_D_Regressor\n\nimport xgboost as xgb\nprint(f\"XGBoost version: {xgb.__version__}\")\n\nimport importlib.metadata\nprint(f\"TabM version: {importlib.metadata.version('pytabkit')}\")\n# Load Data\n\n# We load train, test, and original dataset.\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s6e1/train.csv\")\nprint(\"Train shape:\", train.shape )\ntrain.head(1)\n\ntest = pd.read_csv(\"/kaggle/input/playground-series-s6e1/test.csv\")\nprint(\"Test shape:\", test.shape )\ntest.head(1)\n\norig = pd.read_csv(\"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\")\nprint(\"Original data shape:\", orig.shape )\norig.head(1)\n\n# Basic Feature Engineering\n\n# We engineer two things. First, we create a new feature called \"formula\" based on Kaggler Broccoli Beef's discussion here. Second, we encode the given 11 features as both ordinal numbers and categories for XGB's default categorical handling and TabM's categorical handling.\nTARGET = \"exam_score\"\nCATS = [\"gender\",\"course\",\"internet_access\",\"study_method\",\"sleep_quality\",\"facility_rating\",\"exam_difficulty\"]\nFEATURES = ['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\nprint(f\"There are {len(FEATURES)} features.\")\n\n# https://www.kaggle.com/competitions/playground-series-s6e1/discussion/665915\ndef formula(df):\n    f = (6*df.study_hours + 0.35*df.class_attendance + 1.5*df.sleep_hours +\n                 5*(df.sleep_quality=='good') + -5*(df.sleep_quality=='poor') +\n                 10*(df.study_method=='coaching') + 5*(df.study_method=='mixed') + 2*(df.study_method=='group study') + 1*(df.study_method=='online videos') +\n                 4*(df.facility_rating=='high') + -4*(df.facility_rating=='low') )\n    return f\n\nfor df in [train, test, orig]:\n    df['formula'] = formula(df)\n\nordinal_maps = {\n    \"gender\" : {\"male\":0, \"female\":1, \"other\":2},\n    \"internet_access\" : {\"no\":0, \"yes\":1},\n    \"sleep_quality\" : {\"poor\":0, \"average\":1, \"good\":2},\n    \"facility_rating\" : {\"low\":0, \"medium\":1, \"high\":2},\n    \"exam_difficulty\" : {\"easy\":0, \"moderate\":1, \"hard\":2},\n    \"course\" : {\"ba\":0, \"b.sc\":1, \"diploma\":2, \"b.tech\":3, \"b.com\":4, \"bca\":5, \"bba\":6},\n    \"study_method\" : {\"self-study\":0, \"online videos\":1, \"group study\":2, \"mixed\":3, \"coaching\":4},\n}\n\nfor df in [train, test, orig]:\n    for c in CATS:\n        df[c] = df[c].map(ordinal_maps[c]).fillna(-1).astype('int32')\n\nNEW_CATS = []\nfor c in FEATURES:\n    n = f\"CAT_{c}\"\n    NEW_CATS.append(n)\n    \n    train[n] = train[c].copy()\n    test[n] = test[c].copy()\n    orig[n] = orig[c].copy()\n\n    combine = pd.concat([train[n],test[n],orig[n]],axis=0)\n    v,_ = combine.factorize()\n    train[n] = v[:len(train)]\n    test[n] = v[len(train):len(train)+len(test)]\n    orig[n] = v[len(train)+len(test):]\n    for df in [train, test, orig]:\n        df[n] = df[n].astype(\"int32\")\n\nFEATURES += ['formula'] + NEW_CATS\nprint(f\"Now there are {len(FEATURES)} features.\")\n\n# XGBoost w/ TabM Pseudo Labels\n\n# We train XGBoost using pseudo labels on validation and test data from TabM.\nxgb_params = {\n    \"n_estimators\": 10_000,           \n    \"learning_rate\": 0.01,          \n    \"max_depth\": 8,                  \n    \"subsample\": 0.8,               \n    \"colsample_bytree\": 0.35,        \n    \"min_child_weight\": 5,           \n    \"early_stopping_rounds\": 100,   \n    \"eval_metric\": \"rmse\",\n    \"enable_categorical\": True,\n    \"device\": \"cuda\",\n    \"random_state\": 42,\n}\n\ntabm_params = {\n    \"verbosity\": 0,\n    \"arch_type\": \"tabm-mini-normal\",\n    \"tabm_k\": 24, \n    \"num_emb_type\": \"pwl\", \n    \"d_embedding\": 8,\n    \"batch_size\": 256,\n    \"lr\": 1e-3, \n    \"n_epochs\": 100, \n    \"dropout\": 0.11, \n    \"d_block\": 256, \n    \"n_blocks\": 5, \n    \"patience\": 4, \n    \"weight_decay\": 1e-2, \n    \"device\": \"cuda\",\n    \"random_state\": 42,\n}\n\nN_SPLITS = 10\n\n# PREPARE DATA\nX = train[FEATURES]\nX_test = test[FEATURES]\nX_orig = orig[FEATURES]\ny = train[TARGET].values\ny_orig = orig[TARGET].values\n\n# STORE OOF AND TEST PREDS\noof_preds_xgb = np.zeros(len(train))\ntest_preds_xgb = np.zeros(len(test))\n\n# KFOLD TRAIN AND VALIDATE\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n    print(\"#\"*25)\n    print(f\"### FOLD {fold}\")\n    print(\"#\"*25)\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    # ADD ORIGINAL DATA\n    X_train = pd.concat([X_train,X_orig],axis=0)\n    y_train = np.concatenate([y_train,y_orig],axis=0)\n\n    # TRAIN TABM\n    model = TabM_D_Regressor(**tabm_params)\n    model.fit(\n        X_train, y_train,\n        X_val, y_val,\n        cat_col_names=NEW_CATS,\n    )\n    oof_preds = model.predict(X_val)\n    test_preds = model.predict(X_test)\n\n    # PSEUDO LABEL WITH TABM FOR XGB\n    X_train_xgb = pd.concat([X_train,X_val,X_test],axis=0)\n    y_train_xgb = np.concatenate([y_train,oof_preds,test_preds],axis=0)\n\n    # PREPARE CATS FOR XGB\n    for c in NEW_CATS:\n        X_train_xgb[c] = X_train_xgb[c].astype('category')\n    X_val_xgb = X_train_xgb.iloc[len(X_train):len(X_train)+len(X_val)]\n    X_test_xgb = X_train_xgb.iloc[len(X_train)+len(X_val):]\n\n    # TRAIN XGB WITH PSEUDO LABELS\n    model = XGBRegressor(**xgb_params)\n    model.fit(\n        X_train_xgb,\n        y_train_xgb,\n        eval_set=[(X_val_xgb, y_val)],\n        verbose=200,\n    )\n    oof_preds = model.predict(X_val_xgb)\n    oof_preds_xgb[val_idx] = oof_preds\n\n    rmse_xgb = mean_squared_error(y_val, oof_preds)**0.5\n    print(f\"Fold {fold} RMSE XGB w/ Pseudo: {rmse_xgb:.5f}\")\n\n    test_preds = model.predict(X_test_xgb)\n    test_preds_xgb += test_preds/N_SPLITS\n\n    # CLEAR MEMORY\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# OVERALL CV SCORE\nprint(\"-\" * 40)\nrmse_xgb = mean_squared_error(y, oof_preds_xgb)**0.5\nprint(f\"OOF RMSE XGB w/ Pseudo:       {rmse_xgb:.5f}\")\nprint()\n\n# XGBoost Feature Importance\nfig, ax = plt.subplots(figsize=(8, 8))  \nplot_importance(\n    model,\n    ax=ax,\n    importance_type=\"gain\", # or \"weight\", \"cover\"\n    max_num_features=40,\n    height=0.5,\n)\nax.set_title(\"XGB Feature Importance (last fold)\")\nfig.tight_layout()\nplt.show()\n\n# Compute CV Score\nrmse_xgb = mean_squared_error(y, oof_preds_xgb)**0.5\nprint(f\"OOF RMSE XGB w/ Pseudo:       {rmse_xgb:.5f}\")\n\nnp.save(\"oof_xgb\",oof_preds_xgb)\nnp.save(\"test_preds_xgb\",test_preds_xgb)\n\n# Add validation check before creating submission CSV\nif np.isnan(oof_preds_xgb).any() or np.isnan(test_preds_xgb).any():\n    print(\"Error: Saved predictions contain NaN values.\")\nelse:\n    # Create Submission CSV\n    sub = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\")\n    sub['exam_score'] = np.clip(test_preds_xgb,19.6,100)\n    sub.to_csv(\"submission.csv\",index=False)\n    sub.head()\n\n\nplt.hist(sub['exam_score'],bins=100)\nplt.title(\"XGB w/ Pseudo Test Preds\")\nplt.show()"
  },
  {
    "role": "assistant",
    "content": "Yes, that's correct. The idea behind using two models is to leverage the strengths of each model.\n\n**Why is the XGB model potentially better?**\n\nIn this case, the XGB model is trained on the pseudo labels generated by the TabM model, which are predictions for the validation and test sets. By using these pseudo labels, the XGB model can learn from the strengths of the TabM model, such as its ability to handle complex interactions between features.\n\nAdditionally, XGB models are particularly good at:\n\n1. **Handling overfitting**: XGB models have built-in regularization mechanisms that help prevent overfitting, which can occur when a model is too complex and performs well on the training data but poorly on new, unseen data.\n2. **Handling categorical features**: XGB models can handle categorical features out of the box, which can be particularly useful when dealing with features that have many categories.\n3. **Interpreting feature importance**: XGB models provide feature importance scores, which can help identify which features are most important for making predictions.\n\nBy combining the strengths of the TabM model (e.g., handling complex interactions) with the strengths of the XGB model (e.g., handling overfitting, categorical features, and feature importance), the resulting XGB model may be able to make more accurate predictions than either model alone.\n\n**However, it's not a guarantee**\n\nIt's worth noting that using two models in this way is not a guarantee of better performance. The performance of the XGB model will depend on many factors, including the quality of the pseudo labels generated by the TabM model, the hyperparameters used to train the XGB model, and the specific characteristics of the dataset.\n\nIn general, using multiple models in this way is a common technique in machine learning, known as **model stacking** or **ensemble learning**. The idea is to combine the predictions of multiple models to produce a single, more accurate prediction."
  },
  {
    "role": "user",
    "content": "Please fix this: /usr/local/lib/python3.12/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn("
  },
  {
    "role": "user",
    "content": "Here's my current code:\n# XGBoost with TabM Pseudo Labels!\n\n# In this starter notebook, we demonstrate the following\n\n#     TabM model\n#     XGBoost model\n#     Basic feature engineering\n#     Pseudo label validation and test data\n#     Train XGB with TabM pseudo label data\n\n\n# Import Libraries\n\n!pip install pytabkit\n\nimport pandas as pd, numpy as np, gc\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nimport torch\nfrom pytabkit import TabM_D_Regressor\n\nimport xgboost as xgb\nprint(f\"XGBoost version: {xgb.__version__}\")\n\nimport importlib.metadata\nprint(f\"TabM version: {importlib.metadata.version('pytabkit')}\")\n# Load Data\n\n# We load train, test, and original dataset.\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s6e1/train.csv\")\nprint(\"Train shape:\", train.shape )\ntrain.head(1)\n\ntest = pd.read_csv(\"/kaggle/input/playground-series-s6e1/test.csv\")\nprint(\"Test shape:\", test.shape )\ntest.head(1)\n\norig = pd.read_csv(\"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\")\nprint(\"Original data shape:\", orig.shape )\norig.head(1)\n\n# Basic Feature Engineering\n\n# We engineer two things. First, we create a new feature called \"formula\" based on Kaggler Broccoli Beef's discussion here. Second, we encode the given 11 features as both ordinal numbers and categories for XGB's default categorical handling and TabM's categorical handling.\nTARGET = \"exam_score\"\nCATS = [\"gender\",\"course\",\"internet_access\",\"study_method\",\"sleep_quality\",\"facility_rating\",\"exam_difficulty\"]\nFEATURES = ['age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\nprint(f\"There are {len(FEATURES)} features.\")\n\n# https://www.kaggle.com/competitions/playground-series-s6e1/discussion/665915\ndef formula(df):\n    f = (6*df.study_hours + 0.35*df.class_attendance + 1.5*df.sleep_hours +\n                 5*(df.sleep_quality=='good') + -5*(df.sleep_quality=='poor') +\n                 10*(df.study_method=='coaching') + 5*(df.study_method=='mixed') + 2*(df.study_method=='group study') + 1*(df.study_method=='online videos') +\n                 4*(df.facility_rating=='high') + -4*(df.facility_rating=='low') )\n    return f\n\nfor df in [train, test, orig]:\n    df['formula'] = formula(df)\n\nordinal_maps = {\n    \"gender\" : {\"male\":0, \"female\":1, \"other\":2},\n    \"internet_access\" : {\"no\":0, \"yes\":1},\n    \"sleep_quality\" : {\"poor\":0, \"average\":1, \"good\":2},\n    \"facility_rating\" : {\"low\":0, \"medium\":1, \"high\":2},\n    \"exam_difficulty\" : {\"easy\":0, \"moderate\":1, \"hard\":2},\n    \"course\" : {\"ba\":0, \"b.sc\":1, \"diploma\":2, \"b.tech\":3, \"b.com\":4, \"bca\":5, \"bba\":6},\n    \"study_method\" : {\"self-study\":0, \"online videos\":1, \"group study\":2, \"mixed\":3, \"coaching\":4},\n}\n\nfor df in [train, test, orig]:\n    for c in CATS:\n        df[c] = df[c].map(ordinal_maps[c]).fillna(-1).astype('int32')\n\nNEW_CATS = []\nfor c in FEATURES:\n    n = f\"CAT_{c}\"\n    NEW_CATS.append(n)\n    \n    train[n] = train[c].copy()\n    test[n] = test[c].copy()\n    orig[n] = orig[c].copy()\n\n    combine = pd.concat([train[n],test[n],orig[n]],axis=0)\n    v,_ = combine.factorize()\n    train[n] = v[:len(train)]\n    test[n] = v[len(train):len(train)+len(test)]\n    orig[n] = v[len(train)+len(test):]\n    for df in [train, test, orig]:\n        df[n] = df[n].astype(\"int32\")\n\nFEATURES += ['formula'] + NEW_CATS\nprint(f\"Now there are {len(FEATURES)} features.\")\n\n# XGBoost w/ TabM Pseudo Labels\n\n# We train XGBoost using pseudo labels on validation and test data from TabM.\nxgb_params = {\n    \"n_estimators\": 10_000,           \n    \"learning_rate\": 0.01,          \n    \"max_depth\": 8,                  \n    \"subsample\": 0.8,               \n    \"colsample_bytree\": 0.35,        \n    \"min_child_weight\": 5,           \n    \"early_stopping_rounds\": 100,   \n    \"eval_metric\": \"rmse\",\n    \"enable_categorical\": True,\n    \"device\": \"cuda\",\n    \"random_state\": 42,\n}\n\ntabm_params = {\n    \"verbosity\": 0,\n    \"arch_type\": \"tabm-mini-normal\",\n    \"tabm_k\": 24, \n    \"num_emb_type\": \"pwl\", \n    \"d_embedding\": 8,\n    \"batch_size\": 256,\n    \"lr\": 1e-3, \n    \"n_epochs\": 100, \n    \"dropout\": 0.11, \n    \"d_block\": 256, \n    \"n_blocks\": 5, \n    \"patience\": 4, \n    \"weight_decay\": 1e-2, \n    \"device\": \"cuda\",\n    \"random_state\": 42,\n}\n\nN_SPLITS = 10\n\n# PREPARE DATA\nX = train[FEATURES]\nX_test = test[FEATURES]\nX_orig = orig[FEATURES]\ny = train[TARGET].values\ny_orig = orig[TARGET].values\n\n# STORE OOF AND TEST PREDS\noof_preds_xgb = np.zeros(len(train))\ntest_preds_xgb = np.zeros(len(test))\n\n# KFOLD TRAIN AND VALIDATE\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n    print(\"#\"*25)\n    print(f\"### FOLD {fold}\")\n    print(\"#\"*25)\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    # ADD ORIGINAL DATA\n    X_train = pd.concat([X_train,X_orig],axis=0)\n    y_train = np.concatenate([y_train,y_orig],axis=0)\n\n    # TRAIN TABM\n    model = TabM_D_Regressor(**tabm_params)\n    model.fit(\n        X_train, y_train,\n        X_val, y_val,\n        cat_col_names=NEW_CATS,\n    )\n    oof_preds = model.predict(X_val)\n    test_preds = model.predict(X_test)\n\n    # PSEUDO LABEL WITH TABM FOR XGB\n    X_train_xgb = pd.concat([X_train,X_val,X_test],axis=0)\n    y_train_xgb = np.concatenate([y_train,oof_preds,test_preds],axis=0)\n\n    # PREPARE CATS FOR XGB\n    for c in NEW_CATS:\n        X_train_xgb[c] = X_train_xgb[c].astype('category')\n    X_val_xgb = X_train_xgb.iloc[len(X_train):len(X_train)+len(X_val)]\n    X_test_xgb = X_train_xgb.iloc[len(X_train)+len(X_val):]\n\n    # TRAIN XGB WITH PSEUDO LABELS\n    model = XGBRegressor(**xgb_params)\n    model.fit(\n        X_train_xgb,\n        y_train_xgb,\n        eval_set=[(X_val_xgb, y_val)],\n        verbose=200,\n    )\n    oof_preds = model.predict(X_val_xgb)\n    oof_preds_xgb[val_idx] = oof_preds\n\n    rmse_xgb = mean_squared_error(y_val, oof_preds)**0.5\n    print(f\"Fold {fold} RMSE XGB w/ Pseudo: {rmse_xgb:.5f}\")\n\n    test_preds = model.predict(X_test_xgb)\n    test_preds_xgb += test_preds/N_SPLITS\n\n    # CLEAR MEMORY\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# OVERALL CV SCORE\nprint(\"-\" * 40)\nrmse_xgb = mean_squared_error(y, oof_preds_xgb)**0.5\nprint(f\"OOF RMSE XGB w/ Pseudo:       {rmse_xgb:.5f}\")\nprint()\n\n# XGBoost Feature Importance\nfig, ax = plt.subplots(figsize=(8, 8))  \nplot_importance(\n    model,\n    ax=ax,\n    importance_type=\"gain\", # or \"weight\", \"cover\"\n    max_num_features=40,\n    height=0.5,\n)\nax.set_title(\"XGB Feature Importance (last fold)\")\nfig.tight_layout()\nplt.show()\n\n# Compute CV Score\nrmse_xgb = mean_squared_error(y, oof_preds_xgb)**0.5\nprint(f\"OOF RMSE XGB w/ Pseudo:       {rmse_xgb:.5f}\")\n\nnp.save(\"oof_xgb\",oof_preds_xgb)\nnp.save(\"test_preds_xgb\",test_preds_xgb)\n\n# Add validation check before creating submission CSV\nif np.isnan(oof_preds_xgb).any() or np.isnan(test_preds_xgb).any():\n    print(\"Error: Saved predictions contain NaN values.\")\nelse:\n    # Create Submission CSV\n    sub = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\")\n    sub['exam_score'] = np.clip(test_preds_xgb,19.6,100)\n    sub.to_csv(\"submission.csv\",index=False)\n    sub.head()\n\n\nplt.hist(sub['exam_score'],bins=100)\nplt.title(\"XGB w/ Pseudo Test Preds\")\nplt.show()"
  },
  {
    "role": "assistant",
    "content": "The warning you're seeing is due to a deprecation in scikit-learn, which is being used by the TabM model. \n\nIn scikit-learn version 1.6, the `force_all_finite` parameter was renamed to `ensure_all_finite`. The warning is indicating that `force_all_finite` will be removed in version 1.8.\n\nTo fix this, you can either:\n\n1. Downgrade scikit-learn to a version earlier than 1.6, or\n2. Upgrade the TabM model to use `ensure_all_finite` instead of `force_all_finite`, if it's available.\n\nIt looks like the TabM model is being installed via `!pip install pytabkit`, so you might need to wait for an update to the pytabkit library that uses the new `ensure_all_finite` parameter.\n\nFor now, you can suppress the warning by adding the following code to the top of your script:\n```python\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n```\nHowever, keep in mind that suppressing warnings is generally not a good practice, as it can hide potential issues with your code.\n\nAlternatively, you can update your scikit-learn version to the latest one (after 1.6) and use the following code:\n```python\nimport numpy as np\nfrom sklearn.utils import check_array\nfrom sklearn.exceptions import DataConversionWarning\n\ncheck_array(np.array([1, 2, 3]), ensure_all_finite=True)\n```\nThis way, you can use the latest version of scikit-learn and the `ensure_all_finite` parameter. \n\nHowever, it seems like you don't have direct control over the code in the TabM model. So, in this case, suppressing the warning is the only option you have. \n\nAlso, note that you might want to report this issue to the developers of the TabM model so they can update their library to use the new `ensure_all_finite` parameter."
  }
]