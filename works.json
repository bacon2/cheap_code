[
  {
    "role": "system",
    "content": "You are a helpful assistant."
  },
  {
    "role": "user",
    "content": "Make 1 improvement to your UI's code."
  },
  {
    "role": "user",
    "content": "Here's my current code:\nimport os\nimport re\nimport threading\nimport queue\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nimport json\n\n# -----------------------------\n# Environment / Client Setup\n# -----------------------------\n\nload_dotenv()\n\ngroq_client = OpenAI(\n    api_key=os.getenv(\"GROQ_API_KEY\"),\n    base_url=\"https://api.groq.com/openai/v1\",\n)\n\nopenai_client = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\n# Model configurations\nGROQ_MODELS = [\n    \"llama-3.3-70b-versatile\",\n    \"moonshotai/kimi-k2-instruct\",\n    \"moonshotai/kimi-k2-instruct-0905\",\n    \"openai/gpt-oss-120b\",\n    \"openai/gpt-oss-20b\",\n    \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n]\n\nOPENAI_MODELS = [\n    \"gpt-5.2\",\n    \"gpt-5-mini\",\n    \"gpt-4.1\",\n]\n\nALL_MODELS = GROQ_MODELS + OPENAI_MODELS\n\nCODE_BLOCK_REGEX = re.compile(\n    r\"```(?:\\w+)?\\s*(.*?)```\",\n    re.DOTALL\n)\n\n# -----------------------------\n# UI\n# -----------------------------\nclass CodeProposal(ttk.Frame):\n    def __init__(self, parent, code: str, on_replace, on_add):\n        super().__init__(parent)\n        self.code = code\n\n        # Calculate height: number of lines + 1\n        line_count = code.count(\"\\n\") + 1\n        \n        self.text = tk.Text(\n            self,\n            height=line_count,\n            wrap=\"none\",\n            font=(\"Courier New\", 10),\n            padx=5,\n            pady=5,\n            highlightthickness=1 # Border for code blocks\n        )\n        self.text.insert(\"1.0\", code)\n        self.text.configure(state=\"disabled\")\n        self.text.pack(fill=\"x\", padx=5, pady=(5, 2))\n\n        button_row = ttk.Frame(self)\n        button_row.pack(anchor=\"e\", padx=5, pady=(0, 5))\n\n        ttk.Button(button_row, text=\"Replace\", command=lambda: on_replace(code)).pack(side=\"right\", padx=2)\n        ttk.Button(button_row, text=\"Add\", command=lambda: on_add(code)).pack(side=\"right\", padx=2)\n\nclass ChatUI:\n    def __init__(self, root: tk.Tk):\n        self.root = root\n        self.root.title(\"Chat + Code UI\")\n        self.root.geometry(\"1100x650\")\n\n        self.conversation = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n        ]\n\n        self.token_queue = queue.Queue()\n        self.is_generating = False\n        self.current_assistant_buffer = \"\"\n\n        self.code_contents = \"\"\n\n        # ---- persistence helpers ----\n        self.auto_save_path: str | None = None\n        self.auto_save_after_id: str | None = None\n        self.auto_save_interval_ms = 30_000\n        \n        # Default models\n        self.chat_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n        self.helper_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n\n        self._build_layout()\n        self._bind_keys()\n        self._poll_tokens()\n\n    # -----------------------------\n    # Layout\n    # -----------------------------\n\n    def _build_layout(self) -> None:\n        self.root.rowconfigure(0, weight=0)\n        self.root.rowconfigure(1, weight=1)\n        self.root.rowconfigure(2, weight=0)\n        self.root.columnconfigure(0, weight=1)\n\n        # ----------------------\n        # Model Selection Bar\n        # ----------------------\n        model_frame = ttk.Frame(self.root)\n        model_frame.grid(row=0, column=0, sticky=\"ew\", padx=5, pady=5)\n        \n        ttk.Label(model_frame, text=\"Chat Model:\").pack(side=\"left\", padx=(0, 5))\n        chat_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.chat_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        chat_dropdown.pack(side=\"left\", padx=(0, 20))\n        \n        ttk.Label(model_frame, text=\"Helper Model:\").pack(side=\"left\", padx=(0, 5))\n        helper_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.helper_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        helper_dropdown.pack(side=\"left\")\n\n        # ----------------------\n        # Menu bar\n        # ----------------------\n        menubar = tk.Menu(self.root)\n        file_menu = tk.Menu(menubar, tearoff=0)\n        file_menu.add_command(label=\"Save conversation…\", command=self.save_conversation)\n        file_menu.add_command(label=\"Load conversation…\", command=self.load_conversation)\n        file_menu.add_separator()\n        file_menu.add_command(label=\"Export markdown…\", command=self.export_markdown)\n        menubar.add_cascade(label=\"File\", menu=file_menu)\n        self.root.config(menu=menubar)\n\n        main_pane = ttk.PanedWindow(self.root, orient=\"horizontal\")\n        main_pane.grid(row=1, column=0, sticky=\"nsew\")\n\n        # ----------------------\n        # Chat + Code Proposals\n        # ----------------------\n        chat_frame = ttk.Frame(main_pane)\n        chat_frame.rowconfigure(0, weight=1)\n        chat_frame.columnconfigure(0, weight=1)\n\n        # Scrollable canvas\n        self.chat_canvas = tk.Canvas(chat_frame)\n        self.chat_scrollbar = ttk.Scrollbar(chat_frame, command=self.chat_canvas.yview)\n        self.chat_canvas.configure(yscrollcommand=self.chat_scrollbar.set)\n\n        self.chat_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n        self.chat_canvas.grid(row=0, column=0, sticky=\"nsew\")\n\n        # Inner frame to hold all messages\n        self.chat_inner_frame = ttk.Frame(self.chat_canvas)\n        self.chat_canvas.create_window((0, 0), window=self.chat_inner_frame, anchor=\"nw\")\n\n        # Update scroll region when content changes\n        self.chat_inner_frame.bind(\n            \"<Configure>\",\n            lambda e: self.chat_canvas.configure(scrollregion=self.chat_canvas.bbox(\"all\"))\n        )\n\n        # Track the current message label for streaming\n        self.current_message_label = None\n        \n        # Get background color from a standard widget\n        self.bg_color = self.root.cget(\"background\")\n\n        # ----------------------\n        # Code Panel\n        # ----------------------\n        code_frame = ttk.Frame(main_pane)\n        code_frame.rowconfigure(0, weight=1)\n        code_frame.columnconfigure(0, weight=1)\n\n        self.code_panel = tk.Text(code_frame, wrap=\"none\")\n        self.code_panel.grid(row=0, column=0, sticky=\"nsew\")\n        \n        # Bind to sync code_contents when user edits\n        self.code_panel.bind(\"<<Modified>>\", self._on_code_modified)\n\n        def select_all_code_panel(event):\n            event.widget.tag_add(\"sel\", \"1.0\", \"end\")\n            return \"break\"\n\n        self.code_panel.bind(\"<Control-a>\", select_all_code_panel)\n        self.code_panel.bind(\"<Control-A>\", select_all_code_panel)\n        self.code_panel.bind(\"<Command-a>\", select_all_code_panel)  # For Mac\n\n        code_scroll = ttk.Scrollbar(code_frame, command=self.code_panel.yview)\n        code_scroll.grid(row=0, column=1, sticky=\"ns\")\n        self.code_panel[\"yscrollcommand\"] = code_scroll.set\n\n        main_pane.add(chat_frame)\n        main_pane.add(code_frame)\n\n        # ----------------------\n        # Input Box\n        # ----------------------\n        self.input_box = tk.Text(self.root, height=5, wrap=\"word\")\n        self.input_box.grid(row=2, column=0, sticky=\"nsew\")\n\n        self.root.update_idletasks()\n        main_pane.sashpos(0, int(self.root.winfo_width() * 0.70))\n    \n    def _get_client_for_model(self, model: str):\n        \"\"\"Return the appropriate client for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return openai_client\n        else:\n            return groq_client\n    \n    def _get_max_tokens_for_model(self, model: str):\n        \"\"\"Return appropriate max_tokens for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return 16000\n        else:\n            return 8000\n    \n    def _uses_max_completion_tokens(self, model: str):\n        \"\"\"Check if model uses max_completion_tokens instead of max_tokens.\"\"\"\n        return model in [\"gpt-5.2\", \"gpt-5-mini\"]\n\n    def _bind_keys(self):\n        self.input_box.bind(\"<Return>\", self._on_enter)\n        self.input_box.bind(\"<Shift-Return>\", lambda e: None)\n    \n    def _on_code_modified(self, event):\n        \"\"\"Sync code_contents when user edits the code panel.\"\"\"\n        if self.code_panel.edit_modified():\n            self.code_contents = self.code_panel.get(\"1.0\", \"end-1c\")\n            self.code_panel.edit_modified(False)\n\n    # -----------------------------\n    # Input\n    # -----------------------------\n\n    def _on_enter(self, event):\n        if self.is_generating:\n            return \"break\"\n\n        text = self.input_box.get(\"1.0\", \"end-1c\").strip()\n        if not text:\n            return \"break\"\n\n        self.input_box.delete(\"1.0\", \"end\")\n        self._append_chat(\"You\", text)\n        \n        # Add user message to conversation\n        self.conversation.append({\"role\": \"user\", \"content\": text})\n        \n        # Add invisible context about current code if it exists\n        if self.code_contents.strip():\n            self.conversation.append({\n                \"role\": \"user\",\n                \"content\": f\"Here's my current code:\\n{self.code_contents}\"\n            })\n\n        self._start_generation()\n        return \"break\"\n\n    # -----------------------------\n    # Chat Generation\n    # -----------------------------\n\n    def _start_generation(self):\n        self.is_generating = True\n        self.input_box.configure(state=\"disabled\")\n        self.current_assistant_buffer = \"\"\n\n        threading.Thread(\n            target=self._stream_chat_response, daemon=True\n        ).start()\n\n    def _stream_chat_response(self):\n        try:\n            model = self.chat_model.get()\n            client = self._get_client_for_model(model)\n            max_tokens = self._get_max_tokens_for_model(model)\n            \n            # Build request parameters\n            params = {\n                \"model\": model,\n                \"messages\": self.conversation,\n                \"stream\": True,\n            }\n            \n            # Use max_completion_tokens for GPT-5.2 and GPT-5-mini\n            if self._uses_max_completion_tokens(model):\n                params[\"max_completion_tokens\"] = max_tokens\n            else:\n                params[\"max_tokens\"] = max_tokens\n            \n            stream = client.chat.completions.create(**params)\n\n            for chunk in stream:\n                delta = chunk.choices[0].delta.content or \"\"\n                if delta:\n                    self.token_queue.put(delta)\n\n        except Exception as e:\n            self.token_queue.put(f\"\\n[Error: {e}]\")\n\n        finally:\n            self.token_queue.put(None)\n\n    def _poll_tokens(self):\n        try:\n            while True:\n                token = self.token_queue.get_nowait()\n\n                if token is None:\n                    self._finalize_assistant_message()\n                    break\n\n                self.current_assistant_buffer += token\n                self._append_to_last_chat(token)\n\n        except queue.Empty:\n            pass\n\n        self.root.after(20, self._poll_tokens)\n\n    def _finalize_assistant_message(self):\n        assistant_text = self.current_assistant_buffer\n        self.conversation.append({\"role\": \"assistant\", \"content\": assistant_text})\n\n        self.is_generating = False\n        self.input_box.configure(state=\"normal\")\n\n        # Replace the streaming label with parsed content\n        self._replace_with_parsed_message(assistant_text)\n\n    def _replace_with_parsed_message(self, text: str):\n        \"\"\"Parse the message and replace code blocks with interactive widgets.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.destroy()\n            self.current_message_label = None\n\n        # Create a container for this message\n        message_container = ttk.Frame(self.chat_inner_frame)\n        message_container.pack(fill=\"x\", padx=5, pady=5)\n\n        # Add sender as copyable text\n        sender_text = tk.Text(\n            message_container,\n            height=1,\n            wrap=\"none\",\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            background=self.bg_color\n        )\n        sender_text.insert(\"1.0\", \"Assistant:\")\n        sender_text.configure(state=\"disabled\")\n        sender_text.pack(fill=\"x\")\n\n        # Split text by code blocks\n        parts = self._split_text_and_code(text)\n        \n        for part_type, content in parts:\n            if part_type == \"text\":\n                if content.strip():\n                    text_widget = tk.Text(\n                        message_container,\n                        height=content.count(\"\\n\") + 1 + int(len(content) * 0.012),\n                        wrap=\"word\",\n                        relief=\"flat\",\n                        background=self.bg_color\n                    )\n                    text_widget.insert(\"1.0\", content)\n                    text_widget.configure(state=\"disabled\")\n                    text_widget.pack(fill=\"x\", padx=5, pady=2)\n            elif part_type == \"code\":\n                proposal = CodeProposal(\n                    parent=message_container,\n                    code=content,\n                    on_replace=self._replace_code_panel,\n                    on_add=self._add_code_via_helper,\n                )\n                proposal.pack(fill=\"x\", pady=5)\n\n        # Scroll to bottom\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _split_text_and_code(self, text: str):\n        \"\"\"Split text into alternating text and code blocks.\"\"\"\n        parts = []\n        last_end = 0\n        \n        for match in CODE_BLOCK_REGEX.finditer(text):\n            # Add text before code block\n            if match.start() > last_end:\n                parts.append((\"text\", text[last_end:match.start()]))\n            \n            # Add code block\n            parts.append((\"code\", match.group(1)))\n            last_end = match.end()\n        \n        # Add remaining text after last code block\n        if last_end < len(text):\n            parts.append((\"text\", text[last_end:]))\n        \n        return parts\n\n    # -----------------------------\n    # Code Handling (Line-Number Based Approach)\n    # -----------------------------\n\n    def _add_line_numbers(self, code: str) -> str:\n        \"\"\"Prepend line numbers to each line of code.\"\"\"\n        lines = code.split(\"\\n\")\n        numbered_lines = [f\"{i+1} | {line}\" for i, line in enumerate(lines)]\n        return \"\\n\".join(numbered_lines)\n\n    def _add_code_via_helper(self, new_code: str):\n        \"\"\"Ask helper model for insertion point and lines to delete, then apply.\"\"\"\n        if not self.code_contents.strip():\n            self._replace_code_panel(new_code)\n            return\n\n        # Get insertion instructions from helper model\n        insertion_data = self._get_insertion_instructions(new_code)\n        \n        if not insertion_data:\n            self._append_chat(\"System\", \"Helper model did not return valid instructions.\")\n            return\n\n        # Try to apply the insertion and deletion\n        try:\n            updated_code = self._apply_line_based_edit(\n                insertion_data[\"insertion_point\"],\n                insertion_data[\"delete_lines\"],\n                new_code,\n                insertion_data[\"indent_spaces\"]\n            )\n            self.code_contents = updated_code\n            self.code_panel.delete(\"1.0\", \"end\")\n            self.code_panel.insert(\"1.0\", updated_code)\n            self._append_chat(\"System\", \"Code updated successfully!\")\n        except Exception as e:\n            self._append_chat(\"System\", f\"Edit failed: {e}\")\n\n    def _get_insertion_instructions(self, new_code: str) -> dict | None:\n        \"\"\"Ask helper model for insertion point and lines to delete using line numbers.\"\"\"\n        # Add line numbers to current code\n        numbered_code = self._add_line_numbers(self.code_contents)\n        \n        system_prompt = (\n            \"You are a code editing assistant. Your job is to determine where to insert new code \"\n            \"and which lines (if any) to delete from the existing code.\\n\\n\"\n            \"You will receive:\\n\"\n            \"1. The current code with line numbers (format: '1 | code here')\\n\"\n            \"2. New code to insert\\n\\n\"\n            \"You must respond ONLY with JSON in this exact format:\\n\"\n            \"{\\n\"\n            '  \"insertion_point\": <line_number>,\\n'\n            '  \"delete_lines\": [<line_numbers>],\\n'\n            '  \"indent_spaces\": <number_of_spaces>\\n'\n            \"}\\n\\n\"\n            \"Rules:\\n\"\n            \"- insertion_point: The line number AFTER which to insert the new code (use 0 to insert at the beginning)\\n\"\n            \"- delete_lines: Array of line numbers to delete (can be empty [] if just inserting)\\n\"\n            \"- indent_spaces: Number of spaces to add to the beginning of each line of inserted code (to match surrounding indentation)\\n\"\n            \"- Use the CURRENT line numbers you see - don't worry about how insertion affects numbering\\n\"\n            \"- If replacing code, include those line numbers in delete_lines\\n\"\n            \"- If just inserting, leave delete_lines empty\\n\"\n            \"- Look at the indentation of surrounding code to determine indent_spaces\\n\\n\"\n            \"Example 1 - Replace lines 5-7 with 8 spaces of indentation:\\n\"\n            '{\"insertion_point\": 4, \"delete_lines\": [5, 6, 7], \"indent_spaces\": 8}\\n\\n'\n            \"Example 2 - Insert after line 10 with 4 spaces:\\n\"\n            '{\"insertion_point\": 10, \"delete_lines\": [], \"indent_spaces\": 4}\\n\\n'\n            \"Example 3 - Insert at beginning with no indentation:\\n\"\n            '{\"insertion_point\": 0, \"delete_lines\": [], \"indent_spaces\": 0}\\n\\n'\n            \"Respond with ONLY the JSON, no explanations.\"\n        )\n\n        user_prompt = (\n            \"CURRENT CODE (with line numbers):\\n\"\n            f\"{numbered_code}\\n\\n\"\n            \"NEW CODE TO INSERT:\\n\"\n            f\"{new_code}\\n\"\n        )\n\n        model = self.helper_model.get()\n        client = self._get_client_for_model(model)\n        max_tokens = self._get_max_tokens_for_model(model)\n        \n        params = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n        }\n        \n        if self._uses_max_completion_tokens(model):\n            params[\"max_completion_tokens\"] = max_tokens\n        else:\n            params[\"max_tokens\"] = max_tokens\n        \n        try:\n            response = client.chat.completions.create(**params)\n            response_text = response.choices[0].message.content or \"\"\n            \n            print(\"Helper response:\", response_text)\n            \n            # Extract JSON from response (in case model adds markdown)\n            json_match = re.search(r'\\{[^}]+\\}', response_text, re.DOTALL)\n            if json_match:\n                response_text = json_match.group(0)\n            \n            data = json.loads(response_text)\n            \n            # Validate response format\n            if \"insertion_point\" not in data or \"delete_lines\" not in data or \"indent_spaces\" not in data:\n                return None\n            \n            if not isinstance(data[\"insertion_point\"], int):\n                return None\n            \n            if not isinstance(data[\"delete_lines\"], list):\n                return None\n            \n            if not isinstance(data[\"indent_spaces\"], int):\n                return None\n            \n            return data\n            \n        except Exception as e:\n            print(f\"Error getting insertion instructions: {e}\")\n            return None\n\n    def _apply_line_based_edit(self, insertion_point: int, delete_lines: list, new_code: str, indent_spaces: int) -> str:\n        \"\"\"Apply insertion and deletion based on line numbers, with indentation.\"\"\"\n        lines = self.code_contents.split(\"\\n\")\n        \n        # Validate line numbers\n        if insertion_point < 0 or insertion_point > len(lines):\n            raise ValueError(f\"Invalid insertion point: {insertion_point}\")\n        \n        for line_num in delete_lines:\n            if line_num < 1 or line_num > len(lines):\n                raise ValueError(f\"Invalid line number to delete: {line_num}\")\n        \n        # Convert delete_lines to 0-indexed and sort in reverse order\n        delete_indices = sorted([line_num - 1 for line_num in delete_lines], reverse=True)\n        \n        # Adjust insertion point if we're deleting lines before it\n        adjusted_insertion = insertion_point\n        for idx in delete_indices:\n            if idx < insertion_point:\n                adjusted_insertion -= 1\n        \n        # Delete lines (in reverse order to preserve indices)\n        for idx in delete_indices:\n            del lines[idx]\n        \n        # Apply indentation to new code\n        indent = \" \" * indent_spaces\n        new_lines = new_code.split(\"\\n\")\n        indented_lines = [indent + line for line in new_lines]\n        \n        # Insert new code at the adjusted position\n        lines[adjusted_insertion:adjusted_insertion] = indented_lines\n        \n        return \"\\n\".join(lines)\n\n    def _replace_code_panel(self, code: str):\n        self.code_contents = code\n        self.code_panel.delete(\"1.0\", \"end\")\n        self.code_panel.insert(\"1.0\", code)\n\n    # -----------------------------\n    # Persistence\n    # -----------------------------\n    def save_conversation(self, path: str | None = None):\n        if path is None:\n            path = filedialog.asksaveasfilename(\n                defaultextension=\".json\",\n                filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n            )\n            if not path:\n                return\n        try:\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self.conversation, f, indent=2, ensure_ascii=False)\n            self.auto_save_path = path\n            messagebox.showinfo(\"Saved\", f\"Conversation saved to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Save error\", str(e))\n\n    def load_conversation(self):\n        path = filedialog.askopenfilename(\n            defaultextension=\".json\",\n            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                loaded = json.load(f)\n            if not isinstance(loaded, list):\n                raise ValueError(\"File must contain a list of messages.\")\n            self.conversation = loaded\n            self.auto_save_path = path\n            self._redraw_chat()\n            messagebox.showinfo(\"Loaded\", f\"Conversation loaded from\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Load error\", str(e))\n\n    def export_markdown(self):\n        path = filedialog.asksaveasfilename(\n            defaultextension=\".md\",\n            filetypes=[(\"Markdown files\", \"*.md\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            lines = []\n            for msg in self.conversation:\n                role = msg.get(\"role\", \"\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue\n                name = \"You\" if role == \"user\" else \"Assistant\"\n                lines.append(f\"**{name}:**\\n{content}\\n\")\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\\n\".join(lines))\n            messagebox.showinfo(\"Exported\", f\"Markdown exported to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Export error\", str(e))\n\n    def _redraw_chat(self):\n        for w in self.chat_inner_frame.winfo_children():\n            w.destroy()\n        self.current_message_label = None\n        for msg in self.conversation:\n            role = msg.get(\"role\")\n            content = msg.get(\"content\", \"\")\n            if role == \"system\":\n                continue\n            sender = \"You\" if role == \"user\" else \"Assistant\"\n            if role == \"assistant\":\n                self._replace_with_parsed_message(content)\n            else:\n                self._append_chat(sender, content)\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n\n    def _append_chat(self, sender: str, text: str):\n        \"\"\"Modified for tighter user message spacing.\"\"\"\n        container = ttk.Frame(self.chat_inner_frame)\n        container.pack(fill=\"x\", padx=10, pady=5)\n\n        header = tk.Text(container, height=1, font=(\"TkDefaultFont\", 10, \"bold\"),\n                        relief=\"flat\", background=self.bg_color, highlightthickness=0)\n        header.insert(\"1.0\", f\"{sender}:\")\n        header.configure(state=\"disabled\")\n        header.pack(fill=\"x\")\n\n        # Dynamic height for content\n        line_count = text.count(\"\\n\") + 1\n        body = tk.Text(container, height=line_count, wrap=\"word\", relief=\"flat\",\n                      background=self.bg_color, highlightthickness=0)\n        body.insert(\"1.0\", text)\n        body.configure(state=\"disabled\")\n        body.pack(fill=\"x\", padx=5)\n        \n        if sender == \"Assistant\":\n            self.current_message_label = body\n            self.current_message_container = container\n\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _append_to_last_chat(self, text: str):\n        \"\"\"Append text and grow the text widget height dynamically.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.configure(state=\"normal\")\n            self.current_message_label.insert(\"end\", text)\n            \n            line_count = int(self.current_message_label.index(\"end-1c\").split(\".\")[0])\n            self.current_message_label.configure(height=line_count, state=\"disabled\")\n            \n            self.chat_canvas.update_idletasks()\n            self.chat_canvas.yview_moveto(1.0)\n\n\nif __name__ == \"__main__\":\n    root = tk.Tk()\n    ChatUI(root)\n    root.mainloop()"
  },
  {
    "role": "assistant",
    "content": "One possible improvement is to add a `try-except` block in the `_poll_tokens` method to handle any potential exceptions that may occur while retrieving tokens from the queue. This can prevent the application from crashing unexpectedly.\n\nHere's the modified method:\n\n```python\ndef _poll_tokens(self):\n    try:\n        while True:\n            token = self.token_queue.get_nowait()\n\n            if token is None:\n                self._finalize_assistant_message()\n                break\n\n            self.current_assistant_buffer += token\n            self._append_to_last_chat(token)\n\n    except queue.Empty:\n        pass\n    except Exception as e:\n        print(f\"Error polling tokens: {e}\")\n\n    self.root.after(20, self._poll_tokens)\n```\n\nWith this modification, if an exception occurs while retrieving tokens from the queue, the error will be printed to the console and the application will continue running without crashing. You can also add more specific error handling as needed, depending on the type of exceptions that may occur in your application."
  },
  {
    "role": "user",
    "content": "Congratulations. For the first time, you were able to edit your own UI code without errors. Make another improvement."
  },
  {
    "role": "user",
    "content": "Here's my current code:\nimport os\nimport re\nimport threading\nimport queue\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nimport json\n\n# -----------------------------\n# Environment / Client Setup\n# -----------------------------\n\nload_dotenv()\n\ngroq_client = OpenAI(\n    api_key=os.getenv(\"GROQ_API_KEY\"),\n    base_url=\"https://api.groq.com/openai/v1\",\n)\n\nopenai_client = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\n# Model configurations\nGROQ_MODELS = [\n    \"llama-3.3-70b-versatile\",\n    \"moonshotai/kimi-k2-instruct\",\n    \"moonshotai/kimi-k2-instruct-0905\",\n    \"openai/gpt-oss-120b\",\n    \"openai/gpt-oss-20b\",\n    \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n]\n\nOPENAI_MODELS = [\n    \"gpt-5.2\",\n    \"gpt-5-mini\",\n    \"gpt-4.1\",\n]\n\nALL_MODELS = GROQ_MODELS + OPENAI_MODELS\n\nCODE_BLOCK_REGEX = re.compile(\n    r\"```(?:\\w+)?\\s*(.*?)```\",\n    re.DOTALL\n)\n\n# -----------------------------\n# UI\n# -----------------------------\nclass CodeProposal(ttk.Frame):\n    def __init__(self, parent, code: str, on_replace, on_add):\n        super().__init__(parent)\n        self.code = code\n\n        # Calculate height: number of lines + 1\n        line_count = code.count(\"\\n\") + 1\n        \n        self.text = tk.Text(\n            self,\n            height=line_count,\n            wrap=\"none\",\n            font=(\"Courier New\", 10),\n            padx=5,\n            pady=5,\n            highlightthickness=1 # Border for code blocks\n        )\n        self.text.insert(\"1.0\", code)\n        self.text.configure(state=\"disabled\")\n        self.text.pack(fill=\"x\", padx=5, pady=(5, 2))\n\n        button_row = ttk.Frame(self)\n        button_row.pack(anchor=\"e\", padx=5, pady=(0, 5))\n\n        ttk.Button(button_row, text=\"Replace\", command=lambda: on_replace(code)).pack(side=\"right\", padx=2)\n        ttk.Button(button_row, text=\"Add\", command=lambda: on_add(code)).pack(side=\"right\", padx=2)\n\nclass ChatUI:\n    def __init__(self, root: tk.Tk):\n        self.root = root\n        self.root.title(\"Chat + Code UI\")\n        self.root.geometry(\"1100x650\")\n\n        self.conversation = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n        ]\n\n        self.token_queue = queue.Queue()\n        self.is_generating = False\n        self.current_assistant_buffer = \"\"\n\n        self.code_contents = \"\"\n\n        # ---- persistence helpers ----\n        self.auto_save_path: str | None = None\n        self.auto_save_after_id: str | None = None\n        self.auto_save_interval_ms = 30_000\n        \n        # Default models\n        self.chat_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n        self.helper_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n\n        self._build_layout()\n        self._bind_keys()\n        self._poll_tokens()\n\n    # -----------------------------\n    # Layout\n    # -----------------------------\n\n    def _build_layout(self) -> None:\n        self.root.rowconfigure(0, weight=0)\n        self.root.rowconfigure(1, weight=1)\n        self.root.rowconfigure(2, weight=0)\n        self.root.columnconfigure(0, weight=1)\n\n        # ----------------------\n        # Model Selection Bar\n        # ----------------------\n        model_frame = ttk.Frame(self.root)\n        model_frame.grid(row=0, column=0, sticky=\"ew\", padx=5, pady=5)\n        \n        ttk.Label(model_frame, text=\"Chat Model:\").pack(side=\"left\", padx=(0, 5))\n        chat_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.chat_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        chat_dropdown.pack(side=\"left\", padx=(0, 20))\n        \n        ttk.Label(model_frame, text=\"Helper Model:\").pack(side=\"left\", padx=(0, 5))\n        helper_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.helper_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        helper_dropdown.pack(side=\"left\")\n\n        # ----------------------\n        # Menu bar\n        # ----------------------\n        menubar = tk.Menu(self.root)\n        file_menu = tk.Menu(menubar, tearoff=0)\n        file_menu.add_command(label=\"Save conversation…\", command=self.save_conversation)\n        file_menu.add_command(label=\"Load conversation…\", command=self.load_conversation)\n        file_menu.add_separator()\n        file_menu.add_command(label=\"Export markdown…\", command=self.export_markdown)\n        menubar.add_cascade(label=\"File\", menu=file_menu)\n        self.root.config(menu=menubar)\n\n        main_pane = ttk.PanedWindow(self.root, orient=\"horizontal\")\n        main_pane.grid(row=1, column=0, sticky=\"nsew\")\n\n        # ----------------------\n        # Chat + Code Proposals\n        # ----------------------\n        chat_frame = ttk.Frame(main_pane)\n        chat_frame.rowconfigure(0, weight=1)\n        chat_frame.columnconfigure(0, weight=1)\n\n        # Scrollable canvas\n        self.chat_canvas = tk.Canvas(chat_frame)\n        self.chat_scrollbar = ttk.Scrollbar(chat_frame, command=self.chat_canvas.yview)\n        self.chat_canvas.configure(yscrollcommand=self.chat_scrollbar.set)\n\n        self.chat_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n        self.chat_canvas.grid(row=0, column=0, sticky=\"nsew\")\n\n        # Inner frame to hold all messages\n        self.chat_inner_frame = ttk.Frame(self.chat_canvas)\n        self.chat_canvas.create_window((0, 0), window=self.chat_inner_frame, anchor=\"nw\")\n\n        # Update scroll region when content changes\n        self.chat_inner_frame.bind(\n            \"<Configure>\",\n            lambda e: self.chat_canvas.configure(scrollregion=self.chat_canvas.bbox(\"all\"))\n        )\n\n        # Track the current message label for streaming\n        self.current_message_label = None\n        \n        # Get background color from a standard widget\n        self.bg_color = self.root.cget(\"background\")\n\n        # ----------------------\n        # Code Panel\n        # ----------------------\n        code_frame = ttk.Frame(main_pane)\n        code_frame.rowconfigure(0, weight=1)\n        code_frame.columnconfigure(0, weight=1)\n\n        self.code_panel = tk.Text(code_frame, wrap=\"none\")\n        self.code_panel.grid(row=0, column=0, sticky=\"nsew\")\n        \n        # Bind to sync code_contents when user edits\n        self.code_panel.bind(\"<<Modified>>\", self._on_code_modified)\n\n        def select_all_code_panel(event):\n            event.widget.tag_add(\"sel\", \"1.0\", \"end\")\n            return \"break\"\n\n        self.code_panel.bind(\"<Control-a>\", select_all_code_panel)\n        self.code_panel.bind(\"<Control-A>\", select_all_code_panel)\n        self.code_panel.bind(\"<Command-a>\", select_all_code_panel)  # For Mac\n\n        code_scroll = ttk.Scrollbar(code_frame, command=self.code_panel.yview)\n        code_scroll.grid(row=0, column=1, sticky=\"ns\")\n        self.code_panel[\"yscrollcommand\"] = code_scroll.set\n\n        main_pane.add(chat_frame)\n        main_pane.add(code_frame)\n\n        # ----------------------\n        # Input Box\n        # ----------------------\n        self.input_box = tk.Text(self.root, height=5, wrap=\"word\")\n        self.input_box.grid(row=2, column=0, sticky=\"nsew\")\n\n        self.root.update_idletasks()\n        main_pane.sashpos(0, int(self.root.winfo_width() * 0.70))\n    \n    def _get_client_for_model(self, model: str):\n        \"\"\"Return the appropriate client for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return openai_client\n        else:\n            return groq_client\n    \n    def _get_max_tokens_for_model(self, model: str):\n        \"\"\"Return appropriate max_tokens for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return 16000\n        else:\n            return 8000\n    \n    def _uses_max_completion_tokens(self, model: str):\n        \"\"\"Check if model uses max_completion_tokens instead of max_tokens.\"\"\"\n        return model in [\"gpt-5.2\", \"gpt-5-mini\"]\n\n    def _bind_keys(self):\n        self.input_box.bind(\"<Return>\", self._on_enter)\n        self.input_box.bind(\"<Shift-Return>\", lambda e: None)\n    \n    def _on_code_modified(self, event):\n        \"\"\"Sync code_contents when user edits the code panel.\"\"\"\n        if self.code_panel.edit_modified():\n            self.code_contents = self.code_panel.get(\"1.0\", \"end-1c\")\n            self.code_panel.edit_modified(False)\n\n    # -----------------------------\n    # Input\n    # -----------------------------\n\n    def _on_enter(self, event):\n        if self.is_generating:\n            return \"break\"\n\n        text = self.input_box.get(\"1.0\", \"end-1c\").strip()\n        if not text:\n            return \"break\"\n\n        self.input_box.delete(\"1.0\", \"end\")\n        self._append_chat(\"You\", text)\n        \n        # Add user message to conversation\n        self.conversation.append({\"role\": \"user\", \"content\": text})\n        \n        # Add invisible context about current code if it exists\n        if self.code_contents.strip():\n            self.conversation.append({\n                \"role\": \"user\",\n                \"content\": f\"Here's my current code:\\n{self.code_contents}\"\n            })\n\n        self._start_generation()\n        return \"break\"\n\n    # -----------------------------\n    # Chat Generation\n    # -----------------------------\n\n    def _start_generation(self):\n        self.is_generating = True\n        self.input_box.configure(state=\"disabled\")\n        self.current_assistant_buffer = \"\"\n\n        threading.Thread(\n            target=self._stream_chat_response, daemon=True\n        ).start()\n\n    def _stream_chat_response(self):\n        try:\n            model = self.chat_model.get()\n            client = self._get_client_for_model(model)\n            max_tokens = self._get_max_tokens_for_model(model)\n            \n            # Build request parameters\n            params = {\n                \"model\": model,\n                \"messages\": self.conversation,\n                \"stream\": True,\n            }\n            \n            # Use max_completion_tokens for GPT-5.2 and GPT-5-mini\n            if self._uses_max_completion_tokens(model):\n                params[\"max_completion_tokens\"] = max_tokens\n            else:\n                params[\"max_tokens\"] = max_tokens\n            \n            stream = client.chat.completions.create(**params)\n\n            for chunk in stream:\n                delta = chunk.choices[0].delta.content or \"\"\n                if delta:\n                    self.token_queue.put(delta)\n\n        except Exception as e:\n            self.token_queue.put(f\"\\n[Error: {e}]\")\n\n        finally:\n            self.token_queue.put(None)\n\n    def _poll_tokens(self):\n        try:\n            while True:\n                token = self.token_queue.get_nowait()\n    \n                if token is None:\n                    self._finalize_assistant_message()\n                    break\n    \n                self.current_assistant_buffer += token\n                self._append_to_last_chat(token)\n    \n        except queue.Empty:\n            pass\n        except Exception as e:\n            print(f\"Error polling tokens: {e}\")\n    \n        self.root.after(20, self._poll_tokens)\n    \n    def _finalize_assistant_message(self):\n        assistant_text = self.current_assistant_buffer\n        self.conversation.append({\"role\": \"assistant\", \"content\": assistant_text})\n\n        self.is_generating = False\n        self.input_box.configure(state=\"normal\")\n\n        # Replace the streaming label with parsed content\n        self._replace_with_parsed_message(assistant_text)\n\n    def _replace_with_parsed_message(self, text: str):\n        \"\"\"Parse the message and replace code blocks with interactive widgets.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.destroy()\n            self.current_message_label = None\n\n        # Create a container for this message\n        message_container = ttk.Frame(self.chat_inner_frame)\n        message_container.pack(fill=\"x\", padx=5, pady=5)\n\n        # Add sender as copyable text\n        sender_text = tk.Text(\n            message_container,\n            height=1,\n            wrap=\"none\",\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            background=self.bg_color\n        )\n        sender_text.insert(\"1.0\", \"Assistant:\")\n        sender_text.configure(state=\"disabled\")\n        sender_text.pack(fill=\"x\")\n\n        # Split text by code blocks\n        parts = self._split_text_and_code(text)\n        \n        for part_type, content in parts:\n            if part_type == \"text\":\n                if content.strip():\n                    text_widget = tk.Text(\n                        message_container,\n                        height=content.count(\"\\n\") + 1 + int(len(content) * 0.012),\n                        wrap=\"word\",\n                        relief=\"flat\",\n                        background=self.bg_color\n                    )\n                    text_widget.insert(\"1.0\", content)\n                    text_widget.configure(state=\"disabled\")\n                    text_widget.pack(fill=\"x\", padx=5, pady=2)\n            elif part_type == \"code\":\n                proposal = CodeProposal(\n                    parent=message_container,\n                    code=content,\n                    on_replace=self._replace_code_panel,\n                    on_add=self._add_code_via_helper,\n                )\n                proposal.pack(fill=\"x\", pady=5)\n\n        # Scroll to bottom\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _split_text_and_code(self, text: str):\n        \"\"\"Split text into alternating text and code blocks.\"\"\"\n        parts = []\n        last_end = 0\n        \n        for match in CODE_BLOCK_REGEX.finditer(text):\n            # Add text before code block\n            if match.start() > last_end:\n                parts.append((\"text\", text[last_end:match.start()]))\n            \n            # Add code block\n            parts.append((\"code\", match.group(1)))\n            last_end = match.end()\n        \n        # Add remaining text after last code block\n        if last_end < len(text):\n            parts.append((\"text\", text[last_end:]))\n        \n        return parts\n\n    # -----------------------------\n    # Code Handling (Line-Number Based Approach)\n    # -----------------------------\n\n    def _add_line_numbers(self, code: str) -> str:\n        \"\"\"Prepend line numbers to each line of code.\"\"\"\n        lines = code.split(\"\\n\")\n        numbered_lines = [f\"{i+1} | {line}\" for i, line in enumerate(lines)]\n        return \"\\n\".join(numbered_lines)\n\n    def _add_code_via_helper(self, new_code: str):\n        \"\"\"Ask helper model for insertion point and lines to delete, then apply.\"\"\"\n        if not self.code_contents.strip():\n            self._replace_code_panel(new_code)\n            return\n\n        # Get insertion instructions from helper model\n        insertion_data = self._get_insertion_instructions(new_code)\n        \n        if not insertion_data:\n            self._append_chat(\"System\", \"Helper model did not return valid instructions.\")\n            return\n\n        # Try to apply the insertion and deletion\n        try:\n            updated_code = self._apply_line_based_edit(\n                insertion_data[\"insertion_point\"],\n                insertion_data[\"delete_lines\"],\n                new_code,\n                insertion_data[\"indent_spaces\"]\n            )\n            self.code_contents = updated_code\n            self.code_panel.delete(\"1.0\", \"end\")\n            self.code_panel.insert(\"1.0\", updated_code)\n            self._append_chat(\"System\", \"Code updated successfully!\")\n        except Exception as e:\n            self._append_chat(\"System\", f\"Edit failed: {e}\")\n\n    def _get_insertion_instructions(self, new_code: str) -> dict | None:\n        \"\"\"Ask helper model for insertion point and lines to delete using line numbers.\"\"\"\n        # Add line numbers to current code\n        numbered_code = self._add_line_numbers(self.code_contents)\n        \n        system_prompt = (\n            \"You are a code editing assistant. Your job is to determine where to insert new code \"\n            \"and which lines (if any) to delete from the existing code.\\n\\n\"\n            \"You will receive:\\n\"\n            \"1. The current code with line numbers (format: '1 | code here')\\n\"\n            \"2. New code to insert\\n\\n\"\n            \"You must respond ONLY with JSON in this exact format:\\n\"\n            \"{\\n\"\n            '  \"insertion_point\": <line_number>,\\n'\n            '  \"delete_lines\": [<line_numbers>],\\n'\n            '  \"indent_spaces\": <number_of_spaces>\\n'\n            \"}\\n\\n\"\n            \"Rules:\\n\"\n            \"- insertion_point: The line number AFTER which to insert the new code (use 0 to insert at the beginning)\\n\"\n            \"- delete_lines: Array of line numbers to delete (can be empty [] if just inserting)\\n\"\n            \"- indent_spaces: Number of spaces to add to the beginning of each line of inserted code (to match surrounding indentation)\\n\"\n            \"- Use the CURRENT line numbers you see - don't worry about how insertion affects numbering\\n\"\n            \"- If replacing code, include those line numbers in delete_lines\\n\"\n            \"- If just inserting, leave delete_lines empty\\n\"\n            \"- Look at the indentation of surrounding code to determine indent_spaces\\n\\n\"\n            \"Example 1 - Replace lines 5-7 with 8 spaces of indentation:\\n\"\n            '{\"insertion_point\": 4, \"delete_lines\": [5, 6, 7], \"indent_spaces\": 8}\\n\\n'\n            \"Example 2 - Insert after line 10 with 4 spaces:\\n\"\n            '{\"insertion_point\": 10, \"delete_lines\": [], \"indent_spaces\": 4}\\n\\n'\n            \"Example 3 - Insert at beginning with no indentation:\\n\"\n            '{\"insertion_point\": 0, \"delete_lines\": [], \"indent_spaces\": 0}\\n\\n'\n            \"Respond with ONLY the JSON, no explanations.\"\n        )\n\n        user_prompt = (\n            \"CURRENT CODE (with line numbers):\\n\"\n            f\"{numbered_code}\\n\\n\"\n            \"NEW CODE TO INSERT:\\n\"\n            f\"{new_code}\\n\"\n        )\n\n        model = self.helper_model.get()\n        client = self._get_client_for_model(model)\n        max_tokens = self._get_max_tokens_for_model(model)\n        \n        params = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n        }\n        \n        if self._uses_max_completion_tokens(model):\n            params[\"max_completion_tokens\"] = max_tokens\n        else:\n            params[\"max_tokens\"] = max_tokens\n        \n        try:\n            response = client.chat.completions.create(**params)\n            response_text = response.choices[0].message.content or \"\"\n            \n            print(\"Helper response:\", response_text)\n            \n            # Extract JSON from response (in case model adds markdown)\n            json_match = re.search(r'\\{[^}]+\\}', response_text, re.DOTALL)\n            if json_match:\n                response_text = json_match.group(0)\n            \n            data = json.loads(response_text)\n            \n            # Validate response format\n            if \"insertion_point\" not in data or \"delete_lines\" not in data or \"indent_spaces\" not in data:\n                return None\n            \n            if not isinstance(data[\"insertion_point\"], int):\n                return None\n            \n            if not isinstance(data[\"delete_lines\"], list):\n                return None\n            \n            if not isinstance(data[\"indent_spaces\"], int):\n                return None\n            \n            return data\n            \n        except Exception as e:\n            print(f\"Error getting insertion instructions: {e}\")\n            return None\n\n    def _apply_line_based_edit(self, insertion_point: int, delete_lines: list, new_code: str, indent_spaces: int) -> str:\n        \"\"\"Apply insertion and deletion based on line numbers, with indentation.\"\"\"\n        lines = self.code_contents.split(\"\\n\")\n        \n        # Validate line numbers\n        if insertion_point < 0 or insertion_point > len(lines):\n            raise ValueError(f\"Invalid insertion point: {insertion_point}\")\n        \n        for line_num in delete_lines:\n            if line_num < 1 or line_num > len(lines):\n                raise ValueError(f\"Invalid line number to delete: {line_num}\")\n        \n        # Convert delete_lines to 0-indexed and sort in reverse order\n        delete_indices = sorted([line_num - 1 for line_num in delete_lines], reverse=True)\n        \n        # Adjust insertion point if we're deleting lines before it\n        adjusted_insertion = insertion_point\n        for idx in delete_indices:\n            if idx < insertion_point:\n                adjusted_insertion -= 1\n        \n        # Delete lines (in reverse order to preserve indices)\n        for idx in delete_indices:\n            del lines[idx]\n        \n        # Apply indentation to new code\n        indent = \" \" * indent_spaces\n        new_lines = new_code.split(\"\\n\")\n        indented_lines = [indent + line for line in new_lines]\n        \n        # Insert new code at the adjusted position\n        lines[adjusted_insertion:adjusted_insertion] = indented_lines\n        \n        return \"\\n\".join(lines)\n\n    def _replace_code_panel(self, code: str):\n        self.code_contents = code\n        self.code_panel.delete(\"1.0\", \"end\")\n        self.code_panel.insert(\"1.0\", code)\n\n    # -----------------------------\n    # Persistence\n    # -----------------------------\n    def save_conversation(self, path: str | None = None):\n        if path is None:\n            path = filedialog.asksaveasfilename(\n                defaultextension=\".json\",\n                filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n            )\n            if not path:\n                return\n        try:\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self.conversation, f, indent=2, ensure_ascii=False)\n            self.auto_save_path = path\n            messagebox.showinfo(\"Saved\", f\"Conversation saved to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Save error\", str(e))\n\n    def load_conversation(self):\n        path = filedialog.askopenfilename(\n            defaultextension=\".json\",\n            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                loaded = json.load(f)\n            if not isinstance(loaded, list):\n                raise ValueError(\"File must contain a list of messages.\")\n            self.conversation = loaded\n            self.auto_save_path = path\n            self._redraw_chat()\n            messagebox.showinfo(\"Loaded\", f\"Conversation loaded from\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Load error\", str(e))\n\n    def export_markdown(self):\n        path = filedialog.asksaveasfilename(\n            defaultextension=\".md\",\n            filetypes=[(\"Markdown files\", \"*.md\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            lines = []\n            for msg in self.conversation:\n                role = msg.get(\"role\", \"\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue\n                name = \"You\" if role == \"user\" else \"Assistant\"\n                lines.append(f\"**{name}:**\\n{content}\\n\")\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\\n\".join(lines))\n            messagebox.showinfo(\"Exported\", f\"Markdown exported to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Export error\", str(e))\n\n    def _redraw_chat(self):\n        for w in self.chat_inner_frame.winfo_children():\n            w.destroy()\n        self.current_message_label = None\n        for msg in self.conversation:\n            role = msg.get(\"role\")\n            content = msg.get(\"content\", \"\")\n            if role == \"system\":\n                continue\n            sender = \"You\" if role == \"user\" else \"Assistant\"\n            if role == \"assistant\":\n                self._replace_with_parsed_message(content)\n            else:\n                self._append_chat(sender, content)\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n\n    def _append_chat(self, sender: str, text: str):\n        \"\"\"Modified for tighter user message spacing.\"\"\"\n        container = ttk.Frame(self.chat_inner_frame)\n        container.pack(fill=\"x\", padx=10, pady=5)\n\n        header = tk.Text(container, height=1, font=(\"TkDefaultFont\", 10, \"bold\"),\n                        relief=\"flat\", background=self.bg_color, highlightthickness=0)\n        header.insert(\"1.0\", f\"{sender}:\")\n        header.configure(state=\"disabled\")\n        header.pack(fill=\"x\")\n\n        # Dynamic height for content\n        line_count = text.count(\"\\n\") + 1\n        body = tk.Text(container, height=line_count, wrap=\"word\", relief=\"flat\",\n                      background=self.bg_color, highlightthickness=0)\n        body.insert(\"1.0\", text)\n        body.configure(state=\"disabled\")\n        body.pack(fill=\"x\", padx=5)\n        \n        if sender == \"Assistant\":\n            self.current_message_label = body\n            self.current_message_container = container\n\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _append_to_last_chat(self, text: str):\n        \"\"\"Append text and grow the text widget height dynamically.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.configure(state=\"normal\")\n            self.current_message_label.insert(\"end\", text)\n            \n            line_count = int(self.current_message_label.index(\"end-1c\").split(\".\")[0])\n            self.current_message_label.configure(height=line_count, state=\"disabled\")\n            \n            self.chat_canvas.update_idletasks()\n            self.chat_canvas.yview_moveto(1.0)\n\n\nif __name__ == \"__main__\":\n    root = tk.Tk()\n    ChatUI(root)\n    root.mainloop()\n"
  },
  {
    "role": "assistant",
    "content": "Here's an improvement to the code: \n\nIn the `_append_chat` method, the `line_count` variable is used to calculate the height of the text widget. However, this calculation may not always be accurate, especially for long messages. A better approach would be to let Tkinter automatically adjust the height of the text widget based on its content.\n\nHere's the modified `_append_chat` method:\n\n```python\ndef _append_chat(self, sender: str, text: str):\n    \"\"\"Modified for tighter user message spacing.\"\"\"\n    container = ttk.Frame(self.chat_inner_frame)\n    container.pack(fill=\"x\", padx=10, pady=5)\n\n    header = tk.Text(container, height=1, font=(\"TkDefaultFont\", 10, \"bold\"),\n                    relief=\"flat\", background=self.bg_color, highlightthickness=0)\n    header.insert(\"1.0\", f\"{sender}:\")\n    header.configure(state=\"disabled\")\n    header.pack(fill=\"x\")\n\n    # Dynamic height for content\n    body = tk.Text(container, wrap=\"word\", relief=\"flat\",\n                  background=self.bg_color, highlightthickness=0)\n    body.insert(\"1.0\", text)\n    body.configure(state=\"disabled\")\n    body.pack(fill=\"both\", expand=True)\n    \n    if sender == \"Assistant\":\n        self.current_message_label = body\n        self.current_message_container = container\n\n    self.chat_canvas.update_idletasks()\n    self.chat_canvas.yview_moveto(1.0)\n```\n\nIn this modified version, the `body` text widget is packed with the `fill=\"both\"` and `expand=True` options, which allows it to automatically adjust its height based on its content. This should provide a better user experience, especially for long messages."
  },
  {
    "role": "user",
    "content": "Tell me a story so I can test if the new box size is good."
  },
  {
    "role": "assistant",
    "content": "Once upon a time, in a land far, far away, there was a magical kingdom ruled by a wise and just king. The kingdom was famous for its beautiful forests, sparkling lakes, and rolling hills. The king was loved by his people, and he did his best to make sure they were happy and prosperous.\n\nOne day, a group of travelers came to the kingdom. They were tired and hungry, and they asked the king for help. The king welcomed them with open arms and invited them to stay in the kingdom for as long as they needed. He gave them food and shelter, and even offered to help them find work if they wanted to stay.\n\nAs the travelers settled into their new life in the kingdom, they began to notice something strange. Every night, at exactly midnight, the stars in the sky would twinkle and shine in a peculiar pattern. The travelers were amazed by this sight, and they asked the king what it meant.\n\nThe king smiled and said, \"Ah, my friends, that is the work of the magical creatures that live in our kingdom. They are trying to communicate with us, to tell us something important. But I'm not sure what it means. Perhaps you can help me figure it out?\"\n\nThe travelers were intrigued by the king's words, and they set out to solve the mystery of the twinkling stars. They spent many nights studying the pattern, trying to decipher its meaning. And finally, after many weeks of hard work, they discovered the secret.\n\nThe twinkling stars were a message from the magical creatures, a message of hope and love. They were trying to tell the people of the kingdom that they were not alone, that there was magic and wonder in the world. And the travelers, who had once been strangers in a strange land, now felt like they belonged to the kingdom, like they were a part of something special.\n\nThe king was overjoyed when he heard the news, and he thanked the travelers for their help. He asked them to stay in the kingdom forever, to help him and his people understand the magic of the twinkling stars. And the travelers, who had once been weary and lost, now felt like they had found a new home, a place where they could live happily ever after. \n\nNow you can tell me a story in return and I will answer with another story, and so we will keep the conversation going."
  },
  {
    "role": "user",
    "content": "No thanks, let's code. I'm seeing way too much space under short user messages, but user messages where I paste my code are not tall enough to fit all the code."
  },
  {
    "role": "user",
    "content": "Here's my current code:\nimport os\nimport re\nimport threading\nimport queue\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nimport json\n\n# -----------------------------\n# Environment / Client Setup\n# -----------------------------\n\nload_dotenv()\n\ngroq_client = OpenAI(\n    api_key=os.getenv(\"GROQ_API_KEY\"),\n    base_url=\"https://api.groq.com/openai/v1\",\n)\n\nopenai_client = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\n# Model configurations\nGROQ_MODELS = [\n    \"llama-3.3-70b-versatile\",\n    \"moonshotai/kimi-k2-instruct\",\n    \"moonshotai/kimi-k2-instruct-0905\",\n    \"openai/gpt-oss-120b\",\n    \"openai/gpt-oss-20b\",\n    \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n]\n\nOPENAI_MODELS = [\n    \"gpt-5.2\",\n    \"gpt-5-mini\",\n    \"gpt-4.1\",\n]\n\nALL_MODELS = GROQ_MODELS + OPENAI_MODELS\n\nCODE_BLOCK_REGEX = re.compile(\n    r\"```(?:\\w+)?\\s*(.*?)```\",\n    re.DOTALL\n)\n\n# -----------------------------\n# UI\n# -----------------------------\nclass CodeProposal(ttk.Frame):\n    def __init__(self, parent, code: str, on_replace, on_add):\n        super().__init__(parent)\n        self.code = code\n\n        # Calculate height: number of lines + 1\n        line_count = code.count(\"\\n\") + 1\n        \n        self.text = tk.Text(\n            self,\n            height=line_count,\n            wrap=\"none\",\n            font=(\"Courier New\", 10),\n            padx=5,\n            pady=5,\n            highlightthickness=1 # Border for code blocks\n        )\n        self.text.insert(\"1.0\", code)\n        self.text.configure(state=\"disabled\")\n        self.text.pack(fill=\"x\", padx=5, pady=(5, 2))\n\n        button_row = ttk.Frame(self)\n        button_row.pack(anchor=\"e\", padx=5, pady=(0, 5))\n\n        ttk.Button(button_row, text=\"Replace\", command=lambda: on_replace(code)).pack(side=\"right\", padx=2)\n        ttk.Button(button_row, text=\"Add\", command=lambda: on_add(code)).pack(side=\"right\", padx=2)\n\nclass ChatUI:\n    def __init__(self, root: tk.Tk):\n        self.root = root\n        self.root.title(\"Chat + Code UI\")\n        self.root.geometry(\"1100x650\")\n\n        self.conversation = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n        ]\n\n        self.token_queue = queue.Queue()\n        self.is_generating = False\n        self.current_assistant_buffer = \"\"\n\n        self.code_contents = \"\"\n\n        # ---- persistence helpers ----\n        self.auto_save_path: str | None = None\n        self.auto_save_after_id: str | None = None\n        self.auto_save_interval_ms = 30_000\n        \n        # Default models\n        self.chat_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n        self.helper_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n\n        self._build_layout()\n        self._bind_keys()\n        self._poll_tokens()\n\n    # -----------------------------\n    # Layout\n    # -----------------------------\n\n    def _build_layout(self) -> None:\n        self.root.rowconfigure(0, weight=0)\n        self.root.rowconfigure(1, weight=1)\n        self.root.rowconfigure(2, weight=0)\n        self.root.columnconfigure(0, weight=1)\n\n        # ----------------------\n        # Model Selection Bar\n        # ----------------------\n        model_frame = ttk.Frame(self.root)\n        model_frame.grid(row=0, column=0, sticky=\"ew\", padx=5, pady=5)\n        \n        ttk.Label(model_frame, text=\"Chat Model:\").pack(side=\"left\", padx=(0, 5))\n        chat_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.chat_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        chat_dropdown.pack(side=\"left\", padx=(0, 20))\n        \n        ttk.Label(model_frame, text=\"Helper Model:\").pack(side=\"left\", padx=(0, 5))\n        helper_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.helper_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        helper_dropdown.pack(side=\"left\")\n\n        # ----------------------\n        # Menu bar\n        # ----------------------\n        menubar = tk.Menu(self.root)\n        file_menu = tk.Menu(menubar, tearoff=0)\n        file_menu.add_command(label=\"Save conversation…\", command=self.save_conversation)\n        file_menu.add_command(label=\"Load conversation…\", command=self.load_conversation)\n        file_menu.add_separator()\n        file_menu.add_command(label=\"Export markdown…\", command=self.export_markdown)\n        menubar.add_cascade(label=\"File\", menu=file_menu)\n        self.root.config(menu=menubar)\n\n        main_pane = ttk.PanedWindow(self.root, orient=\"horizontal\")\n        main_pane.grid(row=1, column=0, sticky=\"nsew\")\n\n        # ----------------------\n        # Chat + Code Proposals\n        # ----------------------\n        chat_frame = ttk.Frame(main_pane)\n        chat_frame.rowconfigure(0, weight=1)\n        chat_frame.columnconfigure(0, weight=1)\n\n        # Scrollable canvas\n        self.chat_canvas = tk.Canvas(chat_frame)\n        self.chat_scrollbar = ttk.Scrollbar(chat_frame, command=self.chat_canvas.yview)\n        self.chat_canvas.configure(yscrollcommand=self.chat_scrollbar.set)\n\n        self.chat_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n        self.chat_canvas.grid(row=0, column=0, sticky=\"nsew\")\n\n        # Inner frame to hold all messages\n        self.chat_inner_frame = ttk.Frame(self.chat_canvas)\n        self.chat_canvas.create_window((0, 0), window=self.chat_inner_frame, anchor=\"nw\")\n\n        # Update scroll region when content changes\n        self.chat_inner_frame.bind(\n            \"<Configure>\",\n            lambda e: self.chat_canvas.configure(scrollregion=self.chat_canvas.bbox(\"all\"))\n        )\n\n        # Track the current message label for streaming\n        self.current_message_label = None\n        \n        # Get background color from a standard widget\n        self.bg_color = self.root.cget(\"background\")\n\n        # ----------------------\n        # Code Panel\n        # ----------------------\n        code_frame = ttk.Frame(main_pane)\n        code_frame.rowconfigure(0, weight=1)\n        code_frame.columnconfigure(0, weight=1)\n\n        self.code_panel = tk.Text(code_frame, wrap=\"none\")\n        self.code_panel.grid(row=0, column=0, sticky=\"nsew\")\n        \n        # Bind to sync code_contents when user edits\n        self.code_panel.bind(\"<<Modified>>\", self._on_code_modified)\n\n        def select_all_code_panel(event):\n            event.widget.tag_add(\"sel\", \"1.0\", \"end\")\n            return \"break\"\n\n        self.code_panel.bind(\"<Control-a>\", select_all_code_panel)\n        self.code_panel.bind(\"<Control-A>\", select_all_code_panel)\n        self.code_panel.bind(\"<Command-a>\", select_all_code_panel)  # For Mac\n\n        code_scroll = ttk.Scrollbar(code_frame, command=self.code_panel.yview)\n        code_scroll.grid(row=0, column=1, sticky=\"ns\")\n        self.code_panel[\"yscrollcommand\"] = code_scroll.set\n\n        main_pane.add(chat_frame)\n        main_pane.add(code_frame)\n\n        # ----------------------\n        # Input Box\n        # ----------------------\n        self.input_box = tk.Text(self.root, height=5, wrap=\"word\")\n        self.input_box.grid(row=2, column=0, sticky=\"nsew\")\n\n        self.root.update_idletasks()\n        main_pane.sashpos(0, int(self.root.winfo_width() * 0.70))\n    \n    def _get_client_for_model(self, model: str):\n        \"\"\"Return the appropriate client for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return openai_client\n        else:\n            return groq_client\n    \n    def _get_max_tokens_for_model(self, model: str):\n        \"\"\"Return appropriate max_tokens for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return 16000\n        else:\n            return 8000\n    \n    def _uses_max_completion_tokens(self, model: str):\n        \"\"\"Check if model uses max_completion_tokens instead of max_tokens.\"\"\"\n        return model in [\"gpt-5.2\", \"gpt-5-mini\"]\n\n    def _bind_keys(self):\n        self.input_box.bind(\"<Return>\", self._on_enter)\n        self.input_box.bind(\"<Shift-Return>\", lambda e: None)\n    \n    def _on_code_modified(self, event):\n        \"\"\"Sync code_contents when user edits the code panel.\"\"\"\n        if self.code_panel.edit_modified():\n            self.code_contents = self.code_panel.get(\"1.0\", \"end-1c\")\n            self.code_panel.edit_modified(False)\n\n    # -----------------------------\n    # Input\n    # -----------------------------\n\n    def _on_enter(self, event):\n        if self.is_generating:\n            return \"break\"\n\n        text = self.input_box.get(\"1.0\", \"end-1c\").strip()\n        if not text:\n            return \"break\"\n\n        self.input_box.delete(\"1.0\", \"end\")\n        self._append_chat(\"You\", text)\n        \n        # Add user message to conversation\n        self.conversation.append({\"role\": \"user\", \"content\": text})\n        \n        # Add invisible context about current code if it exists\n        if self.code_contents.strip():\n            self.conversation.append({\n                \"role\": \"user\",\n                \"content\": f\"Here's my current code:\\n{self.code_contents}\"\n            })\n\n        self._start_generation()\n        return \"break\"\n\n    # -----------------------------\n    # Chat Generation\n    # -----------------------------\n\n    def _start_generation(self):\n        self.is_generating = True\n        self.input_box.configure(state=\"disabled\")\n        self.current_assistant_buffer = \"\"\n\n        threading.Thread(\n            target=self._stream_chat_response, daemon=True\n        ).start()\n\n    def _stream_chat_response(self):\n        try:\n            model = self.chat_model.get()\n            client = self._get_client_for_model(model)\n            max_tokens = self._get_max_tokens_for_model(model)\n            \n            # Build request parameters\n            params = {\n                \"model\": model,\n                \"messages\": self.conversation,\n                \"stream\": True,\n            }\n            \n            # Use max_completion_tokens for GPT-5.2 and GPT-5-mini\n            if self._uses_max_completion_tokens(model):\n                params[\"max_completion_tokens\"] = max_tokens\n            else:\n                params[\"max_tokens\"] = max_tokens\n            \n            stream = client.chat.completions.create(**params)\n\n            for chunk in stream:\n                delta = chunk.choices[0].delta.content or \"\"\n                if delta:\n                    self.token_queue.put(delta)\n\n        except Exception as e:\n            self.token_queue.put(f\"\\n[Error: {e}]\")\n\n        finally:\n            self.token_queue.put(None)\n\n    def _poll_tokens(self):\n        try:\n            while True:\n                token = self.token_queue.get_nowait()\n    \n                if token is None:\n                    self._finalize_assistant_message()\n                    break\n    \n                self.current_assistant_buffer += token\n                self._append_to_last_chat(token)\n    \n        except queue.Empty:\n            pass\n        except Exception as e:\n            print(f\"Error polling tokens: {e}\")\n    \n        self.root.after(20, self._poll_tokens)\n    \n    def _finalize_assistant_message(self):\n        assistant_text = self.current_assistant_buffer\n        self.conversation.append({\"role\": \"assistant\", \"content\": assistant_text})\n\n        self.is_generating = False\n        self.input_box.configure(state=\"normal\")\n\n        # Replace the streaming label with parsed content\n        self._replace_with_parsed_message(assistant_text)\n\n    def _replace_with_parsed_message(self, text: str):\n        \"\"\"Parse the message and replace code blocks with interactive widgets.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.destroy()\n            self.current_message_label = None\n\n        # Create a container for this message\n        message_container = ttk.Frame(self.chat_inner_frame)\n        message_container.pack(fill=\"x\", padx=5, pady=5)\n\n        # Add sender as copyable text\n        sender_text = tk.Text(\n            message_container,\n            height=1,\n            wrap=\"none\",\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            background=self.bg_color\n        )\n        sender_text.insert(\"1.0\", \"Assistant:\")\n        sender_text.configure(state=\"disabled\")\n        sender_text.pack(fill=\"x\")\n\n        # Split text by code blocks\n        parts = self._split_text_and_code(text)\n        \n        for part_type, content in parts:\n            if part_type == \"text\":\n                if content.strip():\n                    text_widget = tk.Text(\n                        message_container,\n                        height=content.count(\"\\n\") + 1 + int(len(content) * 0.012),\n                        wrap=\"word\",\n                        relief=\"flat\",\n                        background=self.bg_color\n                    )\n                    text_widget.insert(\"1.0\", content)\n                    text_widget.configure(state=\"disabled\")\n                    text_widget.pack(fill=\"x\", padx=5, pady=2)\n            elif part_type == \"code\":\n                proposal = CodeProposal(\n                    parent=message_container,\n                    code=content,\n                    on_replace=self._replace_code_panel,\n                    on_add=self._add_code_via_helper,\n                )\n                proposal.pack(fill=\"x\", pady=5)\n\n        # Scroll to bottom\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _split_text_and_code(self, text: str):\n        \"\"\"Split text into alternating text and code blocks.\"\"\"\n        parts = []\n        last_end = 0\n        \n        for match in CODE_BLOCK_REGEX.finditer(text):\n            # Add text before code block\n            if match.start() > last_end:\n                parts.append((\"text\", text[last_end:match.start()]))\n            \n            # Add code block\n            parts.append((\"code\", match.group(1)))\n            last_end = match.end()\n        \n        # Add remaining text after last code block\n        if last_end < len(text):\n            parts.append((\"text\", text[last_end:]))\n        \n        return parts\n\n    # -----------------------------\n    # Code Handling (Line-Number Based Approach)\n    # -----------------------------\n\n    def _add_line_numbers(self, code: str) -> str:\n        \"\"\"Prepend line numbers to each line of code.\"\"\"\n        lines = code.split(\"\\n\")\n        numbered_lines = [f\"{i+1} | {line}\" for i, line in enumerate(lines)]\n        return \"\\n\".join(numbered_lines)\n\n    def _add_code_via_helper(self, new_code: str):\n        \"\"\"Ask helper model for insertion point and lines to delete, then apply.\"\"\"\n        if not self.code_contents.strip():\n            self._replace_code_panel(new_code)\n            return\n\n        # Get insertion instructions from helper model\n        insertion_data = self._get_insertion_instructions(new_code)\n        \n        if not insertion_data:\n            self._append_chat(\"System\", \"Helper model did not return valid instructions.\")\n            return\n\n        # Try to apply the insertion and deletion\n        try:\n            updated_code = self._apply_line_based_edit(\n                insertion_data[\"insertion_point\"],\n                insertion_data[\"delete_lines\"],\n                new_code,\n                insertion_data[\"indent_spaces\"]\n            )\n            self.code_contents = updated_code\n            self.code_panel.delete(\"1.0\", \"end\")\n            self.code_panel.insert(\"1.0\", updated_code)\n            self._append_chat(\"System\", \"Code updated successfully!\")\n        except Exception as e:\n            self._append_chat(\"System\", f\"Edit failed: {e}\")\n\n    def _get_insertion_instructions(self, new_code: str) -> dict | None:\n        \"\"\"Ask helper model for insertion point and lines to delete using line numbers.\"\"\"\n        # Add line numbers to current code\n        numbered_code = self._add_line_numbers(self.code_contents)\n        \n        system_prompt = (\n            \"You are a code editing assistant. Your job is to determine where to insert new code \"\n            \"and which lines (if any) to delete from the existing code.\\n\\n\"\n            \"You will receive:\\n\"\n            \"1. The current code with line numbers (format: '1 | code here')\\n\"\n            \"2. New code to insert\\n\\n\"\n            \"You must respond ONLY with JSON in this exact format:\\n\"\n            \"{\\n\"\n            '  \"insertion_point\": <line_number>,\\n'\n            '  \"delete_lines\": [<line_numbers>],\\n'\n            '  \"indent_spaces\": <number_of_spaces>\\n'\n            \"}\\n\\n\"\n            \"Rules:\\n\"\n            \"- insertion_point: The line number AFTER which to insert the new code (use 0 to insert at the beginning)\\n\"\n            \"- delete_lines: Array of line numbers to delete (can be empty [] if just inserting)\\n\"\n            \"- indent_spaces: Number of spaces to add to the beginning of each line of inserted code (to match surrounding indentation)\\n\"\n            \"- Use the CURRENT line numbers you see - don't worry about how insertion affects numbering\\n\"\n            \"- If replacing code, include those line numbers in delete_lines\\n\"\n            \"- If just inserting, leave delete_lines empty\\n\"\n            \"- Look at the indentation of surrounding code to determine indent_spaces\\n\\n\"\n            \"Example 1 - Replace lines 5-7 with 8 spaces of indentation:\\n\"\n            '{\"insertion_point\": 4, \"delete_lines\": [5, 6, 7], \"indent_spaces\": 8}\\n\\n'\n            \"Example 2 - Insert after line 10 with 4 spaces:\\n\"\n            '{\"insertion_point\": 10, \"delete_lines\": [], \"indent_spaces\": 4}\\n\\n'\n            \"Example 3 - Insert at beginning with no indentation:\\n\"\n            '{\"insertion_point\": 0, \"delete_lines\": [], \"indent_spaces\": 0}\\n\\n'\n            \"Respond with ONLY the JSON, no explanations.\"\n        )\n\n        user_prompt = (\n            \"CURRENT CODE (with line numbers):\\n\"\n            f\"{numbered_code}\\n\\n\"\n            \"NEW CODE TO INSERT:\\n\"\n            f\"{new_code}\\n\"\n        )\n\n        model = self.helper_model.get()\n        client = self._get_client_for_model(model)\n        max_tokens = self._get_max_tokens_for_model(model)\n        \n        params = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n        }\n        \n        if self._uses_max_completion_tokens(model):\n            params[\"max_completion_tokens\"] = max_tokens\n        else:\n            params[\"max_tokens\"] = max_tokens\n        \n        try:\n            response = client.chat.completions.create(**params)\n            response_text = response.choices[0].message.content or \"\"\n            \n            print(\"Helper response:\", response_text)\n            \n            # Extract JSON from response (in case model adds markdown)\n            json_match = re.search(r'\\{[^}]+\\}', response_text, re.DOTALL)\n            if json_match:\n                response_text = json_match.group(0)\n            \n            data = json.loads(response_text)\n            \n            # Validate response format\n            if \"insertion_point\" not in data or \"delete_lines\" not in data or \"indent_spaces\" not in data:\n                return None\n            \n            if not isinstance(data[\"insertion_point\"], int):\n                return None\n            \n            if not isinstance(data[\"delete_lines\"], list):\n                return None\n            \n            if not isinstance(data[\"indent_spaces\"], int):\n                return None\n            \n            return data\n            \n        except Exception as e:\n            print(f\"Error getting insertion instructions: {e}\")\n            return None\n\n    def _apply_line_based_edit(self, insertion_point: int, delete_lines: list, new_code: str, indent_spaces: int) -> str:\n        \"\"\"Apply insertion and deletion based on line numbers, with indentation.\"\"\"\n        lines = self.code_contents.split(\"\\n\")\n        \n        # Validate line numbers\n        if insertion_point < 0 or insertion_point > len(lines):\n            raise ValueError(f\"Invalid insertion point: {insertion_point}\")\n        \n        for line_num in delete_lines:\n            if line_num < 1 or line_num > len(lines):\n                raise ValueError(f\"Invalid line number to delete: {line_num}\")\n        \n        # Convert delete_lines to 0-indexed and sort in reverse order\n        delete_indices = sorted([line_num - 1 for line_num in delete_lines], reverse=True)\n        \n        # Adjust insertion point if we're deleting lines before it\n        adjusted_insertion = insertion_point\n        for idx in delete_indices:\n            if idx < insertion_point:\n                adjusted_insertion -= 1\n        \n        # Delete lines (in reverse order to preserve indices)\n        for idx in delete_indices:\n            del lines[idx]\n        \n        # Apply indentation to new code\n        indent = \" \" * indent_spaces\n        new_lines = new_code.split(\"\\n\")\n        indented_lines = [indent + line for line in new_lines]\n        \n        # Insert new code at the adjusted position\n        lines[adjusted_insertion:adjusted_insertion] = indented_lines\n        \n        return \"\\n\".join(lines)\n\n    def _replace_code_panel(self, code: str):\n        self.code_contents = code\n        self.code_panel.delete(\"1.0\", \"end\")\n        self.code_panel.insert(\"1.0\", code)\n\n    # -----------------------------\n    # Persistence\n    # -----------------------------\n    def save_conversation(self, path: str | None = None):\n        if path is None:\n            path = filedialog.asksaveasfilename(\n                defaultextension=\".json\",\n                filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n            )\n            if not path:\n                return\n        try:\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self.conversation, f, indent=2, ensure_ascii=False)\n            self.auto_save_path = path\n            messagebox.showinfo(\"Saved\", f\"Conversation saved to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Save error\", str(e))\n\n    def load_conversation(self):\n        path = filedialog.askopenfilename(\n            defaultextension=\".json\",\n            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                loaded = json.load(f)\n            if not isinstance(loaded, list):\n                raise ValueError(\"File must contain a list of messages.\")\n            self.conversation = loaded\n            self.auto_save_path = path\n            self._redraw_chat()\n            messagebox.showinfo(\"Loaded\", f\"Conversation loaded from\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Load error\", str(e))\n\n    def export_markdown(self):\n        path = filedialog.asksaveasfilename(\n            defaultextension=\".md\",\n            filetypes=[(\"Markdown files\", \"*.md\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            lines = []\n            for msg in self.conversation:\n                role = msg.get(\"role\", \"\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue\n                name = \"You\" if role == \"user\" else \"Assistant\"\n                lines.append(f\"**{name}:**\\n{content}\\n\")\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\\n\".join(lines))\n            messagebox.showinfo(\"Exported\", f\"Markdown exported to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Export error\", str(e))\n\n    def _redraw_chat(self):\n        for w in self.chat_inner_frame.winfo_children():\n            w.destroy()\n        self.current_message_label = None\n        for msg in self.conversation:\n            role = msg.get(\"role\")\n            content = msg.get(\"content\", \"\")\n            if role == \"system\":\n                continue\n            sender = \"You\" if role == \"user\" else \"Assistant\"\n            if role == \"assistant\":\n                self._replace_with_parsed_message(content)\n            else:\n                self._append_chat(sender, content)\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n\n    def _append_chat(self, sender: str, text: str):\n        \"\"\"Modified for tighter user message spacing.\"\"\"\n        container = ttk.Frame(self.chat_inner_frame)\n        container.pack(fill=\"x\", padx=10, pady=5)\n    \n        header = tk.Text(container, height=1, font=(\"TkDefaultFont\", 10, \"bold\"),\n                        relief=\"flat\", background=self.bg_color, highlightthickness=0)\n        header.insert(\"1.0\", f\"{sender}:\")\n        header.configure(state=\"disabled\")\n        header.pack(fill=\"x\")\n    \n        # Dynamic height for content\n        body = tk.Text(container, wrap=\"word\", relief=\"flat\",\n                      background=self.bg_color, highlightthickness=0)\n        body.insert(\"1.0\", text)\n        body.configure(state=\"disabled\")\n        body.pack(fill=\"both\", expand=True)\n        \n        if sender == \"Assistant\":\n            self.current_message_label = body\n            self.current_message_container = container\n    \n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n    \n    def _append_to_last_chat(self, text: str):\n        \"\"\"Append text and grow the text widget height dynamically.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.configure(state=\"normal\")\n            self.current_message_label.insert(\"end\", text)\n            \n            line_count = int(self.current_message_label.index(\"end-1c\").split(\".\")[0])\n            self.current_message_label.configure(height=line_count, state=\"disabled\")\n            \n            self.chat_canvas.update_idletasks()\n            self.chat_canvas.yview_moveto(1.0)\n\n\nif __name__ == \"__main__\":\n    root = tk.Tk()\n    ChatUI(root)\n    root.mainloop()\n\n"
  },
  {
    "role": "assistant",
    "content": "Your problem with too much space under short user messages and not enough space for longer messages can be solved by adjusting the way the message body is packed. Here's an updated `_append_chat` method:\n\n```python\ndef _append_chat(self, sender: str, text: str):\n    \"\"\"Modified for tighter user message spacing.\"\"\"\n    container = ttk.Frame(self.chat_inner_frame)\n    container.pack(fill=\"x\", padx=10, pady=2)  # Reduce padding around container\n\n    header = tk.Text(container, height=1, font=(\"TkDefaultFont\", 10, \"bold\"),\n                    relief=\"flat\", background=self.bg_color, highlightthickness=0)\n    header.insert(\"1.0\", f\"{sender}:\")\n    header.configure(state=\"disabled\")\n    header.pack(fill=\"x\")\n\n    # Dynamic height for content\n    body = tk.Text(container, wrap=\"word\", relief=\"flat\",\n                  background=self.bg_color, highlightthickness=0)\n    body.insert(\"1.0\", text)\n    body.configure(state=\"disabled\")\n    body.pack(fill=\"both\", expand=True)\n\n    if sender == \"Assistant\":\n        self.current_message_label = body\n        self.current_message_container = container\n\n    self.chat_canvas.update_idletasks()\n    self.chat_canvas.yview_moveto(1.0)\n```\n\nThis code reduces the padding around the container (from `pady=5` to `pady=2`) and allows the body of the message to expand to fill the available space. This should reduce the amount of empty space under short messages and allow longer messages to be displayed without cutting them off."
  },
  {
    "role": "user",
    "content": "I'm still experiencing the same problem as before, where short user messages have too much space and long messages have not enough."
  },
  {
    "role": "user",
    "content": "Here's my current code:\nimport os\nimport re\nimport threading\nimport queue\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nimport json\n\n# -----------------------------\n# Environment / Client Setup\n# -----------------------------\n\nload_dotenv()\n\ngroq_client = OpenAI(\n    api_key=os.getenv(\"GROQ_API_KEY\"),\n    base_url=\"https://api.groq.com/openai/v1\",\n)\n\nopenai_client = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\n# Model configurations\nGROQ_MODELS = [\n    \"llama-3.3-70b-versatile\",\n    \"moonshotai/kimi-k2-instruct\",\n    \"moonshotai/kimi-k2-instruct-0905\",\n    \"openai/gpt-oss-120b\",\n    \"openai/gpt-oss-20b\",\n    \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n]\n\nOPENAI_MODELS = [\n    \"gpt-5.2\",\n    \"gpt-5-mini\",\n    \"gpt-4.1\",\n]\n\nALL_MODELS = GROQ_MODELS + OPENAI_MODELS\n\nCODE_BLOCK_REGEX = re.compile(\n    r\"```(?:\\w+)?\\s*(.*?)```\",\n    re.DOTALL\n)\n\n# -----------------------------\n# UI\n# -----------------------------\nclass CodeProposal(ttk.Frame):\n    def __init__(self, parent, code: str, on_replace, on_add):\n        super().__init__(parent)\n        self.code = code\n\n        # Calculate height: number of lines + 1\n        line_count = code.count(\"\\n\") + 1\n        \n        self.text = tk.Text(\n            self,\n            height=line_count,\n            wrap=\"none\",\n            font=(\"Courier New\", 10),\n            padx=5,\n            pady=5,\n            highlightthickness=1 # Border for code blocks\n        )\n        self.text.insert(\"1.0\", code)\n        self.text.configure(state=\"disabled\")\n        self.text.pack(fill=\"x\", padx=5, pady=(5, 2))\n\n        button_row = ttk.Frame(self)\n        button_row.pack(anchor=\"e\", padx=5, pady=(0, 5))\n\n        ttk.Button(button_row, text=\"Replace\", command=lambda: on_replace(code)).pack(side=\"right\", padx=2)\n        ttk.Button(button_row, text=\"Add\", command=lambda: on_add(code)).pack(side=\"right\", padx=2)\n\nclass ChatUI:\n    def __init__(self, root: tk.Tk):\n        self.root = root\n        self.root.title(\"Chat + Code UI\")\n        self.root.geometry(\"1100x650\")\n\n        self.conversation = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n        ]\n\n        self.token_queue = queue.Queue()\n        self.is_generating = False\n        self.current_assistant_buffer = \"\"\n\n        self.code_contents = \"\"\n\n        # ---- persistence helpers ----\n        self.auto_save_path: str | None = None\n        self.auto_save_after_id: str | None = None\n        self.auto_save_interval_ms = 30_000\n        \n        # Default models\n        self.chat_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n        self.helper_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n\n        self._build_layout()\n        self._bind_keys()\n        self._poll_tokens()\n\n    # -----------------------------\n    # Layout\n    # -----------------------------\n\n    def _build_layout(self) -> None:\n        self.root.rowconfigure(0, weight=0)\n        self.root.rowconfigure(1, weight=1)\n        self.root.rowconfigure(2, weight=0)\n        self.root.columnconfigure(0, weight=1)\n\n        # ----------------------\n        # Model Selection Bar\n        # ----------------------\n        model_frame = ttk.Frame(self.root)\n        model_frame.grid(row=0, column=0, sticky=\"ew\", padx=5, pady=5)\n        \n        ttk.Label(model_frame, text=\"Chat Model:\").pack(side=\"left\", padx=(0, 5))\n        chat_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.chat_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        chat_dropdown.pack(side=\"left\", padx=(0, 20))\n        \n        ttk.Label(model_frame, text=\"Helper Model:\").pack(side=\"left\", padx=(0, 5))\n        helper_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.helper_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        helper_dropdown.pack(side=\"left\")\n\n        # ----------------------\n        # Menu bar\n        # ----------------------\n        menubar = tk.Menu(self.root)\n        file_menu = tk.Menu(menubar, tearoff=0)\n        file_menu.add_command(label=\"Save conversation…\", command=self.save_conversation)\n        file_menu.add_command(label=\"Load conversation…\", command=self.load_conversation)\n        file_menu.add_separator()\n        file_menu.add_command(label=\"Export markdown…\", command=self.export_markdown)\n        menubar.add_cascade(label=\"File\", menu=file_menu)\n        self.root.config(menu=menubar)\n\n        main_pane = ttk.PanedWindow(self.root, orient=\"horizontal\")\n        main_pane.grid(row=1, column=0, sticky=\"nsew\")\n\n        # ----------------------\n        # Chat + Code Proposals\n        # ----------------------\n        chat_frame = ttk.Frame(main_pane)\n        chat_frame.rowconfigure(0, weight=1)\n        chat_frame.columnconfigure(0, weight=1)\n\n        # Scrollable canvas\n        self.chat_canvas = tk.Canvas(chat_frame)\n        self.chat_scrollbar = ttk.Scrollbar(chat_frame, command=self.chat_canvas.yview)\n        self.chat_canvas.configure(yscrollcommand=self.chat_scrollbar.set)\n\n        self.chat_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n        self.chat_canvas.grid(row=0, column=0, sticky=\"nsew\")\n\n        # Inner frame to hold all messages\n        self.chat_inner_frame = ttk.Frame(self.chat_canvas)\n        self.chat_canvas.create_window((0, 0), window=self.chat_inner_frame, anchor=\"nw\")\n\n        # Update scroll region when content changes\n        self.chat_inner_frame.bind(\n            \"<Configure>\",\n            lambda e: self.chat_canvas.configure(scrollregion=self.chat_canvas.bbox(\"all\"))\n        )\n\n        # Track the current message label for streaming\n        self.current_message_label = None\n        \n        # Get background color from a standard widget\n        self.bg_color = self.root.cget(\"background\")\n\n        # ----------------------\n        # Code Panel\n        # ----------------------\n        code_frame = ttk.Frame(main_pane)\n        code_frame.rowconfigure(0, weight=1)\n        code_frame.columnconfigure(0, weight=1)\n\n        self.code_panel = tk.Text(code_frame, wrap=\"none\")\n        self.code_panel.grid(row=0, column=0, sticky=\"nsew\")\n        \n        # Bind to sync code_contents when user edits\n        self.code_panel.bind(\"<<Modified>>\", self._on_code_modified)\n\n        def select_all_code_panel(event):\n            event.widget.tag_add(\"sel\", \"1.0\", \"end\")\n            return \"break\"\n\n        self.code_panel.bind(\"<Control-a>\", select_all_code_panel)\n        self.code_panel.bind(\"<Control-A>\", select_all_code_panel)\n        self.code_panel.bind(\"<Command-a>\", select_all_code_panel)  # For Mac\n\n        code_scroll = ttk.Scrollbar(code_frame, command=self.code_panel.yview)\n        code_scroll.grid(row=0, column=1, sticky=\"ns\")\n        self.code_panel[\"yscrollcommand\"] = code_scroll.set\n\n        main_pane.add(chat_frame)\n        main_pane.add(code_frame)\n\n        # ----------------------\n        # Input Box\n        # ----------------------\n        self.input_box = tk.Text(self.root, height=5, wrap=\"word\")\n        self.input_box.grid(row=2, column=0, sticky=\"nsew\")\n\n        self.root.update_idletasks()\n        main_pane.sashpos(0, int(self.root.winfo_width() * 0.70))\n    \n    def _get_client_for_model(self, model: str):\n        \"\"\"Return the appropriate client for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return openai_client\n        else:\n            return groq_client\n    \n    def _get_max_tokens_for_model(self, model: str):\n        \"\"\"Return appropriate max_tokens for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return 16000\n        else:\n            return 8000\n    \n    def _uses_max_completion_tokens(self, model: str):\n        \"\"\"Check if model uses max_completion_tokens instead of max_tokens.\"\"\"\n        return model in [\"gpt-5.2\", \"gpt-5-mini\"]\n\n    def _bind_keys(self):\n        self.input_box.bind(\"<Return>\", self._on_enter)\n        self.input_box.bind(\"<Shift-Return>\", lambda e: None)\n    \n    def _on_code_modified(self, event):\n        \"\"\"Sync code_contents when user edits the code panel.\"\"\"\n        if self.code_panel.edit_modified():\n            self.code_contents = self.code_panel.get(\"1.0\", \"end-1c\")\n            self.code_panel.edit_modified(False)\n\n    # -----------------------------\n    # Input\n    # -----------------------------\n\n    def _on_enter(self, event):\n        if self.is_generating:\n            return \"break\"\n\n        text = self.input_box.get(\"1.0\", \"end-1c\").strip()\n        if not text:\n            return \"break\"\n\n        self.input_box.delete(\"1.0\", \"end\")\n        self._append_chat(\"You\", text)\n        \n        # Add user message to conversation\n        self.conversation.append({\"role\": \"user\", \"content\": text})\n        \n        # Add invisible context about current code if it exists\n        if self.code_contents.strip():\n            self.conversation.append({\n                \"role\": \"user\",\n                \"content\": f\"Here's my current code:\\n{self.code_contents}\"\n            })\n\n        self._start_generation()\n        return \"break\"\n\n    # -----------------------------\n    # Chat Generation\n    # -----------------------------\n\n    def _start_generation(self):\n        self.is_generating = True\n        self.input_box.configure(state=\"disabled\")\n        self.current_assistant_buffer = \"\"\n\n        threading.Thread(\n            target=self._stream_chat_response, daemon=True\n        ).start()\n\n    def _stream_chat_response(self):\n        try:\n            model = self.chat_model.get()\n            client = self._get_client_for_model(model)\n            max_tokens = self._get_max_tokens_for_model(model)\n            \n            # Build request parameters\n            params = {\n                \"model\": model,\n                \"messages\": self.conversation,\n                \"stream\": True,\n            }\n            \n            # Use max_completion_tokens for GPT-5.2 and GPT-5-mini\n            if self._uses_max_completion_tokens(model):\n                params[\"max_completion_tokens\"] = max_tokens\n            else:\n                params[\"max_tokens\"] = max_tokens\n            \n            stream = client.chat.completions.create(**params)\n\n            for chunk in stream:\n                delta = chunk.choices[0].delta.content or \"\"\n                if delta:\n                    self.token_queue.put(delta)\n\n        except Exception as e:\n            self.token_queue.put(f\"\\n[Error: {e}]\")\n\n        finally:\n            self.token_queue.put(None)\n\n    def _poll_tokens(self):\n        try:\n            while True:\n                token = self.token_queue.get_nowait()\n    \n                if token is None:\n                    self._finalize_assistant_message()\n                    break\n    \n                self.current_assistant_buffer += token\n                self._append_to_last_chat(token)\n    \n        except queue.Empty:\n            pass\n        except Exception as e:\n            print(f\"Error polling tokens: {e}\")\n    \n        self.root.after(20, self._poll_tokens)\n    \n    def _finalize_assistant_message(self):\n        assistant_text = self.current_assistant_buffer\n        self.conversation.append({\"role\": \"assistant\", \"content\": assistant_text})\n\n        self.is_generating = False\n        self.input_box.configure(state=\"normal\")\n\n        # Replace the streaming label with parsed content\n        self._replace_with_parsed_message(assistant_text)\n\n    def _replace_with_parsed_message(self, text: str):\n        \"\"\"Parse the message and replace code blocks with interactive widgets.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.destroy()\n            self.current_message_label = None\n\n        # Create a container for this message\n        message_container = ttk.Frame(self.chat_inner_frame)\n        message_container.pack(fill=\"x\", padx=5, pady=5)\n\n        # Add sender as copyable text\n        sender_text = tk.Text(\n            message_container,\n            height=1,\n            wrap=\"none\",\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            background=self.bg_color\n        )\n        sender_text.insert(\"1.0\", \"Assistant:\")\n        sender_text.configure(state=\"disabled\")\n        sender_text.pack(fill=\"x\")\n\n        # Split text by code blocks\n        parts = self._split_text_and_code(text)\n        \n        for part_type, content in parts:\n            if part_type == \"text\":\n                if content.strip():\n                    text_widget = tk.Text(\n                        message_container,\n                        height=content.count(\"\\n\") + 1 + int(len(content) * 0.012),\n                        wrap=\"word\",\n                        relief=\"flat\",\n                        background=self.bg_color\n                    )\n                    text_widget.insert(\"1.0\", content)\n                    text_widget.configure(state=\"disabled\")\n                    text_widget.pack(fill=\"x\", padx=5, pady=2)\n            elif part_type == \"code\":\n                proposal = CodeProposal(\n                    parent=message_container,\n                    code=content,\n                    on_replace=self._replace_code_panel,\n                    on_add=self._add_code_via_helper,\n                )\n                proposal.pack(fill=\"x\", pady=5)\n\n        # Scroll to bottom\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _split_text_and_code(self, text: str):\n        \"\"\"Split text into alternating text and code blocks.\"\"\"\n        parts = []\n        last_end = 0\n        \n        for match in CODE_BLOCK_REGEX.finditer(text):\n            # Add text before code block\n            if match.start() > last_end:\n                parts.append((\"text\", text[last_end:match.start()]))\n            \n            # Add code block\n            parts.append((\"code\", match.group(1)))\n            last_end = match.end()\n        \n        # Add remaining text after last code block\n        if last_end < len(text):\n            parts.append((\"text\", text[last_end:]))\n        \n        return parts\n\n    # -----------------------------\n    # Code Handling (Line-Number Based Approach)\n    # -----------------------------\n\n    def _add_line_numbers(self, code: str) -> str:\n        \"\"\"Prepend line numbers to each line of code.\"\"\"\n        lines = code.split(\"\\n\")\n        numbered_lines = [f\"{i+1} | {line}\" for i, line in enumerate(lines)]\n        return \"\\n\".join(numbered_lines)\n\n    def _add_code_via_helper(self, new_code: str):\n        \"\"\"Ask helper model for insertion point and lines to delete, then apply.\"\"\"\n        if not self.code_contents.strip():\n            self._replace_code_panel(new_code)\n            return\n\n        # Get insertion instructions from helper model\n        insertion_data = self._get_insertion_instructions(new_code)\n        \n        if not insertion_data:\n            self._append_chat(\"System\", \"Helper model did not return valid instructions.\")\n            return\n\n        # Try to apply the insertion and deletion\n        try:\n            updated_code = self._apply_line_based_edit(\n                insertion_data[\"insertion_point\"],\n                insertion_data[\"delete_lines\"],\n                new_code,\n                insertion_data[\"indent_spaces\"]\n            )\n            self.code_contents = updated_code\n            self.code_panel.delete(\"1.0\", \"end\")\n            self.code_panel.insert(\"1.0\", updated_code)\n            self._append_chat(\"System\", \"Code updated successfully!\")\n        except Exception as e:\n            self._append_chat(\"System\", f\"Edit failed: {e}\")\n\n    def _get_insertion_instructions(self, new_code: str) -> dict | None:\n        \"\"\"Ask helper model for insertion point and lines to delete using line numbers.\"\"\"\n        # Add line numbers to current code\n        numbered_code = self._add_line_numbers(self.code_contents)\n        \n        system_prompt = (\n            \"You are a code editing assistant. Your job is to determine where to insert new code \"\n            \"and which lines (if any) to delete from the existing code.\\n\\n\"\n            \"You will receive:\\n\"\n            \"1. The current code with line numbers (format: '1 | code here')\\n\"\n            \"2. New code to insert\\n\\n\"\n            \"You must respond ONLY with JSON in this exact format:\\n\"\n            \"{\\n\"\n            '  \"insertion_point\": <line_number>,\\n'\n            '  \"delete_lines\": [<line_numbers>],\\n'\n            '  \"indent_spaces\": <number_of_spaces>\\n'\n            \"}\\n\\n\"\n            \"Rules:\\n\"\n            \"- insertion_point: The line number AFTER which to insert the new code (use 0 to insert at the beginning)\\n\"\n            \"- delete_lines: Array of line numbers to delete (can be empty [] if just inserting)\\n\"\n            \"- indent_spaces: Number of spaces to add to the beginning of each line of inserted code (to match surrounding indentation)\\n\"\n            \"- Use the CURRENT line numbers you see - don't worry about how insertion affects numbering\\n\"\n            \"- If replacing code, include those line numbers in delete_lines\\n\"\n            \"- If just inserting, leave delete_lines empty\\n\"\n            \"- Look at the indentation of surrounding code to determine indent_spaces\\n\\n\"\n            \"Example 1 - Replace lines 5-7 with 8 spaces of indentation:\\n\"\n            '{\"insertion_point\": 4, \"delete_lines\": [5, 6, 7], \"indent_spaces\": 8}\\n\\n'\n            \"Example 2 - Insert after line 10 with 4 spaces:\\n\"\n            '{\"insertion_point\": 10, \"delete_lines\": [], \"indent_spaces\": 4}\\n\\n'\n            \"Example 3 - Insert at beginning with no indentation:\\n\"\n            '{\"insertion_point\": 0, \"delete_lines\": [], \"indent_spaces\": 0}\\n\\n'\n            \"Respond with ONLY the JSON, no explanations.\"\n        )\n\n        user_prompt = (\n            \"CURRENT CODE (with line numbers):\\n\"\n            f\"{numbered_code}\\n\\n\"\n            \"NEW CODE TO INSERT:\\n\"\n            f\"{new_code}\\n\"\n        )\n\n        model = self.helper_model.get()\n        client = self._get_client_for_model(model)\n        max_tokens = self._get_max_tokens_for_model(model)\n        \n        params = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n        }\n        \n        if self._uses_max_completion_tokens(model):\n            params[\"max_completion_tokens\"] = max_tokens\n        else:\n            params[\"max_tokens\"] = max_tokens\n        \n        try:\n            response = client.chat.completions.create(**params)\n            response_text = response.choices[0].message.content or \"\"\n            \n            print(\"Helper response:\", response_text)\n            \n            # Extract JSON from response (in case model adds markdown)\n            json_match = re.search(r'\\{[^}]+\\}', response_text, re.DOTALL)\n            if json_match:\n                response_text = json_match.group(0)\n            \n            data = json.loads(response_text)\n            \n            # Validate response format\n            if \"insertion_point\" not in data or \"delete_lines\" not in data or \"indent_spaces\" not in data:\n                return None\n            \n            if not isinstance(data[\"insertion_point\"], int):\n                return None\n            \n            if not isinstance(data[\"delete_lines\"], list):\n                return None\n            \n            if not isinstance(data[\"indent_spaces\"], int):\n                return None\n            \n            return data\n            \n        except Exception as e:\n            print(f\"Error getting insertion instructions: {e}\")\n            return None\n\n    def _apply_line_based_edit(self, insertion_point: int, delete_lines: list, new_code: str, indent_spaces: int) -> str:\n        \"\"\"Apply insertion and deletion based on line numbers, with indentation.\"\"\"\n        lines = self.code_contents.split(\"\\n\")\n        \n        # Validate line numbers\n        if insertion_point < 0 or insertion_point > len(lines):\n            raise ValueError(f\"Invalid insertion point: {insertion_point}\")\n        \n        for line_num in delete_lines:\n            if line_num < 1 or line_num > len(lines):\n                raise ValueError(f\"Invalid line number to delete: {line_num}\")\n        \n        # Convert delete_lines to 0-indexed and sort in reverse order\n        delete_indices = sorted([line_num - 1 for line_num in delete_lines], reverse=True)\n        \n        # Adjust insertion point if we're deleting lines before it\n        adjusted_insertion = insertion_point\n        for idx in delete_indices:\n            if idx < insertion_point:\n                adjusted_insertion -= 1\n        \n        # Delete lines (in reverse order to preserve indices)\n        for idx in delete_indices:\n            del lines[idx]\n        \n        # Apply indentation to new code\n        indent = \" \" * indent_spaces\n        new_lines = new_code.split(\"\\n\")\n        indented_lines = [indent + line for line in new_lines]\n        \n        # Insert new code at the adjusted position\n        lines[adjusted_insertion:adjusted_insertion] = indented_lines\n        \n        return \"\\n\".join(lines)\n\n    def _replace_code_panel(self, code: str):\n        self.code_contents = code\n        self.code_panel.delete(\"1.0\", \"end\")\n        self.code_panel.insert(\"1.0\", code)\n\n    # -----------------------------\n    # Persistence\n    # -----------------------------\n    def save_conversation(self, path: str | None = None):\n        if path is None:\n            path = filedialog.asksaveasfilename(\n                defaultextension=\".json\",\n                filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n            )\n            if not path:\n                return\n        try:\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self.conversation, f, indent=2, ensure_ascii=False)\n            self.auto_save_path = path\n            messagebox.showinfo(\"Saved\", f\"Conversation saved to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Save error\", str(e))\n\n    def load_conversation(self):\n        path = filedialog.askopenfilename(\n            defaultextension=\".json\",\n            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                loaded = json.load(f)\n            if not isinstance(loaded, list):\n                raise ValueError(\"File must contain a list of messages.\")\n            self.conversation = loaded\n            self.auto_save_path = path\n            self._redraw_chat()\n            messagebox.showinfo(\"Loaded\", f\"Conversation loaded from\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Load error\", str(e))\n\n    def export_markdown(self):\n        path = filedialog.asksaveasfilename(\n            defaultextension=\".md\",\n            filetypes=[(\"Markdown files\", \"*.md\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            lines = []\n            for msg in self.conversation:\n                role = msg.get(\"role\", \"\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue\n                name = \"You\" if role == \"user\" else \"Assistant\"\n                lines.append(f\"**{name}:**\\n{content}\\n\")\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\\n\".join(lines))\n            messagebox.showinfo(\"Exported\", f\"Markdown exported to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Export error\", str(e))\n\n    def _redraw_chat(self):\n        for w in self.chat_inner_frame.winfo_children():\n            w.destroy()\n        self.current_message_label = None\n        for msg in self.conversation:\n            role = msg.get(\"role\")\n            content = msg.get(\"content\", \"\")\n            if role == \"system\":\n                continue\n            sender = \"You\" if role == \"user\" else \"Assistant\"\n            if role == \"assistant\":\n                self._replace_with_parsed_message(content)\n            else:\n                self._append_chat(sender, content)\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n\n    def _append_chat(self, sender: str, text: str):\n        \"\"\"Modified for tighter user message spacing.\"\"\"\n        container = ttk.Frame(self.chat_inner_frame)\n        container.pack(fill=\"x\", padx=10, pady=2)  # Reduce padding around container\n    \n        header = tk.Text(container, height=1, font=(\"TkDefaultFont\", 10, \"bold\"),\n                        relief=\"flat\", background=self.bg_color, highlightthickness=0)\n        header.insert(\"1.0\", f\"{sender}:\")\n        header.configure(state=\"disabled\")\n        header.pack(fill=\"x\")\n    \n        # Dynamic height for content\n        body = tk.Text(container, wrap=\"word\", relief=\"flat\",\n                      background=self.bg_color, highlightthickness=0)\n        body.insert(\"1.0\", text)\n        body.configure(state=\"disabled\")\n        body.pack(fill=\"both\", expand=True)\n    \n        if sender == \"Assistant\":\n            self.current_message_label = body\n            self.current_message_container = container\n    \n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n    \n    def _append_to_last_chat(self, text: str):\n        \"\"\"Append text and grow the text widget height dynamically.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.configure(state=\"normal\")\n            self.current_message_label.insert(\"end\", text)\n            \n            line_count = int(self.current_message_label.index(\"end-1c\").split(\".\")[0])\n            self.current_message_label.configure(height=line_count, state=\"disabled\")\n            \n            self.chat_canvas.update_idletasks()\n            self.chat_canvas.yview_moveto(1.0)\n\n\nif __name__ == \"__main__\":\n    root = tk.Tk()\n    ChatUI(root)\n    root.mainloop()\n\n\n"
  },
  {
    "role": "assistant",
    "content": "The real fix is to stop guessing a height and instead let Tk compute the *minimum* height that avoids scrollbars.  \nReplace the two helper methods that create the message body with versions that:\n\n1. Remove any hard-coded `height=…`  \n2. Set `\"wrap=word\"`  \n3. Disable the widget *after* the text is inserted (so Tk can measure the wrapped lines)  \n4. Ask the widget for the number of screen lines it actually needs and clamp it between 1 and a sane maximum (12 keeps short messages compact but still shows ~12 lines of pasted code).\n\n```python\n    def _append_chat(self, sender: str, text: str):\n        \"\"\"Compact vertical spacing; dynamic height for any length.\"\"\"\n        container = ttk.Frame(self.chat_inner_frame)\n        container.pack(fill=\"x\", padx=10, pady=2)\n\n        header = tk.Text(container, height=1, font=(\"TkDefaultFont\", 10, \"bold\"),\n                        relief=\"flat\", bg=self.bg_color, highlightthickness=0)\n        header.insert(\"1.0\", f\"{sender}:\")\n        header.configure(state=\"disabled\")\n        header.pack(fill=\"x\")\n\n        # ---- body that sizes itself ----\n        body = tk.Text(container, wrap=\"word\", relief=\"flat\",\n                      bg=self.bg_color, highlightthickness=0)\n        body.insert(\"1.0\", text)\n        body.configure(state=\"disabled\")\n\n        # ask the widget how many screen-lines it needs\n        body.update_idletasks()                # make sure metrics are current\n        lines = int(body.index(\"end-1c\").split(\".\")[0])\n        body.config(height=max(1, min(lines, 12)))   # 1 ≤ height ≤ 12\n\n        body.pack(fill=\"x\")\n\n        if sender == \"Assistant\":\n            self.current_message_label = body\n            self.current_message_container = container\n\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _append_to_last_chat(self, text: str):\n        \"\"\"Grow the *same* widget while streaming.\"\"\"\n        if self.current_message_label:\n            lbl = self.current_message_label\n            lbl.configure(state=\"normal\")\n            lbl.insert(\"end\", text)\n            lbl.configure(state=\"disabled\")\n\n            lbl.update_idletasks()\n            lines = int(lbl.index(\"end-1c\").split(\".\")[0])\n            lbl.configure(height=max(1, min(lines, 12)))\n```\n\nWith these two changes short messages stay at 1-line height and long pasted code expands only as far as the 12-line cap—no wasted blank space, no cut-off text."
  },
  {
    "role": "user",
    "content": "Now every user message box is too small."
  },
  {
    "role": "assistant",
    "content": "The clamp (`max(1, min(lines, 12))`) is now **capping every message at 1 line** because the widget’s height is being measured **before** it is given its final width.  \nFix: measure **after** the parent frame has been rendered with the correct width, then cap only if the user is not the sender (so *your* short messages stay compact while *assistant* blocks can still grow).\n\nReplace the two helpers with:\n\n```python\n# -----------------------------\n# UI Helpers\n# -----------------------------\ndef _append_chat(self, sender: str, text: str):\n    \"\"\"Compact for user, flexible for assistant.\"\"\"\n    container = ttk.Frame(self.chat_inner_frame)\n    container.pack(fill=\"x\", padx=10, pady=2)\n\n    header = tk.Text(container, height=1, font=(\"TkDefaultFont\", 10, \"bold\"),\n                    relief=\"flat\", bg=self.bg_color, highlightthickness=0)\n    header.insert(\"1.0\", f\"{sender}:\")\n    header.configure(state=\"disabled\")\n    header.pack(fill=\"x\")\n\n    body = tk.Text(container, wrap=\"word\", relief=\"flat\",\n                  bg=self.bg_color, highlightthickness=0)\n    body.insert(\"1.0\", text)\n    body.configure(state=\"disabled\")\n    body.pack(fill=\"x\")\n\n    # ---- correct-height once geometry is stable ----\n    def _set_height():\n        body.update_idletasks()\n        lines = int(body.index(\"end-1c\").split(\".\")[0])\n        if sender == \"user\":                 # keep user tiny\n            body.configure(height=max(1, lines))\n        else:                              # assistant may grow\n            body.configure(height=max(1, min(lines, 20)))\n    self.root.after_idle(_set_height)\n\n    if sender == \"Assistant\":\n        self.current_message_label = body\n        self.current_message_container = container\n\n    self.chat_canvas.update_idletasks()\n    self.chat_canvas.yview_moveto(1.0)\n\ndef _append_to_last_chat(self, text: str):\n    \"\"\"Streaming: grow the assistant bubble.\"\"\"\n    if self.current_message_label:\n        lbl = self.current_message_label\n        lbl.configure(state=\"normal\")\n        lbl.insert(\"end\", text)\n        lbl.configure(state=\"disabled\")\n\n        lbl.update_idletasks()\n        lines = int(lbl.index(\"end-1c\").split(\".\")[0])\n        lbl.configure(height=max(1, min(lines, 20)))  # assistant only\n```\n\nNow **user** message boxes shrink to the exact line count (no wasted space) while **assistant** boxes (including code) can expand up to 20 lines before they start scrolling."
  },
  {
    "role": "user",
    "content": "The code for sizing assistant boxes works perfectly, but the user boxes still are too small and require scrolling to see the text."
  },
  {
    "role": "user",
    "content": "Here's my current code:\nimport os\nimport re\nimport threading\nimport queue\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nimport json\n\n# -----------------------------\n# Environment / Client Setup\n# -----------------------------\n\nload_dotenv()\n\ngroq_client = OpenAI(\n    api_key=os.getenv(\"GROQ_API_KEY\"),\n    base_url=\"https://api.groq.com/openai/v1\",\n)\n\nopenai_client = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\n# Model configurations\nGROQ_MODELS = [\n    \"llama-3.3-70b-versatile\",\n    \"moonshotai/kimi-k2-instruct\",\n    \"moonshotai/kimi-k2-instruct-0905\",\n    \"openai/gpt-oss-120b\",\n    \"openai/gpt-oss-20b\",\n    \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n]\n\nOPENAI_MODELS = [\n    \"gpt-5.2\",\n    \"gpt-5-mini\",\n    \"gpt-4.1\",\n]\n\nALL_MODELS = GROQ_MODELS + OPENAI_MODELS\n\nCODE_BLOCK_REGEX = re.compile(\n    r\"```(?:\\w+)?\\s*(.*?)```\",\n    re.DOTALL\n)\n\n# -----------------------------\n# UI\n# -----------------------------\nclass CodeProposal(ttk.Frame):\n    def __init__(self, parent, code: str, on_replace, on_add):\n        super().__init__(parent)\n        self.code = code\n\n        # Calculate height: number of lines + 1\n        line_count = code.count(\"\\n\") + 1\n        \n        self.text = tk.Text(\n            self,\n            height=line_count,\n            wrap=\"none\",\n            font=(\"Courier New\", 10),\n            padx=5,\n            pady=5,\n            highlightthickness=1 # Border for code blocks\n        )\n        self.text.insert(\"1.0\", code)\n        self.text.configure(state=\"disabled\")\n        self.text.pack(fill=\"x\", padx=5, pady=(5, 2))\n\n        button_row = ttk.Frame(self)\n        button_row.pack(anchor=\"e\", padx=5, pady=(0, 5))\n\n        ttk.Button(button_row, text=\"Replace\", command=lambda: on_replace(code)).pack(side=\"right\", padx=2)\n        ttk.Button(button_row, text=\"Add\", command=lambda: on_add(code)).pack(side=\"right\", padx=2)\n\nclass ChatUI:\n    def __init__(self, root: tk.Tk):\n        self.root = root\n        self.root.title(\"Chat + Code UI\")\n        self.root.geometry(\"1100x650\")\n\n        self.conversation = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n        ]\n\n        self.token_queue = queue.Queue()\n        self.is_generating = False\n        self.current_assistant_buffer = \"\"\n\n        self.code_contents = \"\"\n\n        # ---- persistence helpers ----\n        self.auto_save_path: str | None = None\n        self.auto_save_after_id: str | None = None\n        self.auto_save_interval_ms = 30_000\n        \n        # Default models\n        self.chat_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n        self.helper_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n\n        self._build_layout()\n        self._bind_keys()\n        self._poll_tokens()\n\n    # -----------------------------\n    # Layout\n    # -----------------------------\n\n    def _build_layout(self) -> None:\n        self.root.rowconfigure(0, weight=0)\n        self.root.rowconfigure(1, weight=1)\n        self.root.rowconfigure(2, weight=0)\n        self.root.columnconfigure(0, weight=1)\n\n        # ----------------------\n        # Model Selection Bar\n        # ----------------------\n        model_frame = ttk.Frame(self.root)\n        model_frame.grid(row=0, column=0, sticky=\"ew\", padx=5, pady=5)\n        \n        ttk.Label(model_frame, text=\"Chat Model:\").pack(side=\"left\", padx=(0, 5))\n        chat_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.chat_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        chat_dropdown.pack(side=\"left\", padx=(0, 20))\n        \n        ttk.Label(model_frame, text=\"Helper Model:\").pack(side=\"left\", padx=(0, 5))\n        helper_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.helper_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        helper_dropdown.pack(side=\"left\")\n\n        # ----------------------\n        # Menu bar\n        # ----------------------\n        menubar = tk.Menu(self.root)\n        file_menu = tk.Menu(menubar, tearoff=0)\n        file_menu.add_command(label=\"Save conversation…\", command=self.save_conversation)\n        file_menu.add_command(label=\"Load conversation…\", command=self.load_conversation)\n        file_menu.add_separator()\n        file_menu.add_command(label=\"Export markdown…\", command=self.export_markdown)\n        menubar.add_cascade(label=\"File\", menu=file_menu)\n        self.root.config(menu=menubar)\n\n        main_pane = ttk.PanedWindow(self.root, orient=\"horizontal\")\n        main_pane.grid(row=1, column=0, sticky=\"nsew\")\n\n        # ----------------------\n        # Chat + Code Proposals\n        # ----------------------\n        chat_frame = ttk.Frame(main_pane)\n        chat_frame.rowconfigure(0, weight=1)\n        chat_frame.columnconfigure(0, weight=1)\n\n        # Scrollable canvas\n        self.chat_canvas = tk.Canvas(chat_frame)\n        self.chat_scrollbar = ttk.Scrollbar(chat_frame, command=self.chat_canvas.yview)\n        self.chat_canvas.configure(yscrollcommand=self.chat_scrollbar.set)\n\n        self.chat_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n        self.chat_canvas.grid(row=0, column=0, sticky=\"nsew\")\n\n        # Inner frame to hold all messages\n        self.chat_inner_frame = ttk.Frame(self.chat_canvas)\n        self.chat_canvas.create_window((0, 0), window=self.chat_inner_frame, anchor=\"nw\")\n\n        # Update scroll region when content changes\n        self.chat_inner_frame.bind(\n            \"<Configure>\",\n            lambda e: self.chat_canvas.configure(scrollregion=self.chat_canvas.bbox(\"all\"))\n        )\n\n        # Track the current message label for streaming\n        self.current_message_label = None\n        \n        # Get background color from a standard widget\n        self.bg_color = self.root.cget(\"background\")\n\n        # ----------------------\n        # Code Panel\n        # ----------------------\n        code_frame = ttk.Frame(main_pane)\n        code_frame.rowconfigure(0, weight=1)\n        code_frame.columnconfigure(0, weight=1)\n\n        self.code_panel = tk.Text(code_frame, wrap=\"none\")\n        self.code_panel.grid(row=0, column=0, sticky=\"nsew\")\n        \n        # Bind to sync code_contents when user edits\n        self.code_panel.bind(\"<<Modified>>\", self._on_code_modified)\n\n        def select_all_code_panel(event):\n            event.widget.tag_add(\"sel\", \"1.0\", \"end\")\n            return \"break\"\n\n        self.code_panel.bind(\"<Control-a>\", select_all_code_panel)\n        self.code_panel.bind(\"<Control-A>\", select_all_code_panel)\n        self.code_panel.bind(\"<Command-a>\", select_all_code_panel)  # For Mac\n\n        code_scroll = ttk.Scrollbar(code_frame, command=self.code_panel.yview)\n        code_scroll.grid(row=0, column=1, sticky=\"ns\")\n        self.code_panel[\"yscrollcommand\"] = code_scroll.set\n\n        main_pane.add(chat_frame)\n        main_pane.add(code_frame)\n\n        # ----------------------\n        # Input Box\n        # ----------------------\n        self.input_box = tk.Text(self.root, height=5, wrap=\"word\")\n        self.input_box.grid(row=2, column=0, sticky=\"nsew\")\n\n        self.root.update_idletasks()\n        main_pane.sashpos(0, int(self.root.winfo_width() * 0.70))\n    \n    def _get_client_for_model(self, model: str):\n        \"\"\"Return the appropriate client for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return openai_client\n        else:\n            return groq_client\n    \n    def _get_max_tokens_for_model(self, model: str):\n        \"\"\"Return appropriate max_tokens for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return 16000\n        else:\n            return 8000\n    \n    def _uses_max_completion_tokens(self, model: str):\n        \"\"\"Check if model uses max_completion_tokens instead of max_tokens.\"\"\"\n        return model in [\"gpt-5.2\", \"gpt-5-mini\"]\n\n    def _bind_keys(self):\n        self.input_box.bind(\"<Return>\", self._on_enter)\n        self.input_box.bind(\"<Shift-Return>\", lambda e: None)\n    \n    def _on_code_modified(self, event):\n        \"\"\"Sync code_contents when user edits the code panel.\"\"\"\n        if self.code_panel.edit_modified():\n            self.code_contents = self.code_panel.get(\"1.0\", \"end-1c\")\n            self.code_panel.edit_modified(False)\n\n    # -----------------------------\n    # Input\n    # -----------------------------\n\n    def _on_enter(self, event):\n        if self.is_generating:\n            return \"break\"\n\n        text = self.input_box.get(\"1.0\", \"end-1c\").strip()\n        if not text:\n            return \"break\"\n\n        self.input_box.delete(\"1.0\", \"end\")\n        self._append_chat(\"You\", text)\n        \n        # Add user message to conversation\n        self.conversation.append({\"role\": \"user\", \"content\": text})\n        \n        # Add invisible context about current code if it exists\n        if self.code_contents.strip():\n            self.conversation.append({\n                \"role\": \"user\",\n                \"content\": f\"Here's my current code:\\n{self.code_contents}\"\n            })\n\n        self._start_generation()\n        return \"break\"\n\n    # -----------------------------\n    # Chat Generation\n    # -----------------------------\n\n    def _start_generation(self):\n        self.is_generating = True\n        self.input_box.configure(state=\"disabled\")\n        self.current_assistant_buffer = \"\"\n\n        threading.Thread(\n            target=self._stream_chat_response, daemon=True\n        ).start()\n\n    def _stream_chat_response(self):\n        try:\n            model = self.chat_model.get()\n            client = self._get_client_for_model(model)\n            max_tokens = self._get_max_tokens_for_model(model)\n            \n            # Build request parameters\n            params = {\n                \"model\": model,\n                \"messages\": self.conversation,\n                \"stream\": True,\n            }\n            \n            # Use max_completion_tokens for GPT-5.2 and GPT-5-mini\n            if self._uses_max_completion_tokens(model):\n                params[\"max_completion_tokens\"] = max_tokens\n            else:\n                params[\"max_tokens\"] = max_tokens\n            \n            stream = client.chat.completions.create(**params)\n\n            for chunk in stream:\n                delta = chunk.choices[0].delta.content or \"\"\n                if delta:\n                    self.token_queue.put(delta)\n\n        except Exception as e:\n            self.token_queue.put(f\"\\n[Error: {e}]\")\n\n        finally:\n            self.token_queue.put(None)\n\n    def _poll_tokens(self):\n        try:\n            while True:\n                token = self.token_queue.get_nowait()\n    \n                if token is None:\n                    self._finalize_assistant_message()\n                    break\n    \n                self.current_assistant_buffer += token\n                self._append_to_last_chat(token)\n    \n        except queue.Empty:\n            pass\n        except Exception as e:\n            print(f\"Error polling tokens: {e}\")\n    \n        self.root.after(20, self._poll_tokens)\n    \n    def _finalize_assistant_message(self):\n        assistant_text = self.current_assistant_buffer\n        self.conversation.append({\"role\": \"assistant\", \"content\": assistant_text})\n\n        self.is_generating = False\n        self.input_box.configure(state=\"normal\")\n\n        # Replace the streaming label with parsed content\n        self._replace_with_parsed_message(assistant_text)\n\n    def _replace_with_parsed_message(self, text: str):\n        \"\"\"Parse the message and replace code blocks with interactive widgets.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.destroy()\n            self.current_message_label = None\n\n        # Create a container for this message\n        message_container = ttk.Frame(self.chat_inner_frame)\n        message_container.pack(fill=\"x\", padx=5, pady=5)\n\n        # Add sender as copyable text\n        sender_text = tk.Text(\n            message_container,\n            height=1,\n            wrap=\"none\",\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            background=self.bg_color\n        )\n        sender_text.insert(\"1.0\", \"Assistant:\")\n        sender_text.configure(state=\"disabled\")\n        sender_text.pack(fill=\"x\")\n\n        # Split text by code blocks\n        parts = self._split_text_and_code(text)\n        \n        for part_type, content in parts:\n            if part_type == \"text\":\n                if content.strip():\n                    text_widget = tk.Text(\n                        message_container,\n                        height=content.count(\"\\n\") + 1 + int(len(content) * 0.012),\n                        wrap=\"word\",\n                        relief=\"flat\",\n                        background=self.bg_color\n                    )\n                    text_widget.insert(\"1.0\", content)\n                    text_widget.configure(state=\"disabled\")\n                    text_widget.pack(fill=\"x\", padx=5, pady=2)\n            elif part_type == \"code\":\n                proposal = CodeProposal(\n                    parent=message_container,\n                    code=content,\n                    on_replace=self._replace_code_panel,\n                    on_add=self._add_code_via_helper,\n                )\n                proposal.pack(fill=\"x\", pady=5)\n\n        # Scroll to bottom\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _split_text_and_code(self, text: str):\n        \"\"\"Split text into alternating text and code blocks.\"\"\"\n        parts = []\n        last_end = 0\n        \n        for match in CODE_BLOCK_REGEX.finditer(text):\n            # Add text before code block\n            if match.start() > last_end:\n                parts.append((\"text\", text[last_end:match.start()]))\n            \n            # Add code block\n            parts.append((\"code\", match.group(1)))\n            last_end = match.end()\n        \n        # Add remaining text after last code block\n        if last_end < len(text):\n            parts.append((\"text\", text[last_end:]))\n        \n        return parts\n\n    # -----------------------------\n    # Code Handling (Line-Number Based Approach)\n    # -----------------------------\n\n    def _add_line_numbers(self, code: str) -> str:\n        \"\"\"Prepend line numbers to each line of code.\"\"\"\n        lines = code.split(\"\\n\")\n        numbered_lines = [f\"{i+1} | {line}\" for i, line in enumerate(lines)]\n        return \"\\n\".join(numbered_lines)\n\n    def _add_code_via_helper(self, new_code: str):\n        \"\"\"Ask helper model for insertion point and lines to delete, then apply.\"\"\"\n        if not self.code_contents.strip():\n            self._replace_code_panel(new_code)\n            return\n\n        # Get insertion instructions from helper model\n        insertion_data = self._get_insertion_instructions(new_code)\n        \n        if not insertion_data:\n            self._append_chat(\"System\", \"Helper model did not return valid instructions.\")\n            return\n\n        # Try to apply the insertion and deletion\n        try:\n            updated_code = self._apply_line_based_edit(\n                insertion_data[\"insertion_point\"],\n                insertion_data[\"delete_lines\"],\n                new_code,\n                insertion_data[\"indent_spaces\"]\n            )\n            self.code_contents = updated_code\n            self.code_panel.delete(\"1.0\", \"end\")\n            self.code_panel.insert(\"1.0\", updated_code)\n            self._append_chat(\"System\", \"Code updated successfully!\")\n        except Exception as e:\n            self._append_chat(\"System\", f\"Edit failed: {e}\")\n\n    def _get_insertion_instructions(self, new_code: str) -> dict | None:\n        \"\"\"Ask helper model for insertion point and lines to delete using line numbers.\"\"\"\n        # Add line numbers to current code\n        numbered_code = self._add_line_numbers(self.code_contents)\n        \n        system_prompt = (\n            \"You are a code editing assistant. Your job is to determine where to insert new code \"\n            \"and which lines (if any) to delete from the existing code.\\n\\n\"\n            \"You will receive:\\n\"\n            \"1. The current code with line numbers (format: '1 | code here')\\n\"\n            \"2. New code to insert\\n\\n\"\n            \"You must respond ONLY with JSON in this exact format:\\n\"\n            \"{\\n\"\n            '  \"insertion_point\": <line_number>,\\n'\n            '  \"delete_lines\": [<line_numbers>],\\n'\n            '  \"indent_spaces\": <number_of_spaces>\\n'\n            \"}\\n\\n\"\n            \"Rules:\\n\"\n            \"- insertion_point: The line number AFTER which to insert the new code (use 0 to insert at the beginning)\\n\"\n            \"- delete_lines: Array of line numbers to delete (can be empty [] if just inserting)\\n\"\n            \"- indent_spaces: Number of spaces to add to the beginning of each line of inserted code (to match surrounding indentation)\\n\"\n            \"- Use the CURRENT line numbers you see - don't worry about how insertion affects numbering\\n\"\n            \"- If replacing code, include those line numbers in delete_lines\\n\"\n            \"- If just inserting, leave delete_lines empty\\n\"\n            \"- Look at the indentation of surrounding code to determine indent_spaces\\n\\n\"\n            \"Example 1 - Replace lines 5-7 with 8 spaces of indentation:\\n\"\n            '{\"insertion_point\": 4, \"delete_lines\": [5, 6, 7], \"indent_spaces\": 8}\\n\\n'\n            \"Example 2 - Insert after line 10 with 4 spaces:\\n\"\n            '{\"insertion_point\": 10, \"delete_lines\": [], \"indent_spaces\": 4}\\n\\n'\n            \"Example 3 - Insert at beginning with no indentation:\\n\"\n            '{\"insertion_point\": 0, \"delete_lines\": [], \"indent_spaces\": 0}\\n\\n'\n            \"Respond with ONLY the JSON, no explanations.\"\n        )\n\n        user_prompt = (\n            \"CURRENT CODE (with line numbers):\\n\"\n            f\"{numbered_code}\\n\\n\"\n            \"NEW CODE TO INSERT:\\n\"\n            f\"{new_code}\\n\"\n        )\n\n        model = self.helper_model.get()\n        client = self._get_client_for_model(model)\n        max_tokens = self._get_max_tokens_for_model(model)\n        \n        params = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n        }\n        \n        if self._uses_max_completion_tokens(model):\n            params[\"max_completion_tokens\"] = max_tokens\n        else:\n            params[\"max_tokens\"] = max_tokens\n        \n        try:\n            response = client.chat.completions.create(**params)\n            response_text = response.choices[0].message.content or \"\"\n            \n            print(\"Helper response:\", response_text)\n            \n            # Extract JSON from response (in case model adds markdown)\n            json_match = re.search(r'\\{[^}]+\\}', response_text, re.DOTALL)\n            if json_match:\n                response_text = json_match.group(0)\n            \n            data = json.loads(response_text)\n            \n            # Validate response format\n            if \"insertion_point\" not in data or \"delete_lines\" not in data or \"indent_spaces\" not in data:\n                return None\n            \n            if not isinstance(data[\"insertion_point\"], int):\n                return None\n            \n            if not isinstance(data[\"delete_lines\"], list):\n                return None\n            \n            if not isinstance(data[\"indent_spaces\"], int):\n                return None\n            \n            return data\n            \n        except Exception as e:\n            print(f\"Error getting insertion instructions: {e}\")\n            return None\n\n    def _apply_line_based_edit(self, insertion_point: int, delete_lines: list, new_code: str, indent_spaces: int) -> str:\n        \"\"\"Apply insertion and deletion based on line numbers, with indentation.\"\"\"\n        lines = self.code_contents.split(\"\\n\")\n        \n        # Validate line numbers\n        if insertion_point < 0 or insertion_point > len(lines):\n            raise ValueError(f\"Invalid insertion point: {insertion_point}\")\n        \n        for line_num in delete_lines:\n            if line_num < 1 or line_num > len(lines):\n                raise ValueError(f\"Invalid line number to delete: {line_num}\")\n        \n        # Convert delete_lines to 0-indexed and sort in reverse order\n        delete_indices = sorted([line_num - 1 for line_num in delete_lines], reverse=True)\n        \n        # Adjust insertion point if we're deleting lines before it\n        adjusted_insertion = insertion_point\n        for idx in delete_indices:\n            if idx < insertion_point:\n                adjusted_insertion -= 1\n        \n        # Delete lines (in reverse order to preserve indices)\n        for idx in delete_indices:\n            del lines[idx]\n        \n        # Apply indentation to new code\n        indent = \" \" * indent_spaces\n        new_lines = new_code.split(\"\\n\")\n        indented_lines = [indent + line for line in new_lines]\n        \n        # Insert new code at the adjusted position\n        lines[adjusted_insertion:adjusted_insertion] = indented_lines\n        \n        return \"\\n\".join(lines)\n\n    def _replace_code_panel(self, code: str):\n        self.code_contents = code\n        self.code_panel.delete(\"1.0\", \"end\")\n        self.code_panel.insert(\"1.0\", code)\n\n    # -----------------------------\n    # Persistence\n    # -----------------------------\n    def save_conversation(self, path: str | None = None):\n        if path is None:\n            path = filedialog.asksaveasfilename(\n                defaultextension=\".json\",\n                filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n            )\n            if not path:\n                return\n        try:\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self.conversation, f, indent=2, ensure_ascii=False)\n            self.auto_save_path = path\n            messagebox.showinfo(\"Saved\", f\"Conversation saved to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Save error\", str(e))\n\n    def load_conversation(self):\n        path = filedialog.askopenfilename(\n            defaultextension=\".json\",\n            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                loaded = json.load(f)\n            if not isinstance(loaded, list):\n                raise ValueError(\"File must contain a list of messages.\")\n            self.conversation = loaded\n            self.auto_save_path = path\n            self._redraw_chat()\n            messagebox.showinfo(\"Loaded\", f\"Conversation loaded from\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Load error\", str(e))\n\n    def export_markdown(self):\n        path = filedialog.asksaveasfilename(\n            defaultextension=\".md\",\n            filetypes=[(\"Markdown files\", \"*.md\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            lines = []\n            for msg in self.conversation:\n                role = msg.get(\"role\", \"\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue\n                name = \"You\" if role == \"user\" else \"Assistant\"\n                lines.append(f\"**{name}:**\\n{content}\\n\")\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\\n\".join(lines))\n            messagebox.showinfo(\"Exported\", f\"Markdown exported to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Export error\", str(e))\n\n    def _redraw_chat(self):\n        for w in self.chat_inner_frame.winfo_children():\n            w.destroy()\n        self.current_message_label = None\n        for msg in self.conversation:\n            role = msg.get(\"role\")\n            content = msg.get(\"content\", \"\")\n            if role == \"system\":\n                continue\n            sender = \"You\" if role == \"user\" else \"Assistant\"\n            if role == \"assistant\":\n                self._replace_with_parsed_message(content)\n            else:\n                self._append_chat(sender, content)\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n    def _append_chat(self, sender: str, text: str):\n        \"\"\"Compact for user, flexible for assistant.\"\"\"\n        container = ttk.Frame(self.chat_inner_frame)\n        container.pack(fill=\"x\", padx=10, pady=2)\n    \n        header = tk.Text(container, height=1, font=(\"TkDefaultFont\", 10, \"bold\"),\n                        relief=\"flat\", bg=self.bg_color, highlightthickness=0)\n        header.insert(\"1.0\", f\"{sender}:\")\n        header.configure(state=\"disabled\")\n        header.pack(fill=\"x\")\n    \n        body = tk.Text(container, wrap=\"word\", relief=\"flat\",\n                      bg=self.bg_color, highlightthickness=0)\n        body.insert(\"1.0\", text)\n        body.configure(state=\"disabled\")\n        body.pack(fill=\"x\")\n    \n        # ---- correct-height once geometry is stable ----\n        def _set_height():\n            body.update_idletasks()\n            lines = int(body.index(\"end-1c\").split(\".\")[0])\n            if sender == \"user\":                 # keep user tiny\n                body.configure(height=max(1, lines))\n            else:                              # assistant may grow\n                body.configure(height=max(1, min(lines, 20)))\n        self.root.after_idle(_set_height)\n    \n        if sender == \"Assistant\":\n            self.current_message_label = body\n            self.current_message_container = container\n    \n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n    \n    def _append_to_last_chat(self, text: str):\n        \"\"\"Streaming: grow the assistant bubble.\"\"\"\n        if self.current_message_label:\n            lbl = self.current_message_label\n            lbl.configure(state=\"normal\")\n            lbl.insert(\"end\", text)\n            lbl.configure(state=\"disabled\")\n    \n            lbl.update_idletasks()\n            lines = int(lbl.index(\"end-1c\").split(\".\")[0])\n            lbl.configure(height=max(1, min(lines, 20)))  # assistant only\n    \n        \"\"\"Grow the *same* widget while streaming.\"\"\"\n        if self.current_message_label:\n            lbl = self.current_message_label\n            lbl.configure(state=\"normal\")\n            lbl.insert(\"end\", text)\n            lbl.configure(state=\"disabled\")\n\n            lbl.update_idletasks()\n            lines = int(lbl.index(\"end-1c\").split(\".\")[0])\n            lbl.configure(height=max(1, min(lines, 12)))\n    \n\nif __name__ == \"__main__\":\n    root = tk.Tk()\n    ChatUI(root)\n    root.mainloop()\n\n\n\n\n"
  },
  {
    "role": "assistant",
    "content": "Your user bubbles are too small because you’re sizing them using **logical lines**:\n\n```python\nlines = int(body.index(\"end-1c\").split(\".\")[0])\n```\n\nThat returns the number of *newline* lines, not the number of **wrapped display lines**. So a long paragraph (or long pasted code with wrap=word) may be 1 logical line but needs many display lines → the Text widget shows scrollbars.\n\nFix: size by **display lines** using `count(\"1.0\", \"end-1c\", \"displaylines\")`.\n\nAlso: your `_append_to_last_chat` is duplicated and the second half caps to 12, overriding the good behavior. Remove the duplicate.\n\n### Patch (drop-in replacement)\n\nReplace your `_append_chat` and `_append_to_last_chat` with this:\n\n```python\ndef _append_chat(self, sender: str, text: str):\n    container = ttk.Frame(self.chat_inner_frame)\n    container.pack(fill=\"x\", padx=10, pady=2)\n\n    header = tk.Text(\n        container,\n        height=1,\n        font=(\"TkDefaultFont\", 10, \"bold\"),\n        relief=\"flat\",\n        bg=self.bg_color,\n        highlightthickness=0,\n    )\n    header.insert(\"1.0\", f\"{sender}:\")\n    header.configure(state=\"disabled\")\n    header.pack(fill=\"x\")\n\n    body = tk.Text(\n        container,\n        wrap=\"word\",\n        relief=\"flat\",\n        bg=self.bg_color,\n        highlightthickness=0,\n    )\n    body.insert(\"1.0\", text)\n    body.configure(state=\"disabled\")\n    body.pack(fill=\"x\")\n\n    def _set_height():\n        body.update_idletasks()\n        # displaylines counts wrapped lines (what you actually see)\n        display_lines = body.count(\"1.0\", \"end-1c\", \"displaylines\")[0]\n        max_lines = 20  # feel free to tune\n        body.configure(height=max(1, min(display_lines, max_lines)))\n\n    self.root.after_idle(_set_height)\n\n    if sender == \"Assistant\":\n        self.current_message_label = body\n        self.current_message_container = container\n\n    self.chat_canvas.update_idletasks()\n    self.chat_canvas.yview_moveto(1.0)\n\n\ndef _append_to_last_chat(self, text: str):\n    \"\"\"Streaming: grow the assistant bubble by display lines.\"\"\"\n    if not self.current_message_label:\n        return\n\n    lbl = self.current_message_label\n    lbl.configure(state=\"normal\")\n    lbl.insert(\"end\", text)\n    lbl.configure(state=\"disabled\")\n\n    lbl.update_idletasks()\n    display_lines = lbl.count(\"1.0\", \"end-1c\", \"displaylines\")[0]\n    lbl.configure(height=max(1, min(display_lines, 20)))\n```\n\nWhat this changes:\n- User boxes grow to fit wrapped text (no scrolling) up to `max_lines`.\n- Assistant streaming remains correct.\n- Removes the duplicated `_append_to_last_chat` logic that was shrinking assistant boxes to 12.\n\nIf you want user bubbles to be allowed to grow more than assistant bubbles (common), set `max_lines = 30` inside `_set_height()` just for user."
  },
  {
    "role": "user",
    "content": "I had to add a +1 in there, but other than that it works perfectly. Now I need to test it, so I am going to keep typing and fill up a lot of lines so that I can see if it wraps them properly. This text should all be visible without scrolling. Time will tell if it is. Oh no, I need to get going to a meeting soon. I hope it's a fun meeting and not a boring one. Anyway, let's see if this works."
  },
  {
    "role": "assistant",
    "content": "It sounds like you're testing the text wrapping feature of your chat application. I'm glad to hear that the changes I suggested are working for you. Adding a `+1` to the `display_lines` calculation makes sense, as it ensures that the text widget is always at least one line tall, even if the text is very short.\n\nI hope your meeting goes well and is more fun than boring! Testing your application with a lot of text is a great way to ensure that it's working as expected. If you have any other questions or need further assistance, feel free to ask.\n\nAlso, keep in mind that you may want to consider adding some kind of limit to the number of lines that a text widget can display. This can help prevent the widget from becoming too large and using up too much memory. But for now, it sounds like your application is working well, and you can continue to test it with long blocks of text."
  },
  {
    "role": "user",
    "content": "Whenever I load this conversation, I get this error. Everything works fine despite the error. How do I patch this? Exception in Tkinter callback\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/tkinter/__init__.py\", line 1967, in __call__\n    return self.func(*args)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/tkinter/__init__.py\", line 861, in callit\n    func(*args)\n  File \"/home/eli/Documents/Python/cheap_code/main.py\", line 709, in _set_height\n    display_lines = body.count(\"1.0\", \"end-1c\", \"displaylines\")[0] + 1\n                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\nTypeError: 'NoneType' object is not subscriptable"
  },
  {
    "role": "user",
    "content": "Here's my current code:\nimport os\nimport re\nimport threading\nimport queue\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nimport json\n\n# -----------------------------\n# Environment / Client Setup\n# -----------------------------\n\nload_dotenv()\n\ngroq_client = OpenAI(\n    api_key=os.getenv(\"GROQ_API_KEY\"),\n    base_url=\"https://api.groq.com/openai/v1\",\n)\n\nopenai_client = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\n# Model configurations\nGROQ_MODELS = [\n    \"llama-3.3-70b-versatile\",\n    \"moonshotai/kimi-k2-instruct\",\n    \"moonshotai/kimi-k2-instruct-0905\",\n    \"openai/gpt-oss-120b\",\n    \"openai/gpt-oss-20b\",\n    \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n]\n\nOPENAI_MODELS = [\n    \"gpt-5.2\",\n    \"gpt-5-mini\",\n    \"gpt-4.1\",\n]\n\nALL_MODELS = GROQ_MODELS + OPENAI_MODELS\n\nCODE_BLOCK_REGEX = re.compile(\n    r\"```(?:\\w+)?\\s*(.*?)```\",\n    re.DOTALL\n)\n\n# -----------------------------\n# UI\n# -----------------------------\nclass CodeProposal(ttk.Frame):\n    def __init__(self, parent, code: str, on_replace, on_add):\n        super().__init__(parent)\n        self.code = code\n\n        # Calculate height: number of lines + 1\n        line_count = code.count(\"\\n\") + 1\n        \n        self.text = tk.Text(\n            self,\n            height=line_count,\n            wrap=\"none\",\n            font=(\"Courier New\", 10),\n            padx=5,\n            pady=5,\n            highlightthickness=1 # Border for code blocks\n        )\n        self.text.insert(\"1.0\", code)\n        self.text.configure(state=\"disabled\")\n        self.text.pack(fill=\"x\", padx=5, pady=(5, 2))\n\n        button_row = ttk.Frame(self)\n        button_row.pack(anchor=\"e\", padx=5, pady=(0, 5))\n\n        ttk.Button(button_row, text=\"Replace\", command=lambda: on_replace(code)).pack(side=\"right\", padx=2)\n        ttk.Button(button_row, text=\"Add\", command=lambda: on_add(code)).pack(side=\"right\", padx=2)\n\nclass ChatUI:\n    def __init__(self, root: tk.Tk):\n        self.root = root\n        self.root.title(\"Chat + Code UI\")\n        self.root.geometry(\"1100x650\")\n\n        self.conversation = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n        ]\n\n        self.token_queue = queue.Queue()\n        self.is_generating = False\n        self.current_assistant_buffer = \"\"\n\n        self.code_contents = \"\"\n\n        # ---- persistence helpers ----\n        self.auto_save_path: str | None = None\n        self.auto_save_after_id: str | None = None\n        self.auto_save_interval_ms = 30_000\n        \n        # Default models\n        self.chat_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n        self.helper_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n\n        self._build_layout()\n        self._bind_keys()\n        self._poll_tokens()\n\n    # -----------------------------\n    # Layout\n    # -----------------------------\n\n    def _build_layout(self) -> None:\n        self.root.rowconfigure(0, weight=0)\n        self.root.rowconfigure(1, weight=1)\n        self.root.rowconfigure(2, weight=0)\n        self.root.columnconfigure(0, weight=1)\n\n        # ----------------------\n        # Model Selection Bar\n        # ----------------------\n        model_frame = ttk.Frame(self.root)\n        model_frame.grid(row=0, column=0, sticky=\"ew\", padx=5, pady=5)\n        \n        ttk.Label(model_frame, text=\"Chat Model:\").pack(side=\"left\", padx=(0, 5))\n        chat_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.chat_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        chat_dropdown.pack(side=\"left\", padx=(0, 20))\n        \n        ttk.Label(model_frame, text=\"Helper Model:\").pack(side=\"left\", padx=(0, 5))\n        helper_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.helper_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        helper_dropdown.pack(side=\"left\")\n\n        # ----------------------\n        # Menu bar\n        # ----------------------\n        menubar = tk.Menu(self.root)\n        file_menu = tk.Menu(menubar, tearoff=0)\n        file_menu.add_command(label=\"Save conversation…\", command=self.save_conversation)\n        file_menu.add_command(label=\"Load conversation…\", command=self.load_conversation)\n        file_menu.add_separator()\n        file_menu.add_command(label=\"Export markdown…\", command=self.export_markdown)\n        menubar.add_cascade(label=\"File\", menu=file_menu)\n        self.root.config(menu=menubar)\n\n        main_pane = ttk.PanedWindow(self.root, orient=\"horizontal\")\n        main_pane.grid(row=1, column=0, sticky=\"nsew\")\n\n        # ----------------------\n        # Chat + Code Proposals\n        # ----------------------\n        chat_frame = ttk.Frame(main_pane)\n        chat_frame.rowconfigure(0, weight=1)\n        chat_frame.columnconfigure(0, weight=1)\n\n        # Scrollable canvas\n        self.chat_canvas = tk.Canvas(chat_frame)\n        self.chat_scrollbar = ttk.Scrollbar(chat_frame, command=self.chat_canvas.yview)\n        self.chat_canvas.configure(yscrollcommand=self.chat_scrollbar.set)\n\n        self.chat_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n        self.chat_canvas.grid(row=0, column=0, sticky=\"nsew\")\n\n        # Inner frame to hold all messages\n        self.chat_inner_frame = ttk.Frame(self.chat_canvas)\n        self.chat_canvas.create_window((0, 0), window=self.chat_inner_frame, anchor=\"nw\")\n\n        # Update scroll region when content changes\n        self.chat_inner_frame.bind(\n            \"<Configure>\",\n            lambda e: self.chat_canvas.configure(scrollregion=self.chat_canvas.bbox(\"all\"))\n        )\n\n        # Track the current message label for streaming\n        self.current_message_label = None\n        \n        # Get background color from a standard widget\n        self.bg_color = self.root.cget(\"background\")\n\n        # ----------------------\n        # Code Panel\n        # ----------------------\n        code_frame = ttk.Frame(main_pane)\n        code_frame.rowconfigure(0, weight=1)\n        code_frame.columnconfigure(0, weight=1)\n\n        self.code_panel = tk.Text(code_frame, wrap=\"none\")\n        self.code_panel.grid(row=0, column=0, sticky=\"nsew\")\n        \n        # Bind to sync code_contents when user edits\n        self.code_panel.bind(\"<<Modified>>\", self._on_code_modified)\n\n        def select_all_code_panel(event):\n            event.widget.tag_add(\"sel\", \"1.0\", \"end\")\n            return \"break\"\n\n        self.code_panel.bind(\"<Control-a>\", select_all_code_panel)\n        self.code_panel.bind(\"<Control-A>\", select_all_code_panel)\n        self.code_panel.bind(\"<Command-a>\", select_all_code_panel)  # For Mac\n\n        code_scroll = ttk.Scrollbar(code_frame, command=self.code_panel.yview)\n        code_scroll.grid(row=0, column=1, sticky=\"ns\")\n        self.code_panel[\"yscrollcommand\"] = code_scroll.set\n\n        main_pane.add(chat_frame)\n        main_pane.add(code_frame)\n\n        # ----------------------\n        # Input Box\n        # ----------------------\n        self.input_box = tk.Text(self.root, height=5, wrap=\"word\")\n        self.input_box.grid(row=2, column=0, sticky=\"nsew\")\n\n        self.root.update_idletasks()\n        main_pane.sashpos(0, int(self.root.winfo_width() * 0.70))\n    \n    def _get_client_for_model(self, model: str):\n        \"\"\"Return the appropriate client for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return openai_client\n        else:\n            return groq_client\n    \n    def _get_max_tokens_for_model(self, model: str):\n        \"\"\"Return appropriate max_tokens for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return 16000\n        else:\n            return 8000\n    \n    def _uses_max_completion_tokens(self, model: str):\n        \"\"\"Check if model uses max_completion_tokens instead of max_tokens.\"\"\"\n        return model in [\"gpt-5.2\", \"gpt-5-mini\"]\n\n    def _bind_keys(self):\n        self.input_box.bind(\"<Return>\", self._on_enter)\n        self.input_box.bind(\"<Shift-Return>\", lambda e: None)\n    \n    def _on_code_modified(self, event):\n        \"\"\"Sync code_contents when user edits the code panel.\"\"\"\n        if self.code_panel.edit_modified():\n            self.code_contents = self.code_panel.get(\"1.0\", \"end-1c\")\n            self.code_panel.edit_modified(False)\n\n    # -----------------------------\n    # Input\n    # -----------------------------\n\n    def _on_enter(self, event):\n        if self.is_generating:\n            return \"break\"\n\n        text = self.input_box.get(\"1.0\", \"end-1c\").strip()\n        if not text:\n            return \"break\"\n\n        self.input_box.delete(\"1.0\", \"end\")\n        self._append_chat(\"You\", text)\n        \n        # Add user message to conversation\n        self.conversation.append({\"role\": \"user\", \"content\": text})\n        \n        # Add invisible context about current code if it exists\n        if self.code_contents.strip():\n            self.conversation.append({\n                \"role\": \"user\",\n                \"content\": f\"Here's my current code:\\n{self.code_contents}\"\n            })\n\n        self._start_generation()\n        return \"break\"\n\n    # -----------------------------\n    # Chat Generation\n    # -----------------------------\n\n    def _start_generation(self):\n        self.is_generating = True\n        self.input_box.configure(state=\"disabled\")\n        self.current_assistant_buffer = \"\"\n\n        threading.Thread(\n            target=self._stream_chat_response, daemon=True\n        ).start()\n\n    def _stream_chat_response(self):\n        try:\n            model = self.chat_model.get()\n            client = self._get_client_for_model(model)\n            max_tokens = self._get_max_tokens_for_model(model)\n            \n            # Build request parameters\n            params = {\n                \"model\": model,\n                \"messages\": self.conversation,\n                \"stream\": True,\n            }\n            \n            # Use max_completion_tokens for GPT-5.2 and GPT-5-mini\n            if self._uses_max_completion_tokens(model):\n                params[\"max_completion_tokens\"] = max_tokens\n            else:\n                params[\"max_tokens\"] = max_tokens\n            \n            stream = client.chat.completions.create(**params)\n\n            for chunk in stream:\n                delta = chunk.choices[0].delta.content or \"\"\n                if delta:\n                    self.token_queue.put(delta)\n\n        except Exception as e:\n            self.token_queue.put(f\"\\n[Error: {e}]\")\n\n        finally:\n            self.token_queue.put(None)\n\n    def _poll_tokens(self):\n        try:\n            while True:\n                token = self.token_queue.get_nowait()\n    \n                if token is None:\n                    self._finalize_assistant_message()\n                    break\n    \n                self.current_assistant_buffer += token\n                self._append_to_last_chat(token)\n    \n        except queue.Empty:\n            pass\n        except Exception as e:\n            print(f\"Error polling tokens: {e}\")\n    \n        self.root.after(20, self._poll_tokens)\n    \n    def _finalize_assistant_message(self):\n        assistant_text = self.current_assistant_buffer\n        self.conversation.append({\"role\": \"assistant\", \"content\": assistant_text})\n\n        self.is_generating = False\n        self.input_box.configure(state=\"normal\")\n\n        # Replace the streaming label with parsed content\n        self._replace_with_parsed_message(assistant_text)\n\n    def _replace_with_parsed_message(self, text: str):\n        \"\"\"Parse the message and replace code blocks with interactive widgets.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.destroy()\n            self.current_message_label = None\n\n        # Create a container for this message\n        message_container = ttk.Frame(self.chat_inner_frame)\n        message_container.pack(fill=\"x\", padx=5, pady=5)\n\n        # Add sender as copyable text\n        sender_text = tk.Text(\n            message_container,\n            height=1,\n            wrap=\"none\",\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            background=self.bg_color\n        )\n        sender_text.insert(\"1.0\", \"Assistant:\")\n        sender_text.configure(state=\"disabled\")\n        sender_text.pack(fill=\"x\")\n\n        # Split text by code blocks\n        parts = self._split_text_and_code(text)\n        \n        for part_type, content in parts:\n            if part_type == \"text\":\n                if content.strip():\n                    text_widget = tk.Text(\n                        message_container,\n                        height=content.count(\"\\n\") + 1 + int(len(content) * 0.012),\n                        wrap=\"word\",\n                        relief=\"flat\",\n                        background=self.bg_color\n                    )\n                    text_widget.insert(\"1.0\", content)\n                    text_widget.configure(state=\"disabled\")\n                    text_widget.pack(fill=\"x\", padx=5, pady=2)\n            elif part_type == \"code\":\n                proposal = CodeProposal(\n                    parent=message_container,\n                    code=content,\n                    on_replace=self._replace_code_panel,\n                    on_add=self._add_code_via_helper,\n                )\n                proposal.pack(fill=\"x\", pady=5)\n\n        # Scroll to bottom\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _split_text_and_code(self, text: str):\n        \"\"\"Split text into alternating text and code blocks.\"\"\"\n        parts = []\n        last_end = 0\n        \n        for match in CODE_BLOCK_REGEX.finditer(text):\n            # Add text before code block\n            if match.start() > last_end:\n                parts.append((\"text\", text[last_end:match.start()]))\n            \n            # Add code block\n            parts.append((\"code\", match.group(1)))\n            last_end = match.end()\n        \n        # Add remaining text after last code block\n        if last_end < len(text):\n            parts.append((\"text\", text[last_end:]))\n        \n        return parts\n\n    # -----------------------------\n    # Code Handling (Line-Number Based Approach)\n    # -----------------------------\n\n    def _add_line_numbers(self, code: str) -> str:\n        \"\"\"Prepend line numbers to each line of code.\"\"\"\n        lines = code.split(\"\\n\")\n        numbered_lines = [f\"{i+1} | {line}\" for i, line in enumerate(lines)]\n        return \"\\n\".join(numbered_lines)\n\n    def _add_code_via_helper(self, new_code: str):\n        \"\"\"Ask helper model for insertion point and lines to delete, then apply.\"\"\"\n        if not self.code_contents.strip():\n            self._replace_code_panel(new_code)\n            return\n\n        # Get insertion instructions from helper model\n        insertion_data = self._get_insertion_instructions(new_code)\n        \n        if not insertion_data:\n            self._append_chat(\"System\", \"Helper model did not return valid instructions.\")\n            return\n\n        # Try to apply the insertion and deletion\n        try:\n            updated_code = self._apply_line_based_edit(\n                insertion_data[\"insertion_point\"],\n                insertion_data[\"delete_lines\"],\n                new_code,\n                insertion_data[\"indent_spaces\"]\n            )\n            self.code_contents = updated_code\n            self.code_panel.delete(\"1.0\", \"end\")\n            self.code_panel.insert(\"1.0\", updated_code)\n            self._append_chat(\"System\", \"Code updated successfully!\")\n        except Exception as e:\n            self._append_chat(\"System\", f\"Edit failed: {e}\")\n\n    def _get_insertion_instructions(self, new_code: str) -> dict | None:\n        \"\"\"Ask helper model for insertion point and lines to delete using line numbers.\"\"\"\n        # Add line numbers to current code\n        numbered_code = self._add_line_numbers(self.code_contents)\n        \n        system_prompt = (\n            \"You are a code editing assistant. Your job is to determine where to insert new code \"\n            \"and which lines (if any) to delete from the existing code.\\n\\n\"\n            \"You will receive:\\n\"\n            \"1. The current code with line numbers (format: '1 | code here')\\n\"\n            \"2. New code to insert\\n\\n\"\n            \"You must respond ONLY with JSON in this exact format:\\n\"\n            \"{\\n\"\n            '  \"insertion_point\": <line_number>,\\n'\n            '  \"delete_lines\": [<line_numbers>],\\n'\n            '  \"indent_spaces\": <number_of_spaces>\\n'\n            \"}\\n\\n\"\n            \"Rules:\\n\"\n            \"- insertion_point: The line number AFTER which to insert the new code (use 0 to insert at the beginning)\\n\"\n            \"- delete_lines: Array of line numbers to delete (can be empty [] if just inserting)\\n\"\n            \"- indent_spaces: Number of spaces to add to the beginning of each line of inserted code (to match surrounding indentation)\\n\"\n            \"- Use the CURRENT line numbers you see - don't worry about how insertion affects numbering\\n\"\n            \"- If replacing code, include those line numbers in delete_lines\\n\"\n            \"- If just inserting, leave delete_lines empty\\n\"\n            \"- Look at the indentation of surrounding code to determine indent_spaces\\n\\n\"\n            \"Example 1 - Replace lines 5-7 with 8 spaces of indentation:\\n\"\n            '{\"insertion_point\": 4, \"delete_lines\": [5, 6, 7], \"indent_spaces\": 8}\\n\\n'\n            \"Example 2 - Insert after line 10 with 4 spaces:\\n\"\n            '{\"insertion_point\": 10, \"delete_lines\": [], \"indent_spaces\": 4}\\n\\n'\n            \"Example 3 - Insert at beginning with no indentation:\\n\"\n            '{\"insertion_point\": 0, \"delete_lines\": [], \"indent_spaces\": 0}\\n\\n'\n            \"Respond with ONLY the JSON, no explanations.\"\n        )\n\n        user_prompt = (\n            \"CURRENT CODE (with line numbers):\\n\"\n            f\"{numbered_code}\\n\\n\"\n            \"NEW CODE TO INSERT:\\n\"\n            f\"{new_code}\\n\"\n        )\n\n        model = self.helper_model.get()\n        client = self._get_client_for_model(model)\n        max_tokens = self._get_max_tokens_for_model(model)\n        \n        params = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n        }\n        \n        if self._uses_max_completion_tokens(model):\n            params[\"max_completion_tokens\"] = max_tokens\n        else:\n            params[\"max_tokens\"] = max_tokens\n        \n        try:\n            response = client.chat.completions.create(**params)\n            response_text = response.choices[0].message.content or \"\"\n            \n            print(\"Helper response:\", response_text)\n            \n            # Extract JSON from response (in case model adds markdown)\n            json_match = re.search(r'\\{[^}]+\\}', response_text, re.DOTALL)\n            if json_match:\n                response_text = json_match.group(0)\n            \n            data = json.loads(response_text)\n            \n            # Validate response format\n            if \"insertion_point\" not in data or \"delete_lines\" not in data or \"indent_spaces\" not in data:\n                return None\n            \n            if not isinstance(data[\"insertion_point\"], int):\n                return None\n            \n            if not isinstance(data[\"delete_lines\"], list):\n                return None\n            \n            if not isinstance(data[\"indent_spaces\"], int):\n                return None\n            \n            return data\n            \n        except Exception as e:\n            print(f\"Error getting insertion instructions: {e}\")\n            return None\n\n    def _apply_line_based_edit(self, insertion_point: int, delete_lines: list, new_code: str, indent_spaces: int) -> str:\n        \"\"\"Apply insertion and deletion based on line numbers, with indentation.\"\"\"\n        lines = self.code_contents.split(\"\\n\")\n        \n        # Validate line numbers\n        if insertion_point < 0 or insertion_point > len(lines):\n            raise ValueError(f\"Invalid insertion point: {insertion_point}\")\n        \n        for line_num in delete_lines:\n            if line_num < 1 or line_num > len(lines):\n                raise ValueError(f\"Invalid line number to delete: {line_num}\")\n        \n        # Convert delete_lines to 0-indexed and sort in reverse order\n        delete_indices = sorted([line_num - 1 for line_num in delete_lines], reverse=True)\n        \n        # Adjust insertion point if we're deleting lines before it\n        adjusted_insertion = insertion_point\n        for idx in delete_indices:\n            if idx < insertion_point:\n                adjusted_insertion -= 1\n        \n        # Delete lines (in reverse order to preserve indices)\n        for idx in delete_indices:\n            del lines[idx]\n        \n        # Apply indentation to new code\n        indent = \" \" * indent_spaces\n        new_lines = new_code.split(\"\\n\")\n        indented_lines = [indent + line for line in new_lines]\n        \n        # Insert new code at the adjusted position\n        lines[adjusted_insertion:adjusted_insertion] = indented_lines\n        \n        return \"\\n\".join(lines)\n\n    def _replace_code_panel(self, code: str):\n        self.code_contents = code\n        self.code_panel.delete(\"1.0\", \"end\")\n        self.code_panel.insert(\"1.0\", code)\n\n    # -----------------------------\n    # Persistence\n    # -----------------------------\n    def save_conversation(self, path: str | None = None):\n        if path is None:\n            path = filedialog.asksaveasfilename(\n                defaultextension=\".json\",\n                filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n            )\n            if not path:\n                return\n        try:\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self.conversation, f, indent=2, ensure_ascii=False)\n            self.auto_save_path = path\n            messagebox.showinfo(\"Saved\", f\"Conversation saved to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Save error\", str(e))\n\n    def load_conversation(self):\n        path = filedialog.askopenfilename(\n            defaultextension=\".json\",\n            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                loaded = json.load(f)\n            if not isinstance(loaded, list):\n                raise ValueError(\"File must contain a list of messages.\")\n            self.conversation = loaded\n            self.auto_save_path = path\n            self._redraw_chat()\n            messagebox.showinfo(\"Loaded\", f\"Conversation loaded from\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Load error\", str(e))\n\n    def export_markdown(self):\n        path = filedialog.asksaveasfilename(\n            defaultextension=\".md\",\n            filetypes=[(\"Markdown files\", \"*.md\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            lines = []\n            for msg in self.conversation:\n                role = msg.get(\"role\", \"\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue\n                name = \"You\" if role == \"user\" else \"Assistant\"\n                lines.append(f\"**{name}:**\\n{content}\\n\")\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\\n\".join(lines))\n            messagebox.showinfo(\"Exported\", f\"Markdown exported to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Export error\", str(e))\n\n    def _redraw_chat(self):\n        for w in self.chat_inner_frame.winfo_children():\n            w.destroy()\n        self.current_message_label = None\n        for msg in self.conversation:\n            role = msg.get(\"role\")\n            content = msg.get(\"content\", \"\")\n            if role == \"system\":\n                continue\n            sender = \"You\" if role == \"user\" else \"Assistant\"\n            if role == \"assistant\":\n                self._replace_with_parsed_message(content)\n            else:\n                self._append_chat(sender, content)\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n    def _append_chat(self, sender: str, text: str):\n        container = ttk.Frame(self.chat_inner_frame)\n        container.pack(fill=\"x\", padx=10, pady=2)\n    \n        header = tk.Text(\n            container,\n            height=1,\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            bg=self.bg_color,\n            highlightthickness=0,\n        )\n        header.insert(\"1.0\", f\"{sender}:\")\n        header.configure(state=\"disabled\")\n        header.pack(fill=\"x\")\n    \n        body = tk.Text(\n            container,\n            wrap=\"word\",\n            relief=\"flat\",\n            bg=self.bg_color,\n            highlightthickness=0,\n        )\n        body.insert(\"1.0\", text)\n        body.configure(state=\"disabled\")\n        body.pack(fill=\"x\")\n    \n        def _set_height():\n            body.update_idletasks()\n            # displaylines counts wrapped lines (what you actually see)\n            display_lines = body.count(\"1.0\", \"end-1c\", \"displaylines\")[0] + 1\n            max_lines = 20  # feel free to tune\n            body.configure(height=max(1, min(display_lines, max_lines)))\n    \n        self.root.after_idle(_set_height)\n    \n        if sender == \"Assistant\":\n            self.current_message_label = body\n            self.current_message_container = container\n    \n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n    \n    \n    def _append_to_last_chat(self, text: str):\n        \"\"\"Streaming: grow the assistant bubble by display lines.\"\"\"\n        if not self.current_message_label:\n            return\n    \n        lbl = self.current_message_label\n        lbl.configure(state=\"normal\")\n        lbl.insert(\"end\", text)\n        lbl.configure(state=\"disabled\")\n    \n        lbl.update_idletasks()\n        display_lines = lbl.count(\"1.0\", \"end-1c\", \"displaylines\")[0]\n        lbl.configure(height=max(1, min(display_lines, 20)))\n    \n        if self.current_message_label:\n            lbl = self.current_message_label\n            lbl.configure(state=\"normal\")\n            lbl.insert(\"end\", text)\n            lbl.configure(state=\"disabled\")\n\n            lbl.update_idletasks()\n            lines = int(lbl.index(\"end-1c\").split(\".\")[0])\n            lbl.configure(height=max(1, min(lines, 12)))\n    \n\nif __name__ == \"__main__\":\n    root = tk.Tk()\n    ChatUI(root)\n    root.mainloop()\n"
  },
  {
    "role": "assistant",
    "content": "The issue seems to be related to the `count` method of the Text widget, which returns `None` when it can't count the lines.\n\nHere's the modified version of `_append_chat` and `_append_to_last_chat` that should fix the issue:\n\n```python\ndef _append_chat(self, sender: str, text: str):\n    container = ttk.Frame(self.chat_inner_frame)\n    container.pack(fill=\"x\", padx=10, pady=2)\n\n    header = tk.Text(\n        container,\n        height=1,\n        font=(\"TkDefaultFont\", 10, \"bold\"),\n        relief=\"flat\",\n        bg=self.bg_color,\n        highlightthickness=0,\n    )\n    header.insert(\"1.0\", f\"{sender}:\")\n    header.configure(state=\"disabled\")\n    header.pack(fill=\"x\")\n\n    body = tk.Text(\n        container,\n        wrap=\"word\",\n        relief=\"flat\",\n        bg=self.bg_color,\n        highlightthickness=0,\n    )\n    body.insert(\"1.0\", text)\n    body.configure(state=\"disabled\")\n    body.pack(fill=\"x\")\n\n    def _set_height():\n        body.update_idletasks()\n        count_result = body.count(\"1.0\", \"end-1c\", \"displaylines\")\n        if count_result is not None:\n            display_lines = count_result[0] + 1\n            max_lines = 20  # feel free to tune\n            body.configure(height=max(1, min(display_lines, max_lines)))\n\n    self.root.after_idle(_set_height)\n\n    if sender == \"Assistant\":\n        self.current_message_label = body\n        self.current_message_container = container\n\n    self.chat_canvas.update_idletasks()\n    self.chat_canvas.yview_moveto(1.0)\n\n\ndef _append_to_last_chat(self, text: str):\n    \"\"\"Streaming: grow the assistant bubble by display lines.\"\"\"\n    if not self.current_message_label:\n        return\n\n    lbl = self.current_message_label\n    lbl.configure(state=\"normal\")\n    lbl.insert(\"end\", text)\n    lbl.configure(state=\"disabled\")\n\n    lbl.update_idletasks()\n    count_result = lbl.count(\"1.0\", \"end-1c\", \"displaylines\")\n    if count_result is not None:\n        display_lines = count_result[0]\n        lbl.configure(height=max(1, min(display_lines, 20)))\n```\n\nThis version checks if `count_result` is `None` before trying to access its first element. If it's `None`, it simply skips setting the height of the Text widget. This should prevent the `TypeError` you're seeing."
  },
  {
    "role": "user",
    "content": "Thanks, it works. I don't like how the \"Here's my current version of the code:\" snippet gets appended into the official conversation object and then saved in the file. While I want to pass that in to the LLM each time (unless it's blank), I don't want it to be saved in the conversation object. That way, the LLM only sees the most recent version of the code when it is called, which keeps the conversation from getting too long and confusing."
  },
  {
    "role": "user",
    "content": "Here's my current code:\nimport os\nimport re\nimport threading\nimport queue\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nimport json\n\n# -----------------------------\n# Environment / Client Setup\n# -----------------------------\n\nload_dotenv()\n\ngroq_client = OpenAI(\n    api_key=os.getenv(\"GROQ_API_KEY\"),\n    base_url=\"https://api.groq.com/openai/v1\",\n)\n\nopenai_client = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\n# Model configurations\nGROQ_MODELS = [\n    \"llama-3.3-70b-versatile\",\n    \"moonshotai/kimi-k2-instruct\",\n    \"moonshotai/kimi-k2-instruct-0905\",\n    \"openai/gpt-oss-120b\",\n    \"openai/gpt-oss-20b\",\n    \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n]\n\nOPENAI_MODELS = [\n    \"gpt-5.2\",\n    \"gpt-5-mini\",\n    \"gpt-4.1\",\n]\n\nALL_MODELS = GROQ_MODELS + OPENAI_MODELS\n\nCODE_BLOCK_REGEX = re.compile(\n    r\"```(?:\\w+)?\\s*(.*?)```\",\n    re.DOTALL\n)\n\n# -----------------------------\n# UI\n# -----------------------------\nclass CodeProposal(ttk.Frame):\n    def __init__(self, parent, code: str, on_replace, on_add):\n        super().__init__(parent)\n        self.code = code\n\n        # Calculate height: number of lines + 1\n        line_count = code.count(\"\\n\") + 1\n        \n        self.text = tk.Text(\n            self,\n            height=line_count,\n            wrap=\"none\",\n            font=(\"Courier New\", 10),\n            padx=5,\n            pady=5,\n            highlightthickness=1 # Border for code blocks\n        )\n        self.text.insert(\"1.0\", code)\n        self.text.configure(state=\"disabled\")\n        self.text.pack(fill=\"x\", padx=5, pady=(5, 2))\n\n        button_row = ttk.Frame(self)\n        button_row.pack(anchor=\"e\", padx=5, pady=(0, 5))\n\n        ttk.Button(button_row, text=\"Replace\", command=lambda: on_replace(code)).pack(side=\"right\", padx=2)\n        ttk.Button(button_row, text=\"Add\", command=lambda: on_add(code)).pack(side=\"right\", padx=2)\n\nclass ChatUI:\n    def __init__(self, root: tk.Tk):\n        self.root = root\n        self.root.title(\"Chat + Code UI\")\n        self.root.geometry(\"1100x650\")\n\n        self.conversation = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n        ]\n\n        self.token_queue = queue.Queue()\n        self.is_generating = False\n        self.current_assistant_buffer = \"\"\n\n        self.code_contents = \"\"\n\n        # ---- persistence helpers ----\n        self.auto_save_path: str | None = None\n        self.auto_save_after_id: str | None = None\n        self.auto_save_interval_ms = 30_000\n        \n        # Default models\n        self.chat_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n        self.helper_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n\n        self._build_layout()\n        self._bind_keys()\n        self._poll_tokens()\n\n    # -----------------------------\n    # Layout\n    # -----------------------------\n\n    def _build_layout(self) -> None:\n        self.root.rowconfigure(0, weight=0)\n        self.root.rowconfigure(1, weight=1)\n        self.root.rowconfigure(2, weight=0)\n        self.root.columnconfigure(0, weight=1)\n\n        # ----------------------\n        # Model Selection Bar\n        # ----------------------\n        model_frame = ttk.Frame(self.root)\n        model_frame.grid(row=0, column=0, sticky=\"ew\", padx=5, pady=5)\n        \n        ttk.Label(model_frame, text=\"Chat Model:\").pack(side=\"left\", padx=(0, 5))\n        chat_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.chat_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        chat_dropdown.pack(side=\"left\", padx=(0, 20))\n        \n        ttk.Label(model_frame, text=\"Helper Model:\").pack(side=\"left\", padx=(0, 5))\n        helper_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.helper_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        helper_dropdown.pack(side=\"left\")\n\n        # ----------------------\n        # Menu bar\n        # ----------------------\n        menubar = tk.Menu(self.root)\n        file_menu = tk.Menu(menubar, tearoff=0)\n        file_menu.add_command(label=\"Save conversation…\", command=self.save_conversation)\n        file_menu.add_command(label=\"Load conversation…\", command=self.load_conversation)\n        file_menu.add_separator()\n        file_menu.add_command(label=\"Export markdown…\", command=self.export_markdown)\n        menubar.add_cascade(label=\"File\", menu=file_menu)\n        self.root.config(menu=menubar)\n\n        main_pane = ttk.PanedWindow(self.root, orient=\"horizontal\")\n        main_pane.grid(row=1, column=0, sticky=\"nsew\")\n\n        # ----------------------\n        # Chat + Code Proposals\n        # ----------------------\n        chat_frame = ttk.Frame(main_pane)\n        chat_frame.rowconfigure(0, weight=1)\n        chat_frame.columnconfigure(0, weight=1)\n\n        # Scrollable canvas\n        self.chat_canvas = tk.Canvas(chat_frame)\n        self.chat_scrollbar = ttk.Scrollbar(chat_frame, command=self.chat_canvas.yview)\n        self.chat_canvas.configure(yscrollcommand=self.chat_scrollbar.set)\n\n        self.chat_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n        self.chat_canvas.grid(row=0, column=0, sticky=\"nsew\")\n\n        # Inner frame to hold all messages\n        self.chat_inner_frame = ttk.Frame(self.chat_canvas)\n        self.chat_canvas.create_window((0, 0), window=self.chat_inner_frame, anchor=\"nw\")\n\n        # Update scroll region when content changes\n        self.chat_inner_frame.bind(\n            \"<Configure>\",\n            lambda e: self.chat_canvas.configure(scrollregion=self.chat_canvas.bbox(\"all\"))\n        )\n\n        # Track the current message label for streaming\n        self.current_message_label = None\n        \n        # Get background color from a standard widget\n        self.bg_color = self.root.cget(\"background\")\n\n        # ----------------------\n        # Code Panel\n        # ----------------------\n        code_frame = ttk.Frame(main_pane)\n        code_frame.rowconfigure(0, weight=1)\n        code_frame.columnconfigure(0, weight=1)\n\n        self.code_panel = tk.Text(code_frame, wrap=\"none\")\n        self.code_panel.grid(row=0, column=0, sticky=\"nsew\")\n        \n        # Bind to sync code_contents when user edits\n        self.code_panel.bind(\"<<Modified>>\", self._on_code_modified)\n\n        def select_all_code_panel(event):\n            event.widget.tag_add(\"sel\", \"1.0\", \"end\")\n            return \"break\"\n\n        self.code_panel.bind(\"<Control-a>\", select_all_code_panel)\n        self.code_panel.bind(\"<Control-A>\", select_all_code_panel)\n        self.code_panel.bind(\"<Command-a>\", select_all_code_panel)  # For Mac\n\n        code_scroll = ttk.Scrollbar(code_frame, command=self.code_panel.yview)\n        code_scroll.grid(row=0, column=1, sticky=\"ns\")\n        self.code_panel[\"yscrollcommand\"] = code_scroll.set\n\n        main_pane.add(chat_frame)\n        main_pane.add(code_frame)\n\n        # ----------------------\n        # Input Box\n        # ----------------------\n        self.input_box = tk.Text(self.root, height=5, wrap=\"word\")\n        self.input_box.grid(row=2, column=0, sticky=\"nsew\")\n\n        self.root.update_idletasks()\n        main_pane.sashpos(0, int(self.root.winfo_width() * 0.70))\n    \n    def _get_client_for_model(self, model: str):\n        \"\"\"Return the appropriate client for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return openai_client\n        else:\n            return groq_client\n    \n    def _get_max_tokens_for_model(self, model: str):\n        \"\"\"Return appropriate max_tokens for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return 16000\n        else:\n            return 8000\n    \n    def _uses_max_completion_tokens(self, model: str):\n        \"\"\"Check if model uses max_completion_tokens instead of max_tokens.\"\"\"\n        return model in [\"gpt-5.2\", \"gpt-5-mini\"]\n\n    def _bind_keys(self):\n        self.input_box.bind(\"<Return>\", self._on_enter)\n        self.input_box.bind(\"<Shift-Return>\", lambda e: None)\n    \n    def _on_code_modified(self, event):\n        \"\"\"Sync code_contents when user edits the code panel.\"\"\"\n        if self.code_panel.edit_modified():\n            self.code_contents = self.code_panel.get(\"1.0\", \"end-1c\")\n            self.code_panel.edit_modified(False)\n\n    # -----------------------------\n    # Input\n    # -----------------------------\n\n    def _on_enter(self, event):\n        if self.is_generating:\n            return \"break\"\n\n        text = self.input_box.get(\"1.0\", \"end-1c\").strip()\n        if not text:\n            return \"break\"\n\n        self.input_box.delete(\"1.0\", \"end\")\n        self._append_chat(\"You\", text)\n        \n        # Add user message to conversation\n        self.conversation.append({\"role\": \"user\", \"content\": text})\n        \n        # Add invisible context about current code if it exists\n        if self.code_contents.strip():\n            self.conversation.append({\n                \"role\": \"user\",\n                \"content\": f\"Here's my current code:\\n{self.code_contents}\"\n            })\n\n        self._start_generation()\n        return \"break\"\n\n    # -----------------------------\n    # Chat Generation\n    # -----------------------------\n\n    def _start_generation(self):\n        self.is_generating = True\n        self.input_box.configure(state=\"disabled\")\n        self.current_assistant_buffer = \"\"\n\n        threading.Thread(\n            target=self._stream_chat_response, daemon=True\n        ).start()\n\n    def _stream_chat_response(self):\n        try:\n            model = self.chat_model.get()\n            client = self._get_client_for_model(model)\n            max_tokens = self._get_max_tokens_for_model(model)\n            \n            # Build request parameters\n            params = {\n                \"model\": model,\n                \"messages\": self.conversation,\n                \"stream\": True,\n            }\n            \n            # Use max_completion_tokens for GPT-5.2 and GPT-5-mini\n            if self._uses_max_completion_tokens(model):\n                params[\"max_completion_tokens\"] = max_tokens\n            else:\n                params[\"max_tokens\"] = max_tokens\n            \n            stream = client.chat.completions.create(**params)\n\n            for chunk in stream:\n                delta = chunk.choices[0].delta.content or \"\"\n                if delta:\n                    self.token_queue.put(delta)\n\n        except Exception as e:\n            self.token_queue.put(f\"\\n[Error: {e}]\")\n\n        finally:\n            self.token_queue.put(None)\n\n    def _poll_tokens(self):\n        try:\n            while True:\n                token = self.token_queue.get_nowait()\n    \n                if token is None:\n                    self._finalize_assistant_message()\n                    break\n    \n                self.current_assistant_buffer += token\n                self._append_to_last_chat(token)\n    \n        except queue.Empty:\n            pass\n        except Exception as e:\n            print(f\"Error polling tokens: {e}\")\n    \n        self.root.after(20, self._poll_tokens)\n    \n    def _finalize_assistant_message(self):\n        assistant_text = self.current_assistant_buffer\n        self.conversation.append({\"role\": \"assistant\", \"content\": assistant_text})\n\n        self.is_generating = False\n        self.input_box.configure(state=\"normal\")\n\n        # Replace the streaming label with parsed content\n        self._replace_with_parsed_message(assistant_text)\n\n    def _replace_with_parsed_message(self, text: str):\n        \"\"\"Parse the message and replace code blocks with interactive widgets.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.destroy()\n            self.current_message_label = None\n\n        # Create a container for this message\n        message_container = ttk.Frame(self.chat_inner_frame)\n        message_container.pack(fill=\"x\", padx=5, pady=5)\n\n        # Add sender as copyable text\n        sender_text = tk.Text(\n            message_container,\n            height=1,\n            wrap=\"none\",\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            background=self.bg_color\n        )\n        sender_text.insert(\"1.0\", \"Assistant:\")\n        sender_text.configure(state=\"disabled\")\n        sender_text.pack(fill=\"x\")\n\n        # Split text by code blocks\n        parts = self._split_text_and_code(text)\n        \n        for part_type, content in parts:\n            if part_type == \"text\":\n                if content.strip():\n                    text_widget = tk.Text(\n                        message_container,\n                        height=content.count(\"\\n\") + 1 + int(len(content) * 0.012),\n                        wrap=\"word\",\n                        relief=\"flat\",\n                        background=self.bg_color\n                    )\n                    text_widget.insert(\"1.0\", content)\n                    text_widget.configure(state=\"disabled\")\n                    text_widget.pack(fill=\"x\", padx=5, pady=2)\n            elif part_type == \"code\":\n                proposal = CodeProposal(\n                    parent=message_container,\n                    code=content,\n                    on_replace=self._replace_code_panel,\n                    on_add=self._add_code_via_helper,\n                )\n                proposal.pack(fill=\"x\", pady=5)\n\n        # Scroll to bottom\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _split_text_and_code(self, text: str):\n        \"\"\"Split text into alternating text and code blocks.\"\"\"\n        parts = []\n        last_end = 0\n        \n        for match in CODE_BLOCK_REGEX.finditer(text):\n            # Add text before code block\n            if match.start() > last_end:\n                parts.append((\"text\", text[last_end:match.start()]))\n            \n            # Add code block\n            parts.append((\"code\", match.group(1)))\n            last_end = match.end()\n        \n        # Add remaining text after last code block\n        if last_end < len(text):\n            parts.append((\"text\", text[last_end:]))\n        \n        return parts\n\n    # -----------------------------\n    # Code Handling (Line-Number Based Approach)\n    # -----------------------------\n\n    def _add_line_numbers(self, code: str) -> str:\n        \"\"\"Prepend line numbers to each line of code.\"\"\"\n        lines = code.split(\"\\n\")\n        numbered_lines = [f\"{i+1} | {line}\" for i, line in enumerate(lines)]\n        return \"\\n\".join(numbered_lines)\n\n    def _add_code_via_helper(self, new_code: str):\n        \"\"\"Ask helper model for insertion point and lines to delete, then apply.\"\"\"\n        if not self.code_contents.strip():\n            self._replace_code_panel(new_code)\n            return\n\n        # Get insertion instructions from helper model\n        insertion_data = self._get_insertion_instructions(new_code)\n        \n        if not insertion_data:\n            self._append_chat(\"System\", \"Helper model did not return valid instructions.\")\n            return\n\n        # Try to apply the insertion and deletion\n        try:\n            updated_code = self._apply_line_based_edit(\n                insertion_data[\"insertion_point\"],\n                insertion_data[\"delete_lines\"],\n                new_code,\n                insertion_data[\"indent_spaces\"]\n            )\n            self.code_contents = updated_code\n            self.code_panel.delete(\"1.0\", \"end\")\n            self.code_panel.insert(\"1.0\", updated_code)\n            self._append_chat(\"System\", \"Code updated successfully!\")\n        except Exception as e:\n            self._append_chat(\"System\", f\"Edit failed: {e}\")\n\n    def _get_insertion_instructions(self, new_code: str) -> dict | None:\n        \"\"\"Ask helper model for insertion point and lines to delete using line numbers.\"\"\"\n        # Add line numbers to current code\n        numbered_code = self._add_line_numbers(self.code_contents)\n        \n        system_prompt = (\n            \"You are a code editing assistant. Your job is to determine where to insert new code \"\n            \"and which lines (if any) to delete from the existing code.\\n\\n\"\n            \"You will receive:\\n\"\n            \"1. The current code with line numbers (format: '1 | code here')\\n\"\n            \"2. New code to insert\\n\\n\"\n            \"You must respond ONLY with JSON in this exact format:\\n\"\n            \"{\\n\"\n            '  \"insertion_point\": <line_number>,\\n'\n            '  \"delete_lines\": [<line_numbers>],\\n'\n            '  \"indent_spaces\": <number_of_spaces>\\n'\n            \"}\\n\\n\"\n            \"Rules:\\n\"\n            \"- insertion_point: The line number AFTER which to insert the new code (use 0 to insert at the beginning)\\n\"\n            \"- delete_lines: Array of line numbers to delete (can be empty [] if just inserting)\\n\"\n            \"- indent_spaces: Number of spaces to add to the beginning of each line of inserted code (to match surrounding indentation)\\n\"\n            \"- Use the CURRENT line numbers you see - don't worry about how insertion affects numbering\\n\"\n            \"- If replacing code, include those line numbers in delete_lines\\n\"\n            \"- If just inserting, leave delete_lines empty\\n\"\n            \"- Look at the indentation of surrounding code to determine indent_spaces\\n\\n\"\n            \"Example 1 - Replace lines 5-7 with 8 spaces of indentation:\\n\"\n            '{\"insertion_point\": 4, \"delete_lines\": [5, 6, 7], \"indent_spaces\": 8}\\n\\n'\n            \"Example 2 - Insert after line 10 with 4 spaces:\\n\"\n            '{\"insertion_point\": 10, \"delete_lines\": [], \"indent_spaces\": 4}\\n\\n'\n            \"Example 3 - Insert at beginning with no indentation:\\n\"\n            '{\"insertion_point\": 0, \"delete_lines\": [], \"indent_spaces\": 0}\\n\\n'\n            \"Respond with ONLY the JSON, no explanations.\"\n        )\n\n        user_prompt = (\n            \"CURRENT CODE (with line numbers):\\n\"\n            f\"{numbered_code}\\n\\n\"\n            \"NEW CODE TO INSERT:\\n\"\n            f\"{new_code}\\n\"\n        )\n\n        model = self.helper_model.get()\n        client = self._get_client_for_model(model)\n        max_tokens = self._get_max_tokens_for_model(model)\n        \n        params = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n        }\n        \n        if self._uses_max_completion_tokens(model):\n            params[\"max_completion_tokens\"] = max_tokens\n        else:\n            params[\"max_tokens\"] = max_tokens\n        \n        try:\n            response = client.chat.completions.create(**params)\n            response_text = response.choices[0].message.content or \"\"\n            \n            print(\"Helper response:\", response_text)\n            \n            # Extract JSON from response (in case model adds markdown)\n            json_match = re.search(r'\\{[^}]+\\}', response_text, re.DOTALL)\n            if json_match:\n                response_text = json_match.group(0)\n            \n            data = json.loads(response_text)\n            \n            # Validate response format\n            if \"insertion_point\" not in data or \"delete_lines\" not in data or \"indent_spaces\" not in data:\n                return None\n            \n            if not isinstance(data[\"insertion_point\"], int):\n                return None\n            \n            if not isinstance(data[\"delete_lines\"], list):\n                return None\n            \n            if not isinstance(data[\"indent_spaces\"], int):\n                return None\n            \n            return data\n            \n        except Exception as e:\n            print(f\"Error getting insertion instructions: {e}\")\n            return None\n\n    def _apply_line_based_edit(self, insertion_point: int, delete_lines: list, new_code: str, indent_spaces: int) -> str:\n        \"\"\"Apply insertion and deletion based on line numbers, with indentation.\"\"\"\n        lines = self.code_contents.split(\"\\n\")\n        \n        # Validate line numbers\n        if insertion_point < 0 or insertion_point > len(lines):\n            raise ValueError(f\"Invalid insertion point: {insertion_point}\")\n        \n        for line_num in delete_lines:\n            if line_num < 1 or line_num > len(lines):\n                raise ValueError(f\"Invalid line number to delete: {line_num}\")\n        \n        # Convert delete_lines to 0-indexed and sort in reverse order\n        delete_indices = sorted([line_num - 1 for line_num in delete_lines], reverse=True)\n        \n        # Adjust insertion point if we're deleting lines before it\n        adjusted_insertion = insertion_point\n        for idx in delete_indices:\n            if idx < insertion_point:\n                adjusted_insertion -= 1\n        \n        # Delete lines (in reverse order to preserve indices)\n        for idx in delete_indices:\n            del lines[idx]\n        \n        # Apply indentation to new code\n        indent = \" \" * indent_spaces\n        new_lines = new_code.split(\"\\n\")\n        indented_lines = [indent + line for line in new_lines]\n        \n        # Insert new code at the adjusted position\n        lines[adjusted_insertion:adjusted_insertion] = indented_lines\n        \n        return \"\\n\".join(lines)\n\n    def _replace_code_panel(self, code: str):\n        self.code_contents = code\n        self.code_panel.delete(\"1.0\", \"end\")\n        self.code_panel.insert(\"1.0\", code)\n\n    # -----------------------------\n    # Persistence\n    # -----------------------------\n    def save_conversation(self, path: str | None = None):\n        if path is None:\n            path = filedialog.asksaveasfilename(\n                defaultextension=\".json\",\n                filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n            )\n            if not path:\n                return\n        try:\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self.conversation, f, indent=2, ensure_ascii=False)\n            self.auto_save_path = path\n            messagebox.showinfo(\"Saved\", f\"Conversation saved to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Save error\", str(e))\n\n    def load_conversation(self):\n        path = filedialog.askopenfilename(\n            defaultextension=\".json\",\n            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                loaded = json.load(f)\n            if not isinstance(loaded, list):\n                raise ValueError(\"File must contain a list of messages.\")\n            self.conversation = loaded\n            self.auto_save_path = path\n            self._redraw_chat()\n            messagebox.showinfo(\"Loaded\", f\"Conversation loaded from\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Load error\", str(e))\n\n    def export_markdown(self):\n        path = filedialog.asksaveasfilename(\n            defaultextension=\".md\",\n            filetypes=[(\"Markdown files\", \"*.md\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            lines = []\n            for msg in self.conversation:\n                role = msg.get(\"role\", \"\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue\n                name = \"You\" if role == \"user\" else \"Assistant\"\n                lines.append(f\"**{name}:**\\n{content}\\n\")\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\\n\".join(lines))\n            messagebox.showinfo(\"Exported\", f\"Markdown exported to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Export error\", str(e))\n\n    def _redraw_chat(self):\n        for w in self.chat_inner_frame.winfo_children():\n            w.destroy()\n        self.current_message_label = None\n        for msg in self.conversation:\n            role = msg.get(\"role\")\n            content = msg.get(\"content\", \"\")\n            if role == \"system\":\n                continue\n            sender = \"You\" if role == \"user\" else \"Assistant\"\n            if role == \"assistant\":\n                self._replace_with_parsed_message(content)\n            else:\n                self._append_chat(sender, content)\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n    def _append_chat(self, sender: str, text: str):\n        container = ttk.Frame(self.chat_inner_frame)\n        container.pack(fill=\"x\", padx=10, pady=2)\n    \n        header = tk.Text(\n            container,\n            height=1,\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            bg=self.bg_color,\n            highlightthickness=0,\n        )\n        header.insert(\"1.0\", f\"{sender}:\")\n        header.configure(state=\"disabled\")\n        header.pack(fill=\"x\")\n    \n        body = tk.Text(\n            container,\n            wrap=\"word\",\n            relief=\"flat\",\n            bg=self.bg_color,\n            highlightthickness=0,\n        )\n        body.insert(\"1.0\", text)\n        body.configure(state=\"disabled\")\n        body.pack(fill=\"x\")\n    \n        def _set_height():\n            body.update_idletasks()\n            count_result = body.count(\"1.0\", \"end-1c\", \"displaylines\")\n            if count_result is not None:\n                display_lines = count_result[0] + 1\n                max_lines = 20  # feel free to tune\n                body.configure(height=max(1, min(display_lines, max_lines)))\n    \n        self.root.after_idle(_set_height)\n    \n        if sender == \"Assistant\":\n            self.current_message_label = body\n            self.current_message_container = container\n    \n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n    \n    \n    def _append_to_last_chat(self, text: str):\n        \"\"\"Streaming: grow the assistant bubble by display lines.\"\"\"\n        if not self.current_message_label:\n            return\n    \n        lbl = self.current_message_label\n        lbl.configure(state=\"normal\")\n        lbl.insert(\"end\", text)\n        lbl.configure(state=\"disabled\")\n    \n        lbl.update_idletasks()\n        count_result = lbl.count(\"1.0\", \"end-1c\", \"displaylines\")\n        if count_result is not None:\n            display_lines = count_result[0]\n            lbl.configure(height=max(1, min(display_lines, 20)))\n    \n        container.pack(fill=\"x\", padx=10, pady=2)\n    \n        header = tk.Text(\n            container,\n            height=1,\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            bg=self.bg_color,\n            highlightthickness=0,\n        )\n        header.insert(\"1.0\", f\"{sender}:\")\n        header.configure(state=\"disabled\")\n        header.pack(fill=\"x\")\n    \n        body = tk.Text(\n            container,\n            wrap=\"word\",\n            relief=\"flat\",\n            bg=self.bg_color,\n            highlightthickness=0,\n        )\n        body.insert(\"1.0\", text)\n        body.configure(state=\"disabled\")\n        body.pack(fill=\"x\")\n    \n        def _set_height():\n            body.update_idletasks()\n            # displaylines counts wrapped lines (what you actually see)\n            display_lines = body.count(\"1.0\", \"end-1c\", \"displaylines\")[0] + 1\n            max_lines = 20  # feel free to tune\n            body.configure(height=max(1, min(display_lines, max_lines)))\n    \n        self.root.after_idle(_set_height)\n    \n        if sender == \"Assistant\":\n            self.current_message_label = body\n            self.current_message_container = container\n    \n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n    \n    \n    def _append_to_last_chat(self, text: str):\n        \"\"\"Streaming: grow the assistant bubble by display lines.\"\"\"\n        if not self.current_message_label:\n            return\n    \n        lbl = self.current_message_label\n        lbl.configure(state=\"normal\")\n        lbl.insert(\"end\", text)\n        lbl.configure(state=\"disabled\")\n    \n        lbl.update_idletasks()\n        display_lines = lbl.count(\"1.0\", \"end-1c\", \"displaylines\")[0]\n        lbl.configure(height=max(1, min(display_lines, 20)))\n    \n        if self.current_message_label:\n            lbl = self.current_message_label\n            lbl.configure(state=\"normal\")\n            lbl.insert(\"end\", text)\n            lbl.configure(state=\"disabled\")\n\n            lbl.update_idletasks()\n            lines = int(lbl.index(\"end-1c\").split(\".\")[0])\n            lbl.configure(height=max(1, min(lines, 12)))\n    \n\nif __name__ == \"__main__\":\n    root = tk.Tk()\n    ChatUI(root)\n    root.mainloop()\n\n"
  },
  {
    "role": "assistant",
    "content": "To exclude the \"Here's my current code:\" snippet from the conversation object, you can modify the `_on_enter` method to only add it to the conversation when generating the input for the LLM, but not when updating the conversation object.\n\nHere's an updated version of the `_on_enter` method:\n\n```python\ndef _on_enter(self, event):\n    if self.is_generating:\n        return \"break\"\n\n    text = self.input_box.get(\"1.0\", \"end-1c\").strip()\n    if not text:\n        return \"break\"\n\n    self.input_box.delete(\"1.0\", \"end\")\n    self._append_chat(\"You\", text)\n    \n    # Add user message to conversation\n    self.conversation.append({\"role\": \"user\", \"content\": text})\n\n    # Prepare the input for the LLM\n    messages = [{\"role\": \"user\", \"content\": text}]\n    if self.code_contents.strip():\n        messages.append({\n            \"role\": \"user\",\n            \"content\": f\"Here's my current code:\\n{self.code_contents}\"\n        })\n\n    # Start generation\n    self._start_generation_with_messages(messages)\n    return \"break\"\n```\n\nThen, you can create a new method `_start_generation_with_messages` that takes the prepared messages as an argument:\n\n```python\ndef _start_generation_with_messages(self, messages):\n    self.is_generating = True\n    self.input_box.configure(state=\"disabled\")\n    self.current_assistant_buffer = \"\"\n\n    threading.Thread(\n        target=self._stream_chat_response_with_messages, args=(messages,), daemon=True\n    ).start()\n```\n\nAnd update the `_stream_chat_response` method to `_stream_chat_response_with_messages`:\n\n```python\ndef _stream_chat_response_with_messages(self, messages):\n    try:\n        model = self.chat_model.get()\n        client = self._get_client_for_model(model)\n        max_tokens = self._get_max_tokens_for_model(model)\n        \n        # Build request parameters\n        params = {\n            \"model\": model,\n            \"messages\": self.conversation + messages,\n            \"stream\": True,\n        }\n        \n        # Use max_completion_tokens for GPT-5.2 and GPT-5-mini\n        if self._uses_max_completion_tokens(model):\n            params[\"max_completion_tokens\"] = max_tokens\n        else:\n            params[\"max_tokens\"] = max_tokens\n        \n        stream = client.chat.completions.create(**params)\n\n        for chunk in stream:\n            delta = chunk.choices[0].delta.content or \"\"\n            if delta:\n                self.token_queue.put(delta)\n\n    except Exception as e:\n        self.token_queue.put(f\"\\n[Error: {e}]\")\n\n    finally:\n        self.token_queue.put(None)\n```\n\nThis way, the \"Here's my current code:\" snippet is only added to the conversation when generating the input for the LLM, but not when updating the conversation object."
  },
  {
    "role": "user",
    "content": "Thank you, but sender and container are not defined inside the _append_to_last_chat method. Please fix this."
  },
  {
    "role": "user",
    "content": "Here's my current code:\nimport os\nimport re\nimport threading\nimport queue\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nimport json\n\n# -----------------------------\n# Environment / Client Setup\n# -----------------------------\n\nload_dotenv()\n\ngroq_client = OpenAI(\n    api_key=os.getenv(\"GROQ_API_KEY\"),\n    base_url=\"https://api.groq.com/openai/v1\",\n)\n\nopenai_client = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\n# Model configurations\nGROQ_MODELS = [\n    \"llama-3.3-70b-versatile\",\n    \"moonshotai/kimi-k2-instruct\",\n    \"moonshotai/kimi-k2-instruct-0905\",\n    \"openai/gpt-oss-120b\",\n    \"openai/gpt-oss-20b\",\n    \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n]\n\nOPENAI_MODELS = [\n    \"gpt-5.2\",\n    \"gpt-5-mini\",\n    \"gpt-4.1\",\n]\n\nALL_MODELS = GROQ_MODELS + OPENAI_MODELS\n\nCODE_BLOCK_REGEX = re.compile(\n    r\"```(?:\\w+)?\\s*(.*?)```\",\n    re.DOTALL\n)\n\n# -----------------------------\n# UI\n# -----------------------------\nclass CodeProposal(ttk.Frame):\n    def __init__(self, parent, code: str, on_replace, on_add):\n        super().__init__(parent)\n        self.code = code\n\n        # Calculate height: number of lines + 1\n        line_count = code.count(\"\\n\") + 1\n        \n        self.text = tk.Text(\n            self,\n            height=line_count,\n            wrap=\"none\",\n            font=(\"Courier New\", 10),\n            padx=5,\n            pady=5,\n            highlightthickness=1 # Border for code blocks\n        )\n        self.text.insert(\"1.0\", code)\n        self.text.configure(state=\"disabled\")\n        self.text.pack(fill=\"x\", padx=5, pady=(5, 2))\n\n        button_row = ttk.Frame(self)\n        button_row.pack(anchor=\"e\", padx=5, pady=(0, 5))\n\n        ttk.Button(button_row, text=\"Replace\", command=lambda: on_replace(code)).pack(side=\"right\", padx=2)\n        ttk.Button(button_row, text=\"Add\", command=lambda: on_add(code)).pack(side=\"right\", padx=2)\n\nclass ChatUI:\n    def __init__(self, root: tk.Tk):\n        self.root = root\n        self.root.title(\"Chat + Code UI\")\n        self.root.geometry(\"1100x650\")\n\n        self.conversation = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n        ]\n\n        self.token_queue = queue.Queue()\n        self.is_generating = False\n        self.current_assistant_buffer = \"\"\n\n        self.code_contents = \"\"\n\n        # ---- persistence helpers ----\n        self.auto_save_path: str | None = None\n        self.auto_save_after_id: str | None = None\n        self.auto_save_interval_ms = 30_000\n        \n        # Default models\n        self.chat_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n        self.helper_model = tk.StringVar(value=\"llama-3.3-70b-versatile\")\n\n        self._build_layout()\n        self._bind_keys()\n        self._poll_tokens()\n\n    # -----------------------------\n    # Layout\n    # -----------------------------\n\n    def _build_layout(self) -> None:\n        self.root.rowconfigure(0, weight=0)\n        self.root.rowconfigure(1, weight=1)\n        self.root.rowconfigure(2, weight=0)\n        self.root.columnconfigure(0, weight=1)\n\n        # ----------------------\n        # Model Selection Bar\n        # ----------------------\n        model_frame = ttk.Frame(self.root)\n        model_frame.grid(row=0, column=0, sticky=\"ew\", padx=5, pady=5)\n        \n        ttk.Label(model_frame, text=\"Chat Model:\").pack(side=\"left\", padx=(0, 5))\n        chat_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.chat_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        chat_dropdown.pack(side=\"left\", padx=(0, 20))\n        \n        ttk.Label(model_frame, text=\"Helper Model:\").pack(side=\"left\", padx=(0, 5))\n        helper_dropdown = ttk.Combobox(\n            model_frame,\n            textvariable=self.helper_model,\n            values=ALL_MODELS,\n            state=\"readonly\",\n            width=40\n        )\n        helper_dropdown.pack(side=\"left\")\n\n        # ----------------------\n        # Menu bar\n        # ----------------------\n        menubar = tk.Menu(self.root)\n        file_menu = tk.Menu(menubar, tearoff=0)\n        file_menu.add_command(label=\"Save conversation…\", command=self.save_conversation)\n        file_menu.add_command(label=\"Load conversation…\", command=self.load_conversation)\n        file_menu.add_separator()\n        file_menu.add_command(label=\"Export markdown…\", command=self.export_markdown)\n        menubar.add_cascade(label=\"File\", menu=file_menu)\n        self.root.config(menu=menubar)\n\n        main_pane = ttk.PanedWindow(self.root, orient=\"horizontal\")\n        main_pane.grid(row=1, column=0, sticky=\"nsew\")\n\n        # ----------------------\n        # Chat + Code Proposals\n        # ----------------------\n        chat_frame = ttk.Frame(main_pane)\n        chat_frame.rowconfigure(0, weight=1)\n        chat_frame.columnconfigure(0, weight=1)\n\n        # Scrollable canvas\n        self.chat_canvas = tk.Canvas(chat_frame)\n        self.chat_scrollbar = ttk.Scrollbar(chat_frame, command=self.chat_canvas.yview)\n        self.chat_canvas.configure(yscrollcommand=self.chat_scrollbar.set)\n\n        self.chat_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n        self.chat_canvas.grid(row=0, column=0, sticky=\"nsew\")\n\n        # Inner frame to hold all messages\n        self.chat_inner_frame = ttk.Frame(self.chat_canvas)\n        self.chat_canvas.create_window((0, 0), window=self.chat_inner_frame, anchor=\"nw\")\n\n        # Update scroll region when content changes\n        self.chat_inner_frame.bind(\n            \"<Configure>\",\n            lambda e: self.chat_canvas.configure(scrollregion=self.chat_canvas.bbox(\"all\"))\n        )\n\n        # Track the current message label for streaming\n        self.current_message_label = None\n        \n        # Get background color from a standard widget\n        self.bg_color = self.root.cget(\"background\")\n\n        # ----------------------\n        # Code Panel\n        # ----------------------\n        code_frame = ttk.Frame(main_pane)\n        code_frame.rowconfigure(0, weight=1)\n        code_frame.columnconfigure(0, weight=1)\n\n        self.code_panel = tk.Text(code_frame, wrap=\"none\")\n        self.code_panel.grid(row=0, column=0, sticky=\"nsew\")\n        \n        # Bind to sync code_contents when user edits\n        self.code_panel.bind(\"<<Modified>>\", self._on_code_modified)\n\n        def select_all_code_panel(event):\n            event.widget.tag_add(\"sel\", \"1.0\", \"end\")\n            return \"break\"\n\n        self.code_panel.bind(\"<Control-a>\", select_all_code_panel)\n        self.code_panel.bind(\"<Control-A>\", select_all_code_panel)\n        self.code_panel.bind(\"<Command-a>\", select_all_code_panel)  # For Mac\n\n        code_scroll = ttk.Scrollbar(code_frame, command=self.code_panel.yview)\n        code_scroll.grid(row=0, column=1, sticky=\"ns\")\n        self.code_panel[\"yscrollcommand\"] = code_scroll.set\n\n        main_pane.add(chat_frame)\n        main_pane.add(code_frame)\n\n        # ----------------------\n        # Input Box\n        # ----------------------\n        self.input_box = tk.Text(self.root, height=5, wrap=\"word\")\n        self.input_box.grid(row=2, column=0, sticky=\"nsew\")\n\n        self.root.update_idletasks()\n        main_pane.sashpos(0, int(self.root.winfo_width() * 0.70))\n    \n    def _get_client_for_model(self, model: str):\n        \"\"\"Return the appropriate client for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return openai_client\n        else:\n            return groq_client\n    \n    def _get_max_tokens_for_model(self, model: str):\n        \"\"\"Return appropriate max_tokens for the given model.\"\"\"\n        if model in OPENAI_MODELS:\n            return 16000\n        else:\n            return 8000\n    \n    def _uses_max_completion_tokens(self, model: str):\n        \"\"\"Check if model uses max_completion_tokens instead of max_tokens.\"\"\"\n        return model in [\"gpt-5.2\", \"gpt-5-mini\"]\n\n    def _bind_keys(self):\n        self.input_box.bind(\"<Return>\", self._on_enter)\n        self.input_box.bind(\"<Shift-Return>\", lambda e: None)\n    \n    def _on_code_modified(self, event):\n        \"\"\"Sync code_contents when user edits the code panel.\"\"\"\n        if self.code_panel.edit_modified():\n            self.code_contents = self.code_panel.get(\"1.0\", \"end-1c\")\n            self.code_panel.edit_modified(False)\n\n    # -----------------------------\n    # Input\n    # -----------------------------\n\n        def _on_enter(self, event):\n            if self.is_generating:\n                return \"break\"\n        \n            text = self.input_box.get(\"1.0\", \"end-1c\").strip()\n            if not text:\n                return \"break\"\n        \n            self.input_box.delete(\"1.0\", \"end\")\n            self._append_chat(\"You\", text)\n            \n            # Add user message to conversation\n            self.conversation.append({\"role\": \"user\", \"content\": text})\n        \n            # Prepare the input for the LLM\n            messages = [{\"role\": \"user\", \"content\": text}]\n            if self.code_contents.strip():\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": f\"Here's my current code:\\n{self.code_contents}\"\n                })\n        \n            # Start generation\n            self._start_generation_with_messages(messages)\n            return \"break\"\n        \n        return \"break\"\n\n    # -----------------------------\n    # Chat Generation\n    # -----------------------------\n\n        def _start_generation_with_messages(self, messages):\n            self.is_generating = True\n            self.input_box.configure(state=\"disabled\")\n            self.current_assistant_buffer = \"\"\n        \n            threading.Thread(\n                target=self._stream_chat_response_with_messages, args=(messages,), daemon=True\n            ).start()\n        \n    def _start_generation(self):\n        self.is_generating = True\n        self.input_box.configure(state=\"disabled\")\n        self.current_assistant_buffer = \"\"\n\n        threading.Thread(\n            target=self._stream_chat_response, daemon=True\n        ).start()\n\n    def _stream_chat_response(self):\n        def _stream_chat_response_with_messages(self, messages):\n            try:\n                model = self.chat_model.get()\n                client = self._get_client_for_model(model)\n                max_tokens = self._get_max_tokens_for_model(model)\n                \n                # Build request parameters\n                params = {\n                    \"model\": model,\n                    \"messages\": self.conversation + messages,\n                    \"stream\": True,\n                }\n                \n                # Use max_completion_tokens for GPT-5.2 and GPT-5-mini\n                if self._uses_max_completion_tokens(model):\n                    params[\"max_completion_tokens\"] = max_tokens\n                else:\n                    params[\"max_tokens\"] = max_tokens\n                \n                stream = client.chat.completions.create(**params)\n        \n                for chunk in stream:\n                    delta = chunk.choices[0].delta.content or \"\"\n                    if delta:\n                        self.token_queue.put(delta)\n        \n            except Exception as e:\n                self.token_queue.put(f\"\\n[Error: {e}]\")\n        \n            finally:\n                self.token_queue.put(None)\n        \n        try:\n            model = self.chat_model.get()\n            client = self._get_client_for_model(model)\n            max_tokens = self._get_max_tokens_for_model(model)\n            \n            # Build request parameters\n            params = {\n                \"model\": model,\n                \"messages\": self.conversation,\n                \"stream\": True,\n            }\n            \n            # Use max_completion_tokens for GPT-5.2 and GPT-5-mini\n            if self._uses_max_completion_tokens(model):\n                params[\"max_completion_tokens\"] = max_tokens\n            else:\n                params[\"max_tokens\"] = max_tokens\n            \n            stream = client.chat.completions.create(**params)\n\n            for chunk in stream:\n                delta = chunk.choices[0].delta.content or \"\"\n                if delta:\n                    self.token_queue.put(delta)\n\n        except Exception as e:\n            self.token_queue.put(f\"\\n[Error: {e}]\")\n\n        finally:\n            self.token_queue.put(None)\n\n    def _poll_tokens(self):\n        try:\n            while True:\n                token = self.token_queue.get_nowait()\n    \n                if token is None:\n                    self._finalize_assistant_message()\n                    break\n    \n                self.current_assistant_buffer += token\n                self._append_to_last_chat(token)\n    \n        except queue.Empty:\n            pass\n        except Exception as e:\n            print(f\"Error polling tokens: {e}\")\n    \n        self.root.after(20, self._poll_tokens)\n    \n    def _finalize_assistant_message(self):\n        assistant_text = self.current_assistant_buffer\n        self.conversation.append({\"role\": \"assistant\", \"content\": assistant_text})\n\n        self.is_generating = False\n        self.input_box.configure(state=\"normal\")\n\n        # Replace the streaming label with parsed content\n        self._replace_with_parsed_message(assistant_text)\n\n    def _replace_with_parsed_message(self, text: str):\n        \"\"\"Parse the message and replace code blocks with interactive widgets.\"\"\"\n        if self.current_message_label:\n            self.current_message_label.destroy()\n            self.current_message_label = None\n\n        # Create a container for this message\n        message_container = ttk.Frame(self.chat_inner_frame)\n        message_container.pack(fill=\"x\", padx=5, pady=5)\n\n        # Add sender as copyable text\n        sender_text = tk.Text(\n            message_container,\n            height=1,\n            wrap=\"none\",\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            background=self.bg_color\n        )\n        sender_text.insert(\"1.0\", \"Assistant:\")\n        sender_text.configure(state=\"disabled\")\n        sender_text.pack(fill=\"x\")\n\n        # Split text by code blocks\n        parts = self._split_text_and_code(text)\n        \n        for part_type, content in parts:\n            if part_type == \"text\":\n                if content.strip():\n                    text_widget = tk.Text(\n                        message_container,\n                        height=content.count(\"\\n\") + 1 + int(len(content) * 0.012),\n                        wrap=\"word\",\n                        relief=\"flat\",\n                        background=self.bg_color\n                    )\n                    text_widget.insert(\"1.0\", content)\n                    text_widget.configure(state=\"disabled\")\n                    text_widget.pack(fill=\"x\", padx=5, pady=2)\n            elif part_type == \"code\":\n                proposal = CodeProposal(\n                    parent=message_container,\n                    code=content,\n                    on_replace=self._replace_code_panel,\n                    on_add=self._add_code_via_helper,\n                )\n                proposal.pack(fill=\"x\", pady=5)\n\n        # Scroll to bottom\n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n\n    def _split_text_and_code(self, text: str):\n        \"\"\"Split text into alternating text and code blocks.\"\"\"\n        parts = []\n        last_end = 0\n        \n        for match in CODE_BLOCK_REGEX.finditer(text):\n            # Add text before code block\n            if match.start() > last_end:\n                parts.append((\"text\", text[last_end:match.start()]))\n            \n            # Add code block\n            parts.append((\"code\", match.group(1)))\n            last_end = match.end()\n        \n        # Add remaining text after last code block\n        if last_end < len(text):\n            parts.append((\"text\", text[last_end:]))\n        \n        return parts\n\n    # -----------------------------\n    # Code Handling (Line-Number Based Approach)\n    # -----------------------------\n\n    def _add_line_numbers(self, code: str) -> str:\n        \"\"\"Prepend line numbers to each line of code.\"\"\"\n        lines = code.split(\"\\n\")\n        numbered_lines = [f\"{i+1} | {line}\" for i, line in enumerate(lines)]\n        return \"\\n\".join(numbered_lines)\n\n    def _add_code_via_helper(self, new_code: str):\n        \"\"\"Ask helper model for insertion point and lines to delete, then apply.\"\"\"\n        if not self.code_contents.strip():\n            self._replace_code_panel(new_code)\n            return\n\n        # Get insertion instructions from helper model\n        insertion_data = self._get_insertion_instructions(new_code)\n        \n        if not insertion_data:\n            self._append_chat(\"System\", \"Helper model did not return valid instructions.\")\n            return\n\n        # Try to apply the insertion and deletion\n        try:\n            updated_code = self._apply_line_based_edit(\n                insertion_data[\"insertion_point\"],\n                insertion_data[\"delete_lines\"],\n                new_code,\n                insertion_data[\"indent_spaces\"]\n            )\n            self.code_contents = updated_code\n            self.code_panel.delete(\"1.0\", \"end\")\n            self.code_panel.insert(\"1.0\", updated_code)\n            self._append_chat(\"System\", \"Code updated successfully!\")\n        except Exception as e:\n            self._append_chat(\"System\", f\"Edit failed: {e}\")\n\n    def _get_insertion_instructions(self, new_code: str) -> dict | None:\n        \"\"\"Ask helper model for insertion point and lines to delete using line numbers.\"\"\"\n        # Add line numbers to current code\n        numbered_code = self._add_line_numbers(self.code_contents)\n        \n        system_prompt = (\n            \"You are a code editing assistant. Your job is to determine where to insert new code \"\n            \"and which lines (if any) to delete from the existing code.\\n\\n\"\n            \"You will receive:\\n\"\n            \"1. The current code with line numbers (format: '1 | code here')\\n\"\n            \"2. New code to insert\\n\\n\"\n            \"You must respond ONLY with JSON in this exact format:\\n\"\n            \"{\\n\"\n            '  \"insertion_point\": <line_number>,\\n'\n            '  \"delete_lines\": [<line_numbers>],\\n'\n            '  \"indent_spaces\": <number_of_spaces>\\n'\n            \"}\\n\\n\"\n            \"Rules:\\n\"\n            \"- insertion_point: The line number AFTER which to insert the new code (use 0 to insert at the beginning)\\n\"\n            \"- delete_lines: Array of line numbers to delete (can be empty [] if just inserting)\\n\"\n            \"- indent_spaces: Number of spaces to add to the beginning of each line of inserted code (to match surrounding indentation)\\n\"\n            \"- Use the CURRENT line numbers you see - don't worry about how insertion affects numbering\\n\"\n            \"- If replacing code, include those line numbers in delete_lines\\n\"\n            \"- If just inserting, leave delete_lines empty\\n\"\n            \"- Look at the indentation of surrounding code to determine indent_spaces\\n\\n\"\n            \"Example 1 - Replace lines 5-7 with 8 spaces of indentation:\\n\"\n            '{\"insertion_point\": 4, \"delete_lines\": [5, 6, 7], \"indent_spaces\": 8}\\n\\n'\n            \"Example 2 - Insert after line 10 with 4 spaces:\\n\"\n            '{\"insertion_point\": 10, \"delete_lines\": [], \"indent_spaces\": 4}\\n\\n'\n            \"Example 3 - Insert at beginning with no indentation:\\n\"\n            '{\"insertion_point\": 0, \"delete_lines\": [], \"indent_spaces\": 0}\\n\\n'\n            \"Respond with ONLY the JSON, no explanations.\"\n        )\n\n        user_prompt = (\n            \"CURRENT CODE (with line numbers):\\n\"\n            f\"{numbered_code}\\n\\n\"\n            \"NEW CODE TO INSERT:\\n\"\n            f\"{new_code}\\n\"\n        )\n\n        model = self.helper_model.get()\n        client = self._get_client_for_model(model)\n        max_tokens = self._get_max_tokens_for_model(model)\n        \n        params = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n        }\n        \n        if self._uses_max_completion_tokens(model):\n            params[\"max_completion_tokens\"] = max_tokens\n        else:\n            params[\"max_tokens\"] = max_tokens\n        \n        try:\n            response = client.chat.completions.create(**params)\n            response_text = response.choices[0].message.content or \"\"\n            \n            print(\"Helper response:\", response_text)\n            \n            # Extract JSON from response (in case model adds markdown)\n            json_match = re.search(r'\\{[^}]+\\}', response_text, re.DOTALL)\n            if json_match:\n                response_text = json_match.group(0)\n            \n            data = json.loads(response_text)\n            \n            # Validate response format\n            if \"insertion_point\" not in data or \"delete_lines\" not in data or \"indent_spaces\" not in data:\n                return None\n            \n            if not isinstance(data[\"insertion_point\"], int):\n                return None\n            \n            if not isinstance(data[\"delete_lines\"], list):\n                return None\n            \n            if not isinstance(data[\"indent_spaces\"], int):\n                return None\n            \n            return data\n            \n        except Exception as e:\n            print(f\"Error getting insertion instructions: {e}\")\n            return None\n\n    def _apply_line_based_edit(self, insertion_point: int, delete_lines: list, new_code: str, indent_spaces: int) -> str:\n        \"\"\"Apply insertion and deletion based on line numbers, with indentation.\"\"\"\n        lines = self.code_contents.split(\"\\n\")\n        \n        # Validate line numbers\n        if insertion_point < 0 or insertion_point > len(lines):\n            raise ValueError(f\"Invalid insertion point: {insertion_point}\")\n        \n        for line_num in delete_lines:\n            if line_num < 1 or line_num > len(lines):\n                raise ValueError(f\"Invalid line number to delete: {line_num}\")\n        \n        # Convert delete_lines to 0-indexed and sort in reverse order\n        delete_indices = sorted([line_num - 1 for line_num in delete_lines], reverse=True)\n        \n        # Adjust insertion point if we're deleting lines before it\n        adjusted_insertion = insertion_point\n        for idx in delete_indices:\n            if idx < insertion_point:\n                adjusted_insertion -= 1\n        \n        # Delete lines (in reverse order to preserve indices)\n        for idx in delete_indices:\n            del lines[idx]\n        \n        # Apply indentation to new code\n        indent = \" \" * indent_spaces\n        new_lines = new_code.split(\"\\n\")\n        indented_lines = [indent + line for line in new_lines]\n        \n        # Insert new code at the adjusted position\n        lines[adjusted_insertion:adjusted_insertion] = indented_lines\n        \n        return \"\\n\".join(lines)\n\n    def _replace_code_panel(self, code: str):\n        self.code_contents = code\n        self.code_panel.delete(\"1.0\", \"end\")\n        self.code_panel.insert(\"1.0\", code)\n\n    # -----------------------------\n    # Persistence\n    # -----------------------------\n    def save_conversation(self, path: str | None = None):\n        if path is None:\n            path = filedialog.asksaveasfilename(\n                defaultextension=\".json\",\n                filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n            )\n            if not path:\n                return\n        try:\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self.conversation, f, indent=2, ensure_ascii=False)\n            self.auto_save_path = path\n            messagebox.showinfo(\"Saved\", f\"Conversation saved to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Save error\", str(e))\n\n    def load_conversation(self):\n        path = filedialog.askopenfilename(\n            defaultextension=\".json\",\n            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                loaded = json.load(f)\n            if not isinstance(loaded, list):\n                raise ValueError(\"File must contain a list of messages.\")\n            self.conversation = loaded\n            self.auto_save_path = path\n            self._redraw_chat()\n            messagebox.showinfo(\"Loaded\", f\"Conversation loaded from\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Load error\", str(e))\n\n    def export_markdown(self):\n        path = filedialog.asksaveasfilename(\n            defaultextension=\".md\",\n            filetypes=[(\"Markdown files\", \"*.md\"), (\"All files\", \"*.*\")]\n        )\n        if not path:\n            return\n        try:\n            lines = []\n            for msg in self.conversation:\n                role = msg.get(\"role\", \"\")\n                content = msg.get(\"content\", \"\")\n                if role == \"system\":\n                    continue\n                name = \"You\" if role == \"user\" else \"Assistant\"\n                lines.append(f\"**{name}:**\\n{content}\\n\")\n            with open(path, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"\\n\".join(lines))\n            messagebox.showinfo(\"Exported\", f\"Markdown exported to\\n{path}\")\n        except Exception as e:\n            messagebox.showerror(\"Export error\", str(e))\n\n    def _redraw_chat(self):\n        for w in self.chat_inner_frame.winfo_children():\n            w.destroy()\n        self.current_message_label = None\n        for msg in self.conversation:\n            role = msg.get(\"role\")\n            content = msg.get(\"content\", \"\")\n            if role == \"system\":\n                continue\n            sender = \"You\" if role == \"user\" else \"Assistant\"\n            if role == \"assistant\":\n                self._replace_with_parsed_message(content)\n            else:\n                self._append_chat(sender, content)\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n\n    # -----------------------------\n    # UI Helpers\n    # -----------------------------\n    def _append_chat(self, sender: str, text: str):\n        container = ttk.Frame(self.chat_inner_frame)\n        container.pack(fill=\"x\", padx=10, pady=2)\n    \n        header = tk.Text(\n            container,\n            height=1,\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            bg=self.bg_color,\n            highlightthickness=0,\n        )\n        header.insert(\"1.0\", f\"{sender}:\")\n        header.configure(state=\"disabled\")\n        header.pack(fill=\"x\")\n    \n        body = tk.Text(\n            container,\n            wrap=\"word\",\n            relief=\"flat\",\n            bg=self.bg_color,\n            highlightthickness=0,\n        )\n        body.insert(\"1.0\", text)\n        body.configure(state=\"disabled\")\n        body.pack(fill=\"x\")\n    \n        def _set_height():\n            body.update_idletasks()\n            count_result = body.count(\"1.0\", \"end-1c\", \"displaylines\")\n            if count_result is not None:\n                display_lines = count_result[0] + 1\n                max_lines = 20  # feel free to tune\n                body.configure(height=max(1, min(display_lines, max_lines)))\n    \n        self.root.after_idle(_set_height)\n    \n        if sender == \"Assistant\":\n            self.current_message_label = body\n            self.current_message_container = container\n    \n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n    \n    \n    def _append_to_last_chat(self, text: str):\n        \"\"\"Streaming: grow the assistant bubble by display lines.\"\"\"\n        if not self.current_message_label:\n            return\n    \n        lbl = self.current_message_label\n        lbl.configure(state=\"normal\")\n        lbl.insert(\"end\", text)\n        lbl.configure(state=\"disabled\")\n    \n        lbl.update_idletasks()\n        count_result = lbl.count(\"1.0\", \"end-1c\", \"displaylines\")\n        if count_result is not None:\n            display_lines = count_result[0]\n            lbl.configure(height=max(1, min(display_lines, 20)))\n    \n        container.pack(fill=\"x\", padx=10, pady=2)\n    \n        header = tk.Text(\n            container,\n            height=1,\n            font=(\"TkDefaultFont\", 10, \"bold\"),\n            relief=\"flat\",\n            bg=self.bg_color,\n            highlightthickness=0,\n        )\n        header.insert(\"1.0\", f\"{sender}:\")\n        header.configure(state=\"disabled\")\n        header.pack(fill=\"x\")\n    \n        body = tk.Text(\n            container,\n            wrap=\"word\",\n            relief=\"flat\",\n            bg=self.bg_color,\n            highlightthickness=0,\n        )\n        body.insert(\"1.0\", text)\n        body.configure(state=\"disabled\")\n        body.pack(fill=\"x\")\n    \n        def _set_height():\n            body.update_idletasks()\n            # displaylines counts wrapped lines (what you actually see)\n            display_lines = body.count(\"1.0\", \"end-1c\", \"displaylines\")[0] + 1\n            max_lines = 20  # feel free to tune\n            body.configure(height=max(1, min(display_lines, max_lines)))\n    \n        self.root.after_idle(_set_height)\n    \n        if sender == \"Assistant\":\n            self.current_message_label = body\n            self.current_message_container = container\n    \n        self.chat_canvas.update_idletasks()\n        self.chat_canvas.yview_moveto(1.0)\n    \n    \n    def _append_to_last_chat(self, text: str):\n        \"\"\"Streaming: grow the assistant bubble by display lines.\"\"\"\n        if not self.current_message_label:\n            return\n    \n        lbl = self.current_message_label\n        lbl.configure(state=\"normal\")\n        lbl.insert(\"end\", text)\n        lbl.configure(state=\"disabled\")\n    \n        lbl.update_idletasks()\n        display_lines = lbl.count(\"1.0\", \"end-1c\", \"displaylines\")[0]\n        lbl.configure(height=max(1, min(display_lines, 20)))\n    \n        if self.current_message_label:\n            lbl = self.current_message_label\n            lbl.configure(state=\"normal\")\n            lbl.insert(\"end\", text)\n            lbl.configure(state=\"disabled\")\n\n            lbl.update_idletasks()\n            lines = int(lbl.index(\"end-1c\").split(\".\")[0])\n            lbl.configure(height=max(1, min(lines, 12)))\n    \n\nif __name__ == \"__main__\":\n    root = tk.Tk()\n    ChatUI(root)\n    root.mainloop()\n\n"
  },
  {
    "role": "assistant",
    "content": "You're right, `sender` and `container` are not defined in the `_append_to_last_chat` method.\n\nThe duplicated code in the `_append_to_last_chat` method seems to be a mistake. The method should only append the text to the last chat, it should not create a new chat.\n\nHere's the corrected `_append_to_last_chat` method:\n\n```python\ndef _append_to_last_chat(self, text: str):\n    \"\"\"Streaming: grow the assistant bubble by display lines.\"\"\"\n    if not self.current_message_label:\n        return\n    \n    lbl = self.current_message_label\n    lbl.configure(state=\"normal\")\n    lbl.insert(\"end\", text)\n    lbl.configure(state=\"disabled\")\n    \n    lbl.update_idletasks()\n    count_result = lbl.count(\"1.0\", \"end-1c\", \"displaylines\")\n    if count_result is not None:\n        display_lines = count_result[0]\n        lbl.configure(height=max(1, min(display_lines, 20)))\n```\n\nAnd, `self.current_message_container` should be defined in the `_append_chat` method, so that it can be used in the `_append_to_last_chat` method.\n\nHere's the corrected `_append_chat` method:\n\n```python\ndef _append_chat(self, sender: str, text: str):\n    container = ttk.Frame(self.chat_inner_frame)\n    container.pack(fill=\"x\", padx=10, pady=2)\n    \n    header = tk.Text(\n        container,\n        height=1,\n        font=(\"TkDefaultFont\", 10, \"bold\"),\n        relief=\"flat\",\n        bg=self.bg_color,\n        highlightthickness=0,\n    )\n    header.insert(\"1.0\", f\"{sender}:\")\n    header.configure(state=\"disabled\")\n    header.pack(fill=\"x\")\n    \n    body = tk.Text(\n        container,\n        wrap=\"word\",\n        relief=\"flat\",\n        bg=self.bg_color,\n        highlightthickness=0,\n    )\n    body.insert(\"1.0\", text)\n    body.configure(state=\"disabled\")\n    body.pack(fill=\"x\")\n    \n    def _set_height():\n        body.update_idletasks()\n        count_result = body.count(\"1.0\", \"end-1c\", \"displaylines\")\n        if count_result is not None:\n            display_lines = count_result[0] + 1\n            max_lines = 20  # feel free to tune\n            body.configure(height=max(1, min(display_lines, max_lines)))\n    \n    self.root.after_idle(_set_height)\n    \n    if sender == \"Assistant\":\n        self.current_message_label = body\n        self.current_message_container = container\n    \n    self.chat_canvas.update_idletasks()\n    self.chat_canvas.yview_moveto(1.0)\n```"
  }
]